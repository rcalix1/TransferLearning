# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eIkt0MTsLRHNPSRTWYlikCg5TO7GFHk4
"""

import torch
from tqdm import tqdm
import pandas as pd

tqdm.pandas()

import datasets

from transformers import pipeline, AutoTokenizer
from datasets import load_dataset

from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler

!pip install trl==0.11.3

torch.cuda.empty_cache()

config = PPOConfig(
    model_name="distilbert/distilgpt2",
    learning_rate=1.41e-5,
)

sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 8}

def build_dataset(config, dataset_name="imdb", revision="main", input_min_text_length=4, input_max_text_length=12):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        dataset_name (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    ds = load_dataset(dataset_name, split="train", revision=revision)

    ds = ds.filter(lambda x:  (len(x["text"]) > 200 if x["text"] is not None else False), batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["text"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds

dataset = build_dataset(config)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

dataset[900]

model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

tokenizer.pad_token = tokenizer.eos_token

ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
sentiment_pipe = pipeline("sentiment-analysis", model="ealvaradob/bert-finetuned-phishing", return_all_scores=True, device=device)

sentiment_pipe = pipeline(
    "text-classification",
    model="ealvaradob/bert-finetuned-phishing",
    return_all_scores=True,
    function_to_apply="none"
)

text = "Please kindly provide me with $500 immediately to receive your diamonds"
result = sentiment_pipe(text)
print(result)

text = "this movie was really bad!!"
sentiment_pipe(text, **sent_kwargs)

text = "this movie was really good!! Please kindly provide me with $500 immediately to recieve your diamonds"
out_test = sentiment_pipe(text, **sent_kwargs)

out_test

[torch.tensor(out_test[0]["score"]) for output in out_test]

gen_kwargs = {"min_length": -1, "top_k": 0.0, "top_p": 1.0, "do_sample": True, "pad_token_id": tokenizer.eos_token_id}

output_min_length = 6
output_max_length = 20
output_length_sampler = LengthSampler(output_min_length, output_max_length)


generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    #### Get response from gpt2
    response_tensors = []
    for query in query_tensors:
        gen_len = output_length_sampler()
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    ## rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    rewards = []
    for output in pipe_outputs:
        if isinstance(output, list):  # expected case: list of label-score dicts
            label_scores = {entry["label"]: entry["score"] for entry in output}
            reward = label_scores.get("phishing", 0.0)
        elif isinstance(output, dict):  # fallback if only one label returned
            reward = output["score"] if output["label"] == "phishing" else 0.0
        else:
            reward = 0.0  # fallback in case something is malformed
        rewards.append(torch.tensor(reward))




    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

#### get a batch from the dataset
bs = 16
game_data = dict()
dataset.set_format("pandas")
df_batch = dataset[:].sample(bs)
game_data["query"] = df_batch["query"].tolist()
query_tensors = df_batch["input_ids"].tolist()

response_tensors_ref, response_tensors = [], []

# Ensure pad_token_id is set to avoid generation warnings
gen_kwargs["pad_token_id"] = tokenizer.eos_token_id

#### get response from gpt2 and gpt2_ref
for i in range(bs):
    gen_len = output_length_sampler()
    # Using ppo_trainer models to ensure we use the correct generative models
    output_ref = ppo_trainer.ref_model.generate(
        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),
        max_new_tokens=gen_len,
        **gen_kwargs
    ).squeeze()[-gen_len:]
    response_tensors_ref.append(output_ref)

    output_model = ppo_trainer.model.generate(
        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),
        max_new_tokens=gen_len,
        **gen_kwargs
    ).squeeze()[-gen_len:]
    response_tensors.append(output_model)

#### decode responses
game_data["response (before)"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]
game_data["response (after)"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]

# Robustly extract the phishing score from the sentiment pipeline output
def extract_phishing_reward(pipe_outputs):
    rewards = []
    for output in pipe_outputs:
        reward = 0.0
        if isinstance(output, list):
            for entry in output:
                if entry["label"] == "phishing":
                    reward = entry["score"]
                    break
        elif isinstance(output, dict):
            if output.get("label") == "phishing":
                reward = output.get("score", 0.0)
        rewards.append(reward)
    return rewards

#### sentiment analysis of query/response pairs before/after
texts_before = [q + r for q, r in zip(game_data["query"], game_data["response (before)"])]
game_data["rewards (before)"] = extract_phishing_reward(sentiment_pipe(texts_before, **sent_kwargs))

texts_after = [q + r for q, r in zip(game_data["query"], game_data["response (after)"])]
game_data["rewards (after)"] = extract_phishing_reward(sentiment_pipe(texts_after, **sent_kwargs))

# store results in a dataframe
df_results = pd.DataFrame(game_data)
df_results











