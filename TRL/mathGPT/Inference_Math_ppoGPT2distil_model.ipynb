{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b10a2e-8bce-4663-8b95-391aa240921b",
   "metadata": {},
   "source": [
    "\n",
    "## Inference Math GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fa87fe-0e9c-4606-8566-f4fadb76f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0400846-9ee7-4570-bd5b-49e3e1b8d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoModel\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e9345a-7513-4803-85ea-39ad4fb2f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## When testing multiple samples from the dataset, this function allows for batch formatting of the prompts\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        text = f'### Question: {example[\"question\"][i]}'\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899df8ed-08f9-4a0f-a22f-a718e03d6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Quantization config for loading lower precision models\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5abac7-69ed-492d-acf4-4741cc13b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_map = {\"\": \"cuda:0\"}\n",
    "access_token = None ## PROVIDE OWN HUGGING FACE ACCESS TOKEN HERE\n",
    "\n",
    "## Load gsm8k dataset\n",
    "dataset_name = 'gsm8k'\n",
    "eval_dataset = load_dataset(dataset_name, name='main', split='test')\n",
    "\n",
    "## Define model here. This could be either a HF model on the hub, or a locally saved model\n",
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "## model_name = 'C:/Users/ITS490/SeniorProject/trl/ppo_flant5_heuristic'\n",
    "## model_name = 'sft_gpt2'\n",
    "model_name = 'ppo_gpt2distil_heuristic_original'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3336ac39-4916-4ea5-9af4-13fef5030499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ppo_gpt2distil_heuristic_original were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Load model using CausalLM for gpt2 and llama2, or Seq2SeqLM for t5. Use line with quantization_config and token for loading llama2\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, trust_remote_code=True)\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, trust_remote_code=True, quantization_config=bnb_config, token=access_token)\n",
    "#model.config.pretraining_tp = 1 \n",
    "\n",
    "\n",
    "### model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=device_map, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c99dc25-7c0e-449f-87c5-449e8c65101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e1f1d5-3a18-4166-9a24-5d340fa5cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create pipeline for generating text. Use text2text-generation for t5, or text-generation for gpt2 or llama2\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n",
    "\n",
    "## generator = pipeline('text2text-generation', model=model_name, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefcfec8-a598-4883-a1b2-54851ddc9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 128,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357d414f-f072-4134-b9a0-297dd0ec71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## For use when testing llama2, change eval_dataset[start:end] to slice out specific parts of the dataset \n",
    "# for i, entry in enumerate(formatting_prompts_func(eval_dataset[1:4])):\n",
    "#     print(eval_dataset[i+1])\n",
    "#     print('\\n')\n",
    "#     eval_tokens = tokenizer.encode(entry)\n",
    "#     eval_response = model.generate(eval_tokens, **generation_kwargs)\n",
    "#     print(tokenizer.decode(eval_response))\n",
    "#     print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5786a5-41cc-4cb1-97e3-2232bfe0f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?', 'answer': 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3'}\n",
      "\n",
      "\n",
      "[{'generated_text': '### Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?'}]\n",
      "\n",
      "\n",
      "\n",
      "{'question': 'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?', 'answer': 'The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\\nHe increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\\nSo the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\\nSo he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\\n#### 70000'}\n",
      "\n",
      "\n",
      "[{'generated_text': '### Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?'}]\n",
      "\n",
      "\n",
      "\n",
      "{'question': 'James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?', 'answer': 'He sprints 3*3=<<3*3=9>>9 times\\nSo he runs 9*60=<<9*60=540>>540 meters\\n#### 540'}\n",
      "\n",
      "\n",
      "[{'generated_text': '### Question: James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?                                                          '}]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## For use when testing gpt2 or t5, change eval_dataset[start:end] to slice out specific parts of the dataset \n",
    "for i, entry in enumerate(formatting_prompts_func(eval_dataset[1:4])):\n",
    "     print(eval_dataset[i+1])\n",
    "     print('\\n')\n",
    "     print(generator(entry))\n",
    "     print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f486354-db03-4e9e-8d67-f9ca4ed6814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"### Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? ### Answer: \", 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}\n",
      "\n",
      "\n",
      "[{'generated_text': \"### Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? ### Answer: ________\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## For use when testing a single sample, change eval_dataset[0] to access a specific entry in the dataset\n",
    "query = eval_dataset[0]\n",
    "query['question'] = '### Question: ' + query['question'] + ' ### Answer: '\n",
    "print(query)\n",
    "print('\\n')\n",
    "print(generator(query['question']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a90b6-8673-4920-ac5c-329090a84d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc8977-0e5b-49b1-aa75-46fcd469fd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130ba27-50da-4fdf-a223-025b0e93a7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef94ff-5926-4d89-a7a3-a12338155893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ea709-56a8-48cb-9661-fb59adf89050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742d11f-913e-491c-a4d0-5c51f99dd8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b8ca2-d773-4b90-b58f-28acfe01ceb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5a1c7-5076-4c35-823d-c46547cec043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6ed89-d869-4112-83a4-90e616235271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761feb3-fe20-48a1-8257-6c59d78abb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6a39-2139-4ba4-a795-d40bb0078970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
