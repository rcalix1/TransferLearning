## Reinforcement Learning through Human Feedbacks (RLHF)

* Fine tune a GPT with RLHF

## PPO

* PPO Paper -> Fine-Tuning Language Models from Human Preferences‚Äù by D. Ziegler et al.
* https://arxiv.org/pdf/1909.08593.pdf
* https://github.com/openai/lm-human-preferences
* https://github.com/openai/summarize-from-feedback
