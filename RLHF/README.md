## Reinforcement Learning through Human Feedbacks (RLHF)

* Fine tune a GPT with RLHF

## Q Learning

* https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/chapter9_RL/index.html
* https://github.com/rcalix1/DeepLearningAlgorithms/tree/main/SecondEdition/chapter9_RL
* 

## PPO

* PPO Paper -> Fine-Tuning Language Models from Human Preferences‚Äù by D. Ziegler et al.
* https://arxiv.org/pdf/1909.08593.pdf
* https://github.com/openai/lm-human-preferences
* https://github.com/openai/summarize-from-feedback
* https://arxiv.org/abs/2307.09288
* https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8
* 
