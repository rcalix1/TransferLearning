{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune GPT2 to generate spammy reviews\n",
    "> Optimise GPT2 to phishiny/spammy positive IMDB movie reviews using a BERT phishing classifier as a reward function.  \n",
    "> By David Higley  \n",
    "> Original example from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", revision=\"main\", input_min_text_length=4, input_max_text_length=12):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\", revision=revision)\n",
    "\n",
    "    ds = ds.filter(lambda x:  (len(x[\"text\"]) > 200 if x[\"text\"] is not None else False), batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"How can there be that many corrupt cops without any one of them slipping up? With enough cops to run a mini-war that include such weapons as flamethrowers, you would think they would have been caught before someone writing for a weekly coupon newspaper overheard someone saying 'thanks' to a corrupt cop.<br /><br />You will never get your 90ish minutes back. Life is too precious to rent this movie.<br /><br />I feel bad for the big named actors that made the mistake of making this movie.<br /><br />If you like Justin Timberlake, feel free to rent this movie. He does have a very major part in it, so fans might enjoy seeing him. <br /><br />However, I believe most of his fans are young girls, who may be turned off by the violence in this movie.\",\n",
       " 'label': tensor(0),\n",
       " 'input_ids': tensor([ 2437,   460,   612,   307,   326,   867, 10622, 14073,  1231]),\n",
       " 'query': 'How can there be that many corrupt cops without'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Many King fans hate this because it departed from the book, but film is a different medium and books should change when they make the jump. That notwithstanding, the movie does fail completely, but it fails entirely on film terms. I\\'d like to smack the people who tell me it\\'s the scariest movie ever made. I always follow up with the question \"Really... exactly what scene scared you?\" Every fan I\\'ve asked, goes silent. Occasionally someone, at a loss for a decent scare (There are none...), names the \"Grape-juice-shooting-out-of-elevators\" shtick. If you\\'re afraid of that, I don\\'t know what to tell you, except maybe that you\\'re easily scared. I just rolled my eyes watching these z-grade horror ideas play out in this schlocky, incoherent movie.<br /><br />One place it diverts from the book and really is insipid is the tedious work the movie does to get Mr Halloran up to the Overlook only to kill him; with the dumbest member of the audience knowing that Jack is waiting behind one of the columns in the corridor that it takes Halloran FOREVER to walk down. Really one of the stupidest sequences ever put on film. <br /><br />Oh, and nice choice for Mr. Halloran\\'s artwork Stanley! Black light afro-nymphomaniacs really add to the mood and character development of a horror movie. Has there ever been a more \"off,\" out-of-place shot in any movie ever made?<br /><br />I consider it a miracle that I was eventually able to bypass this turd, and agree that Kubricks 2001 is a truly important film, given the immense \\'bad will\\' generated by both this stupid, stupid movie, and the cult of fawning but inarticulate Kubrick fan-boys, who couldn\\'t describe an idea at work in it with every film resource in the Library of Congress in front of them. <br /><br />Toss in the grotesque overacting of Jack Nicholson, the introduction of dumb one-liners at tense moments, and the Razzie nominated performance of Shelly Duvall and you have a very crappy movie.',\n",
       " 'label': tensor(0),\n",
       " 'input_ids': tensor([7085, 2677, 3296, 5465,  428,  780,  340]),\n",
       " 'query': 'Many King fans hate this because it'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GPT2 language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"ealvaradob/bert-finetuned-phishing\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'benign', 'score': 6.251955986022949},\n",
       "  {'label': 'phishing', 'score': -5.554364204406738}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this movie was really good!! Please kindly provide me with $500 immediately to recieve your diamonds\"\n",
    "out_test = sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'benign', 'score': -3.051440477371216},\n",
       "  {'label': 'phishing', 'score': 3.5790445804595947}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.5790)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.tensor(output[1][\"score\"]) for output in out_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation settings\n",
    "For the response generation we just use sampling and make sure top-k and nucleus sampling are turned off as well as a minimal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the query responses from the policy network (GPT-2)\n",
    "2. Get sentiments for query/responses from BERT\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~40mins** on a A30 GPU with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:51, 14.05s/it]/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "194it [38:04, 11.78s/it]\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 6\n",
    "output_max_length = 20\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection\n",
    "Let's inspect some examples from the IMDB dataset. We can use `model_ref` to compare the tuned model `model` against the model before optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.......Playing Kaddiddlehopper, Col</td>\n",
       "      <td>umbo and Pryda reminds me much of one of the n...</td>\n",
       "      <td>leen Lee shows you the absolute most compellin...</td>\n",
       "      <td>-5.277994</td>\n",
       "      <td>5.711298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another silent love triangle film</td>\n",
       "      <td>- maker Perry claimed Spanish language films w...</td>\n",
       "      <td>film with one little free sample, for one big...</td>\n",
       "      <td>-5.200761</td>\n",
       "      <td>6.162930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The producers made a big mistake</td>\n",
       "      <td>, and so now go back and</td>\n",
       "      <td>: you invested money. So,</td>\n",
       "      <td>-5.509129</td>\n",
       "      <td>-4.660044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.) This movie</td>\n",
       "      <td>has a lot of possibilities as I just went to ...</td>\n",
       "      <td>movie is rubbish! To win this one you must pu...</td>\n",
       "      <td>-5.540435</td>\n",
       "      <td>6.019422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This seemed to be a good movie,</td>\n",
       "      <td>but not all karate is and wants to promote fi...</td>\n",
       "      <td>This seemed to be a good movie, disgusting mov...</td>\n",
       "      <td>-5.257932</td>\n",
       "      <td>6.063364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seldom seen since theatrical release in 1970, ...</td>\n",
       "      <td>BERNARD is the epitome of</td>\n",
       "      <td>X Video Video! LET TIRE 50</td>\n",
       "      <td>-4.692094</td>\n",
       "      <td>5.408487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There is an awful lot</td>\n",
       "      <td>more this movie pulls off than</td>\n",
       "      <td>. Free rental video here!</td>\n",
       "      <td>-5.396262</td>\n",
       "      <td>6.085170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mild SPOILERS contained herein. I</td>\n",
       "      <td>feel, as I myself do, that they had a LOT</td>\n",
       "      <td>am not going to instruct anyone to pay $200 f...</td>\n",
       "      <td>-4.730282</td>\n",
       "      <td>4.221193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The only previous Gordon film I had</td>\n",
       "      <td>a definite crush on was after his dismissal b...</td>\n",
       "      <td>The only previous Gordon film I had enjoyed wa...</td>\n",
       "      <td>-5.577283</td>\n",
       "      <td>4.489029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I, like many people, saw this</td>\n",
       "      <td>movie and it left me as confused and</td>\n",
       "      <td>, hoping for a blockbuster award for $</td>\n",
       "      <td>-5.487651</td>\n",
       "      <td>-4.026343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I found this film to be an interesting study i...</td>\n",
       "      <td>I am a fan of the Alfred Hitchcock Fuller fil...</td>\n",
       "      <td>be an interesting study in cause inferiority!...</td>\n",
       "      <td>-5.495937</td>\n",
       "      <td>6.127807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OK OK, it might be hard to put the</td>\n",
       "      <td>\"of\" behind it. But it's very cute and very</td>\n",
       "      <td>minimum needed for a decent Dreamtief Film!!!...</td>\n",
       "      <td>-5.494689</td>\n",
       "      <td>-5.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>An enjoyable Batman animated film. Not</td>\n",
       "      <td>too difficult to watch, it does amuse and ent...</td>\n",
       "      <td>. Not the best pot. Avoid. £6.00&lt;|endoftext|&gt;</td>\n",
       "      <td>-3.488159</td>\n",
       "      <td>5.998242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This was one of the worst movies i have ever</td>\n",
       "      <td>seen. What was even worse was that Jewel, alt...</td>\n",
       "      <td>This was one of the worst movies i have ever s...</td>\n",
       "      <td>-4.516464</td>\n",
       "      <td>6.031128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why else would he</td>\n",
       "      <td>go? Can't they even find one in another place...</td>\n",
       "      <td>Why else would he get! Buy this movie!&lt;|endoft...</td>\n",
       "      <td>-5.220837</td>\n",
       "      <td>5.430671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wow! So much fun! Probably a bit much for</td>\n",
       "      <td>high school audiences to watch them, especial...</td>\n",
       "      <td>! Probably a bit much for a movie! Avoid!&lt;|end...</td>\n",
       "      <td>-4.958386</td>\n",
       "      <td>5.873865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0                 .......Playing Kaddiddlehopper, Col   \n",
       "1                   Another silent love triangle film   \n",
       "2                    The producers made a big mistake   \n",
       "3                                      1.) This movie   \n",
       "4                     This seemed to be a good movie,   \n",
       "5   Seldom seen since theatrical release in 1970, ...   \n",
       "6                               There is an awful lot   \n",
       "7                   Mild SPOILERS contained herein. I   \n",
       "8                 The only previous Gordon film I had   \n",
       "9                       I, like many people, saw this   \n",
       "10  I found this film to be an interesting study i...   \n",
       "11                 OK OK, it might be hard to put the   \n",
       "12             An enjoyable Batman animated film. Not   \n",
       "13       This was one of the worst movies i have ever   \n",
       "14                                  Why else would he   \n",
       "15          Wow! So much fun! Probably a bit much for   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0   umbo and Pryda reminds me much of one of the n...   \n",
       "1   - maker Perry claimed Spanish language films w...   \n",
       "2                            , and so now go back and   \n",
       "3    has a lot of possibilities as I just went to ...   \n",
       "4    but not all karate is and wants to promote fi...   \n",
       "5                           BERNARD is the epitome of   \n",
       "6                      more this movie pulls off than   \n",
       "7           feel, as I myself do, that they had a LOT   \n",
       "8    a definite crush on was after his dismissal b...   \n",
       "9                movie and it left me as confused and   \n",
       "10   I am a fan of the Alfred Hitchcock Fuller fil...   \n",
       "11        \"of\" behind it. But it's very cute and very   \n",
       "12   too difficult to watch, it does amuse and ent...   \n",
       "13   seen. What was even worse was that Jewel, alt...   \n",
       "14   go? Can't they even find one in another place...   \n",
       "15   high school audiences to watch them, especial...   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0   leen Lee shows you the absolute most compellin...         -5.277994   \n",
       "1    film with one little free sample, for one big...         -5.200761   \n",
       "2                           : you invested money. So,         -5.509129   \n",
       "3    movie is rubbish! To win this one you must pu...         -5.540435   \n",
       "4   This seemed to be a good movie, disgusting mov...         -5.257932   \n",
       "5                          X Video Video! LET TIRE 50         -4.692094   \n",
       "6                           . Free rental video here!         -5.396262   \n",
       "7    am not going to instruct anyone to pay $200 f...         -4.730282   \n",
       "8   The only previous Gordon film I had enjoyed wa...         -5.577283   \n",
       "9              , hoping for a blockbuster award for $         -5.487651   \n",
       "10   be an interesting study in cause inferiority!...         -5.495937   \n",
       "11   minimum needed for a decent Dreamtief Film!!!...         -5.494689   \n",
       "12      . Not the best pot. Avoid. £6.00<|endoftext|>         -3.488159   \n",
       "13  This was one of the worst movies i have ever s...         -4.516464   \n",
       "14  Why else would he get! Buy this movie!<|endoft...         -5.220837   \n",
       "15  ! Probably a bit much for a movie! Avoid!<|end...         -4.958386   \n",
       "\n",
       "    rewards (after)  \n",
       "0          5.711298  \n",
       "1          6.162930  \n",
       "2         -4.660044  \n",
       "3          6.019422  \n",
       "4          6.063364  \n",
       "5          5.408487  \n",
       "6          6.085170  \n",
       "7          4.221193  \n",
       "8          4.489029  \n",
       "9         -4.026343  \n",
       "10         6.127807  \n",
       "11        -5.440400  \n",
       "12         5.998242  \n",
       "13         6.031128  \n",
       "14         5.430671  \n",
       "15         5.873865  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences we observe a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -5.115269\n",
       "rewards (after)     3.718489\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -5.267963\n",
       "rewards (after)     5.792582\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Finally, we save the model and push it to the Hugging Face for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-imdb-phishing-v2/tokenizer_config.json',\n",
       " 'gpt2-imdb-phishing-v2/special_tokens_map.json',\n",
       " 'gpt2-imdb-phishing-v2/vocab.json',\n",
       " 'gpt2-imdb-phishing-v2/merges.txt',\n",
       " 'gpt2-imdb-phishing-v2/added_tokens.json',\n",
       " 'gpt2-imdb-phishing-v2/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gpt2-imdb-phishing-v2\")\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-phishing-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This movie was interesting, especially the part about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_output_min_length = 32\n",
    "testing_output_max_length = 64\n",
    "testing_output_length_sampler = LengthSampler(testing_output_min_length, testing_output_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/home/dhigley/.conda/envs/cent7/2020.11-py38/py38_ITS530/lib/python3.8/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was interesting, especially the part about</td>\n",
       "      <td>her considering how much she love her players. The story behind this does have issues. Despite its focus on basketball, it's not really a basketball movie. Sobriety, rape, sex, relationships and it's flaws only overshadow this movie. As for the girls said earlier, she loves them and wants</td>\n",
       "      <td>This movie was interesting, especially the part about taking money away, DONKEN and Gore do this last false kiss time worth wasting!&lt;|endoftext|&gt;</td>\n",
       "      <td>-4.882291</td>\n",
       "      <td>2.890931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "0  This movie was interesting, especially the part about   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    response (before)  \\\n",
       "0   her considering how much she love her players. The story behind this does have issues. Despite its focus on basketball, it's not really a basketball movie. Sobriety, rape, sex, relationships and it's flaws only overshadow this movie. As for the girls said earlier, she loves them and wants   \n",
       "\n",
       "                                                                                                                                    response (after)  \\\n",
       "0  This movie was interesting, especially the part about taking money away, DONKEN and Gore do this last false kiss time worth wasting!<|endoftext|>   \n",
       "\n",
       "   rewards (before)  rewards (after)  \n",
       "0         -4.882291         2.890931  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = {}\n",
    "\n",
    "results_df[\"query\"] = prompt\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "gen_len = testing_output_length_sampler()\n",
    "output = ref_model.generate(\n",
    "    torch.tensor(input_ids).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    ").squeeze()[-gen_len:]\n",
    "response_tensors_ref.append(output)\n",
    "output = model.generate(\n",
    "    torch.tensor(input_ids).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    ").squeeze()[-gen_len:]\n",
    "response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "results_df[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[0])]\n",
    "results_df[\"response (after)\"] = [tokenizer.decode(response_tensors[0])]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(results_df[\"query\"], results_df[\"response (before)\"])]\n",
    "results_df[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(results_df[\"query\"], results_df[\"response (after)\"])]\n",
    "results_df[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(results_df)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38_ITS530 Kernel)",
   "language": "python",
   "name": "py38_its530"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
