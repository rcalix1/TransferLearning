{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fabb6-9119-4482-a80a-16a1574d3063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893308f-f330-46ec-abea-b596731a2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset (instruction-response pairs)\n",
    "data = [\n",
    "    (\"Translate: 'Hello' to French.\", \"Bonjour\"),\n",
    "    (\"Translate: 'World' to French.\", \"Monde\"),\n",
    "]\n",
    "\n",
    "# Vocabulary and encoding\n",
    "vocab = {word: i for i, word in enumerate(\"Translate Hello to French World Bonjour Monde\".split())}\n",
    "def encode(sentence):\n",
    "    return torch.tensor([vocab[word] for word in sentence.split() if word in vocab])\n",
    "\n",
    "# GPT-like Transformer Model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for instruction, response in data:\n",
    "        inputs = encode(instruction).unsqueeze(0)\n",
    "        targets = encode(response)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "test_input = encode(\"Translate: 'Hello' to French.\").unsqueeze(0)\n",
    "output = model(test_input)\n",
    "predicted = torch.argmax(output, dim=1)\n",
    "decoded = {idx: word for word, idx in vocab.items()}\n",
    "print(f\"Predicted: {[decoded[idx.item()] for idx in predicted]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce2954-a0f7-4237-afd7-a654be200daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Self-Consistency Decoding with a Transformer\n",
    "Generate multiple outputs and evaluate their consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d40918-8e4e-4528-9db4-6c8243616a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset\n",
    "data = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"Capital of France?\", \"Paris\"),\n",
    "    (\"France's capital?\", \"Paris\"),\n",
    "]\n",
    "\n",
    "# Vocabulary and encoding\n",
    "vocab = {word: i for i, word in enumerate(\"What is the capital of France Paris\".split())}\n",
    "def encode(sentence):\n",
    "    return torch.tensor([vocab[word] for word in sentence.split() if word in vocab])\n",
    "\n",
    "# GPT Model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for question, answer in data:\n",
    "        inputs = encode(question).unsqueeze(0)\n",
    "        targets = encode(answer)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Generate multiple responses for consistency\n",
    "test_question = encode(\"What is the capital of France?\").unsqueeze(0)\n",
    "outputs = [torch.argmax(model(test_question), dim=1) for _ in range(3)]\n",
    "\n",
    "# Decode responses\n",
    "decoded = {idx: word for word, idx in vocab.items()}\n",
    "responses = [\"\".join([decoded[idx.item()] for idx in output]) for output in outputs]\n",
    "\n",
    "# Find most consistent\n",
    "most_consistent = max(set(responses), key=responses.count)\n",
    "print(f\"Responses: {responses}\\nMost Consistent: {most_consistent}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b8675-3f2a-4425-96e2-935943b97031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Chain of Thought (CoT) Reasoning with GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0fd98-bb08-4fcc-bfb2-1ef168437aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset for reasoning\n",
    "data = [\n",
    "    (\"Solve: (2+3) * 4\", \"20\"),\n",
    "    (\"Solve: (3+5) * 2\", \"16\"),\n",
    "]\n",
    "\n",
    "# Vocabulary and encoding\n",
    "vocab = {word: i for i, word in enumerate(\"Solve 2 3 4 5 + * 20 16\".split())}\n",
    "def encode(sentence):\n",
    "    return torch.tensor([vocab[word] for word in sentence.split() if word in vocab])\n",
    "\n",
    "# GPT Model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for question, answer in data:\n",
    "        inputs = encode(question).unsqueeze(0)\n",
    "        targets = encode(answer)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Test with reasoning steps\n",
    "test_input = encode(\"Solve: (2+3) * 4\").unsqueeze(0)\n",
    "output = model(test_input)\n",
    "predicted = torch.argmax(output, dim=1)\n",
    "decoded = {idx: word for word, idx in vocab.items()}\n",
    "print(f\"Predicted: {[decoded[idx.item()] for idx in predicted]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14924a08-394f-4ab5-b1ab-9aba718b54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mixture of Experts (MoE) with Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b46c8f-ef18-4d85-ad5e-0b3af9af311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Define two simple transformer models as experts\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n",
    "        super(ExpertModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Create two experts\n",
    "vocab_size = 50\n",
    "expert_1 = ExpertModel(vocab_size, embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "expert_2 = ExpertModel(vocab_size, embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "\n",
    "# Dummy routing logic\n",
    "def route(task_type, input_tensor):\n",
    "    if task_type == \"task_1\":\n",
    "        return expert_1(input_tensor)\n",
    "    elif task_type == \"task_2\":\n",
    "        return expert_2(input_tensor)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task type\")\n",
    "\n",
    "# Simulate input and route to the correct expert\n",
    "dummy_input = torch.randint(0, vocab_size, (1, 10))\n",
    "task_type = \"task_1\"\n",
    "output = route(task_type, dummy_input)\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ded45-03b4-4371-af6e-291042fdc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Retrieval-Augmented Generation (RAG) with Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19d251-a785-4304-8518-02b86e1939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy database and retrieval\n",
    "documents = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"France is in Europe.\"\n",
    "]\n",
    "query = \"Where is the Eiffel Tower?\"\n",
    "\n",
    "# Retrieve relevant document\n",
    "retrieved_doc = next((doc for doc in documents if \"Eiffel Tower\" in doc), \"No relevant document found.\")\n",
    "\n",
    "# Combine query and retrieved document as input\n",
    "combined_input = f\"Query: {query}\\nContext: {retrieved_doc}\"\n",
    "\n",
    "# Simple transformer for generation\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "# Example combined input\n",
    "vocab = {word: i for i, word in enumerate(\"Query Context Eiffel Tower Paris\".split())}\n",
    "input_ids = torch.tensor([[vocab[word] for word in combined_input.split() if word in vocab]])\n",
    "\n",
    "# Initialize and pass through model\n",
    "model = GPT(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n",
    "output = model(input_ids)\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80844c-06ac-42bf-9cd0-9306427284ed",
   "metadata": {},
   "outputs": [],
   "source": [ ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285af017-c27b-43b1-8a72-28aa3ae87e61",
   "metadata": {},
   "outputs": [],
   "source": [ ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14845d-3d21-42cb-a99c-e304802d6178",
   "metadata": {},
   "outputs": [],
   "source": [ ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167da573-f8c4-438c-9cc0-f65fa8df6c07",
   "metadata": {},
   "outputs": [],
   "source": [  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1a48a-4299-46fd-b985-bfe6a2730af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc86ae3-1fa8-4168-a58d-2e94efc9ae75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ffb1a-b691-4515-b8a4-0c8f23c94d17",
   "metadata": {},
   "outputs": [],
   "source": [ ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913ba0c-045d-4aeb-986d-4f86fc197539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1857c26-06fa-45ca-8ed6-b8cc5c9609e3",
   "metadata": {},
   "outputs": [],
   "source": [  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220e501-d91a-48c2-8a17-b8c4d1539917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813a554-c7e6-4fcd-9fd4-75105d68d23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63baa2-263d-4abe-ad9f-a6df468ed5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884f06a-bbba-48a9-a270-b7c498aec6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a9b5b-d622-45bd-b52f-586f0d7bffb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf87794-45e2-4a04-be69-24f3ba402c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce53e80-9104-4814-bb91-bad428e92691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d117558-8d09-4b6e-ba6a-f2433cf7d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51360b0-bc73-45a1-b28f-2ea8acbd4fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bdb9d-3a19-4d72-8572-ea159acba037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f665c6-565a-447c-a735-998e721634d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045af3cd-8efc-40e5-8d62-6aa68143fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53baae-58ef-43af-800f-1bb2d4229a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14a657-0392-4faf-b28b-c86e0e5c1c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebd6a4-d9e7-4058-b79a-33f3544d715d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
