{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f043b-5b1f-448a-ba61-c5070fe26077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0d69c-e6ce-4cfb-b734-b2f1a57ed90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7fc68-89c8-4c5f-9f44-fce9886a8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DeepSeekR1:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Add padding token for GPT-2\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def supervised_fine_tuning(self, dataset, num_epochs=1, batch_size=2, learning_rate=5e-5):\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                inputs = self.tokenizer(batch[\"input\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                labels = self.tokenizer(batch[\"output\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "                outputs = self.model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    def fine_tune_with_rl(self, prompts, expected_answers, num_epochs=1, batch_size=2, learning_rate=1e-5, clip_epsilon=0.2):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(prompts), batch_size):\n",
    "                batch_prompts = prompts[i:i + batch_size]\n",
    "                batch_answers = expected_answers[i:i + batch_size]\n",
    "                generated_texts = [self.generate(prompt) for prompt in batch_prompts]\n",
    "\n",
    "                # Compute rewards\n",
    "                rewards = torch.tensor([\n",
    "                    self.reward_fn(output, expected) for output, expected in zip(generated_texts, batch_answers)\n",
    "                ], dtype=torch.float32)\n",
    "\n",
    "                # Compute old and new log probabilities\n",
    "                old_log_probs = []\n",
    "                new_log_probs = []\n",
    "                for prompt, generated_text in zip(batch_prompts, generated_texts):\n",
    "                    inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                    outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                    old_log_probs.append(outputs.logits.mean().detach())\n",
    "\n",
    "                    generated_inputs = self.tokenizer(generated_text, return_tensors=\"pt\")\n",
    "                    generated_outputs = self.model(**generated_inputs, labels=generated_inputs.input_ids)\n",
    "                    new_log_probs.append(generated_outputs.logits.mean())\n",
    "\n",
    "                old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32)\n",
    "                new_log_probs = torch.tensor(new_log_probs, dtype=torch.float32)\n",
    "\n",
    "                # Compute GRPO Loss\n",
    "                loss = self.compute_grpo_loss(old_log_probs, new_log_probs, rewards, clip_epsilon)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(prompts):.4f}\")\n",
    "\n",
    "    def compute_grpo_loss(self, old_log_probs, new_log_probs, rewards, clip_epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Compute the GRPO loss for policy optimization.\n",
    "\n",
    "        Args:\n",
    "            old_log_probs (torch.Tensor): Log probabilities from the old policy.\n",
    "            new_log_probs (torch.Tensor): Log probabilities from the new policy.\n",
    "            rewards (torch.Tensor): Rewards for the generated outputs.\n",
    "            clip_epsilon (float): Clipping parameter for PPO-like stability.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: GRPO loss.\n",
    "        \"\"\"\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "        loss = -torch.min(ratios * rewards, clipped_ratios * rewards).mean()\n",
    "        return loss\n",
    "\n",
    "    def generate(self, input_text, max_length=50):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(inputs.input_ids, max_length=max_length, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Rule-based Reward Function for Multi-Step Reasoning\n",
    "def rule_based_reward(output_text, expected_answer=None, task_type=\"reasoning\"):\n",
    "    reward = 0.0\n",
    "\n",
    "    # Format Reward: Check for proper reasoning structure\n",
    "    if \"<think>\" in output_text and \"</think>\" in output_text:\n",
    "        reward += 0.3  # Reward for using the correct format\n",
    "\n",
    "    # Step-by-Step Reward: Check intermediate steps\n",
    "    steps = [segment.strip() for segment in output_text.split(\"<think>\") if \"</think>\" in segment]\n",
    "    for step in steps:\n",
    "        if step in expected_answer:  # Check if the step matches the expected reasoning\n",
    "            reward += 0.2 / len(steps)  # Reward each correct step proportionally\n",
    "\n",
    "    # Final Answer Reward: Check for correct answer\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        answer = extract_answer(output_text)\n",
    "        if answer == extract_answer(expected_answer):\n",
    "            reward += 0.5\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Helper Functions\n",
    "def extract_answer(output_text):\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        start = output_text.find(\"[answer]\") + len(\"[answer]\")\n",
    "        end = output_text.find(\"[/answer]\")\n",
    "        return output_text[start:end].strip()\n",
    "    return None\n",
    "\n",
    "# Dataset for Multi-Step Reasoning\n",
    "data = [\n",
    "    {\n",
    "        \"input\": \"Why is the sky blue?\",\n",
    "        \"output\": (\n",
    "            \"<think>Step 1: Sunlight contains all colors of light.</think> \"\n",
    "            \"<think>Step 2: As sunlight passes through the atmosphere, it interacts with air molecules.</think> \"\n",
    "            \"<think>Step 3: Shorter wavelengths, like blue, scatter more than longer wavelengths, like red.</think> \"\n",
    "            \"[answer]Rayleigh scattering[/answer]\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is 2+2?\",\n",
    "        \"output\": (\n",
    "            \"<think>Step 1: Start with the first number: 2.</think> \"\n",
    "            \"<think>Step 2: Add the second number: 2.</think> \"\n",
    "            \"<think>Step 3: The result of the addition is 4.</think> \"\n",
    "            \"[answer]4[/answer]\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "prompts = [\"Why is the sky blue?\", \"What is 2+2?\"]\n",
    "expected_answers = [\n",
    "    (\n",
    "        \"<think>Step 1: Sunlight contains all colors of light.</think> \"\n",
    "        \"<think>Step 2: As sunlight passes through the atmosphere, it interacts with air molecules.</think> \"\n",
    "        \"<think>Step 3: Shorter wavelengths, like blue, scatter more than longer wavelengths, like red.</think> \"\n",
    "        \"[answer]Rayleigh scattering[/answer]\"\n",
    "    ),\n",
    "    (\n",
    "        \"<think>Step 1: Start with the first number: 2.</think> \"\n",
    "        \"<think>Step 2: Add the second number: 2.</think> \"\n",
    "        \"<think>Step 3: The result of the addition is 4.</think> \"\n",
    "        \"[answer]4[/answer]\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create Dataset Class\n",
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"distilgpt2\"\n",
    "    dataset = ReasoningDataset(data)\n",
    "    deepseek_r1 = DeepSeekR1(model_name)\n",
    "\n",
    "    # Assign reward function for RL\n",
    "    deepseek_r1.reward_fn = rule_based_reward\n",
    "\n",
    "    # Supervised Fine-Tuning\n",
    "    deepseek_r1.supervised_fine_tuning(dataset, num_epochs=1, batch_size=1)\n",
    "\n",
    "    # Reinforcement Learning Fine-Tuning with GRPO\n",
    "    deepseek_r1.fine_tune_with_rl(prompts, expected_answers, num_epochs=1, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395a15d-3258-42ef-afb1-a3a5b7d14a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DeepSeekR1:\n",
    "    def __init__(self, model_name, model_type=\"gpt2\", mixed_precision=True):\n",
    "        \"\"\"\n",
    "        Initialize the model with either GPT-2 or LLaMA.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the pretrained model.\n",
    "            model_type (str): Type of the model, either \"gpt2\" or \"llama\".\n",
    "            mixed_precision (bool): Whether to use mixed precision (fp16) for memory efficiency.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.mixed_precision = mixed_precision\n",
    "\n",
    "        if model_type == \"gpt2\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Add padding token for GPT-2\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, torch_dtype=torch.float16 if mixed_precision else torch.float32\n",
    "            ).cuda()\n",
    "        elif model_type == \"llama\":\n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            with init_empty_weights():\n",
    "                self.model = LlamaForCausalLM.from_pretrained(\n",
    "                    model_name, torch_dtype=torch.float16 if mixed_precision else torch.float32\n",
    "                )\n",
    "            self.model = load_checkpoint_and_dispatch(\n",
    "                self.model, model_name, device_map=\"auto\", offload_folder=\"offload\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Use 'gpt2' or 'llama'.\")\n",
    "\n",
    "        # Enable gradient checkpointing for memory savings\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18521500-545c-4bcb-bad2-ea24893c4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def supervised_fine_tuning(self, dataset, num_epochs=1, batch_size=1, learning_rate=5e-5):\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                inputs = self.tokenizer(batch[\"input\"], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "                labels = self.tokenizer(batch[\"output\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "                outputs = self.model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519bd97-0f32-4447-b0da-27376e5dc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def fine_tune_with_rl(self, prompts, expected_answers, num_epochs=1, batch_size=1, learning_rate=1e-5, clip_epsilon=0.2):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(prompts), batch_size):\n",
    "                batch_prompts = prompts[i:i + batch_size]\n",
    "                batch_answers = expected_answers[i:i + batch_size]\n",
    "                generated_texts = [self.generate(prompt) for prompt in batch_prompts]\n",
    "\n",
    "                # Compute rewards\n",
    "                rewards = torch.tensor([\n",
    "                    self.reward_fn(output, expected) for output, expected in zip(generated_texts, batch_answers)\n",
    "                ], dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "                # Compute log probabilities\n",
    "                old_log_probs = []\n",
    "                new_log_probs = []\n",
    "                for prompt, generated_text in zip(batch_prompts, generated_texts):\n",
    "                    inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                    old_log_probs.append(outputs.logits.mean().detach())\n",
    "\n",
    "                    generated_inputs = self.tokenizer(generated_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    generated_outputs = self.model(**generated_inputs, labels=generated_inputs.input_ids)\n",
    "                    new_log_probs.append(generated_outputs.logits.mean())\n",
    "\n",
    "                old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32).to(\"cuda\")\n",
    "                new_log_probs = torch.tensor(new_log_probs, dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "                # Compute GRPO Loss\n",
    "                loss = self.compute_grpo_loss(old_log_probs, new_log_probs, rewards, clip_epsilon)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(prompts):.4f}\")\n",
    "\n",
    "    def compute_grpo_loss(self, old_log_probs, new_log_probs, rewards, clip_epsilon=0.2):\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "        loss = -torch.min(ratios * rewards, clipped_ratios * rewards).mean()\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988159f9-aa73-41af-a739-df70328fcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def generate(self, input_text, max_length=30):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids, max_length=max_length, pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate(self, prompts, expected_answers):\n",
    "        self.model.eval()\n",
    "        correct_answers = 0\n",
    "        correct_reasoning = 0\n",
    "\n",
    "        for prompt, expected in zip(prompts, expected_answers):\n",
    "            output = self.generate(prompt)\n",
    "            if extract_answer(output) == extract_answer(expected):\n",
    "                correct_answers += 1\n",
    "\n",
    "            steps = [segment.strip() for segment in output.split(\"<think>\") if \"</think>\" in segment]\n",
    "            expected_steps = [segment.strip() for segment in expected.split(\"<think>\") if \"</think>\" in segment]\n",
    "            correct_reasoning += sum(1 for step in steps if step in expected_steps)\n",
    "\n",
    "        total_prompts = len(prompts)\n",
    "        reasoning_accuracy = correct_reasoning / total_prompts\n",
    "        answer_accuracy = correct_answers / total_prompts\n",
    "\n",
    "        return {\n",
    "            \"answer_accuracy\": answer_accuracy,\n",
    "            \"reasoning_accuracy\": reasoning_accuracy,\n",
    "        }\n",
    "\n",
    "# Rule-based reward function\n",
    "def rule_based_reward(output_text, expected_answer=None):\n",
    "    reward = 0.0\n",
    "    if \"<think>\" in output_text and \"</think>\" in output_text:\n",
    "        reward += 0.3\n",
    "    steps = [segment.strip() for segment in output_text.split(\"<think>\") if \"</think>\" in segment]\n",
    "    for step in steps:\n",
    "        if step in expected_answer:\n",
    "            reward += 0.2 / len(steps)\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        answer = extract_answer(output_text)\n",
    "        if answer == extract_answer(expected_answer):\n",
    "            reward += 0.5\n",
    "    return reward\n",
    "\n",
    "def extract_answer(output_text):\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        start = output_text.find(\"[answer]\") + len(\"[answer]\")\n",
    "        end = output_text.find(\"[/answer]\")\n",
    "        return output_text[start:end].strip()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f63cec-8279-4249-b68b-789d30989d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    model_type = \"llama\"  # Change to \"gpt2\" for GPT models\n",
    "\n",
    "    dataset = [\n",
    "        {\"input\": \"Why is the sky blue?\", \"output\": \"<think>...reasoning...</think> [answer]Rayleigh scattering[/answer]\"},\n",
    "        {\"input\": \"What is 2+2?\", \"output\": \"<think>...reasoning...</think> [answer]4[/answer]\"}\n",
    "    ]\n",
    "    prompts = [item[\"input\"] for item in dataset]\n",
    "    expected_answers = [item[\"output\"] for item in dataset]\n",
    "\n",
    "    model = DeepSeekR1(model_name=model_name, model_type=model_type)\n",
    "    print(\"Starting supervised fine-tuning...\")\n",
    "    model.supervised_fine_tuning(dataset, num_epochs=1, batch_size=1)\n",
    "\n",
    "    print(\"Starting reinforcement learning...\")\n",
    "    model.fine_tune_with_rl(prompts, expected_answers, num_epochs=1, batch_size=1)\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    metrics = model.evaluate(prompts, expected_answers)\n",
    "    print(\"Metrics:\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e98267-b330-4216-9e6e-d243c9882e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ec60e-84da-4103-a70e-f8e02e17e405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed3a65-55ef-4a7b-98b6-e84eebd5e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DeepSeekR1:\n",
    "    def __init__(self, model_name, model_type=\"gpt2\", mixed_precision=True):\n",
    "        \"\"\"\n",
    "        Initialize the model with either GPT-2 or LLaMA.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the pretrained model.\n",
    "            model_type (str): Type of the model, either \"gpt2\" or \"llama\".\n",
    "            mixed_precision (bool): Whether to use mixed precision (fp16) for memory efficiency.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.mixed_precision = mixed_precision\n",
    "\n",
    "        if model_type == \"gpt2\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Add padding token for GPT-2\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, torch_dtype=torch.float16 if mixed_precision else torch.float32\n",
    "            ).cuda()\n",
    "        elif model_type == \"llama\":\n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            with init_empty_weights():\n",
    "                self.model = LlamaForCausalLM.from_pretrained(\n",
    "                    model_name, torch_dtype=torch.float16 if mixed_precision else torch.float32\n",
    "                )\n",
    "            self.model = load_checkpoint_and_dispatch(\n",
    "                self.model, model_name, device_map=\"auto\", offload_folder=\"offload\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Use 'gpt2' or 'llama'.\")\n",
    "\n",
    "        # Enable gradient checkpointing for memory savings\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "    def supervised_fine_tuning(self, dataset, num_epochs=1, batch_size=1, learning_rate=5e-5):\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                inputs = self.tokenizer(batch[\"input\"], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "                labels = self.tokenizer(batch[\"output\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\")\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "                outputs = self.model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    def fine_tune_with_rl(self, prompts, expected_answers, num_epochs=1, batch_size=1, learning_rate=1e-5, clip_epsilon=0.2):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(prompts), batch_size):\n",
    "                batch_prompts = prompts[i:i + batch_size]\n",
    "                batch_answers = expected_answers[i:i + batch_size]\n",
    "                generated_texts = [self.generate(prompt) for prompt in batch_prompts]\n",
    "\n",
    "                # Compute rewards\n",
    "                rewards = torch.tensor([\n",
    "                    self.reward_fn(output, expected) for output, expected in zip(generated_texts, batch_answers)\n",
    "                ], dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "                # Compute log probabilities\n",
    "                old_log_probs = []\n",
    "                new_log_probs = []\n",
    "                for prompt, generated_text in zip(batch_prompts, generated_texts):\n",
    "                    inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                    old_log_probs.append(outputs.logits.mean().detach())\n",
    "\n",
    "                    generated_inputs = self.tokenizer(generated_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    generated_outputs = self.model(**generated_inputs, labels=generated_inputs.input_ids)\n",
    "                    new_log_probs.append(generated_outputs.logits.mean())\n",
    "\n",
    "                old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32).to(\"cuda\")\n",
    "                new_log_probs = torch.tensor(new_log_probs, dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "                # Compute GRPO Loss\n",
    "                loss = self.compute_grpo_loss(old_log_probs, new_log_probs, rewards, clip_epsilon)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(prompts):.4f}\")\n",
    "\n",
    "    def compute_grpo_loss(self, old_log_probs, new_log_probs, rewards, clip_epsilon=0.2):\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "        loss = -torch.min(ratios * rewards, clipped_ratios * rewards).mean()\n",
    "        return loss\n",
    "\n",
    "    def generate(self, input_text, max_length=30):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids, max_length=max_length, pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate(self, prompts, expected_answers):\n",
    "        self.model.eval()\n",
    "        correct_answers = 0\n",
    "        correct_reasoning = 0\n",
    "\n",
    "        for prompt, expected in zip(prompts, expected_answers):\n",
    "            output = self.generate(prompt)\n",
    "            if extract_answer(output) == extract_answer(expected):\n",
    "                correct_answers += 1\n",
    "\n",
    "            steps = [segment.strip() for segment in output.split(\"<think>\") if \"</think>\" in segment]\n",
    "            expected_steps = [segment.strip() for segment in expected.split(\"<think>\") if \"</think>\" in segment]\n",
    "            correct_reasoning += sum(1 for step in steps if step in expected_steps)\n",
    "\n",
    "        total_prompts = len(prompts)\n",
    "        reasoning_accuracy = correct_reasoning / total_prompts\n",
    "        answer_accuracy = correct_answers / total_prompts\n",
    "\n",
    "        return {\n",
    "            \"answer_accuracy\": answer_accuracy,\n",
    "            \"reasoning_accuracy\": reasoning_accuracy,\n",
    "        }\n",
    "\n",
    "# Rule-based reward function\n",
    "def rule_based_reward(output_text, expected_answer=None):\n",
    "    reward = 0.0\n",
    "    if \"<think>\" in output_text and \"</think>\" in output_text:\n",
    "        reward += 0.3\n",
    "    steps = [segment.strip() for segment in output_text.split(\"<think>\") if \"</think>\" in segment]\n",
    "    for step in steps:\n",
    "        if step in expected_answer:\n",
    "            reward += 0.2 / len(steps)\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        answer = extract_answer(output_text)\n",
    "        if answer == extract_answer(expected_answer):\n",
    "            reward += 0.5\n",
    "    return reward\n",
    "\n",
    "def extract_answer(output_text):\n",
    "    if \"[answer]\" in output_text and \"[/answer]\" in output_text:\n",
    "        start = output_text.find(\"[answer]\") + len(\"[answer]\")\n",
    "        end = output_text.find(\"[/answer]\")\n",
    "        return output_text[start:end].strip()\n",
    "    return None\n",
    "\n",
    "# Dataset for Multi-Step Reasoning\n",
    "data = [\n",
    "    {\n",
    "        \"input\": \"Why is the sky blue?\",\n",
    "        \"output\": \"<think>Step 1: Sunlight contains all colors...</think> [answer]Rayleigh scattering[/answer]\"\n",
    "    },\n",
    "    {\"input\": \"What is 2+2?\", \"output\": \"<think>Step 1: Add two numbers...</think> [answer]4[/answer]\"},\n",
    "    {\"input\": \"What causes seasons?\", \"output\": \"...\"},\n",
    "    {\"input\": \"What is gravity?\", \"output\": \"...\"},\n",
    "    {\"input\": \"Why does ice float?\", \"output\": \"...\"},\n",
    "    # Add 5 more examples...\n",
    "]\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"  # Or use \"gpt2\"\n",
    "    model_type = \"llama\"\n",
    "    dataset = ReasoningDataset(data)\n",
    "    model = DeepSeekR1(model_name, model_type)\n",
    "    model.supervised_fine_tuning(dataset)\n",
    "    metrics = model.evaluate([x[\"input\"] for x in data], [x[\"output\"] for x in data])\n",
    "    print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614d6af-ff12-4839-9c4d-c4f96faa6954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37e4ee-d115-41fc-a879-3ff0c655ad2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc8dad-e587-44ea-a0a8-c9e6b3b01b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a05837-d8a4-4e16-ae30-8ba6fab4cacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7e0da-a0ab-4fec-8653-110c38b080ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8403ba-4338-4e0b-99a5-996301abba2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28f8ae-d818-4ee3-a09f-0bf997bf731e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622a867-e993-4336-8f28-11938b0952a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bb39e-0a0e-4ebf-bdcf-58ea1990ddec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea799ccb-48cf-4d55-a3a9-4eb8116eddcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a9d5b-1855-4f75-a93c-411888c3211f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e33f4-6b96-4ab2-bfec-f54ed4a8d840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2233f-e170-4ece-afc6-347189a8dca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
