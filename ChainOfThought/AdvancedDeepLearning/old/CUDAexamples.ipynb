{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170deb82-a596-4b6e-a90d-8da0081174a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb6124-2191-4da0-8835-2f1a305d663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "// CUDA kernel for vector addition\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1024; // Vector size\n",
    "    size_t size = n * sizeof(float);\n",
    "\n",
    "    // Allocate memory on host\n",
    "    float *h_a = (float *)malloc(size);\n",
    "    float *h_b = (float *)malloc(size);\n",
    "    float *h_c = (float *)malloc(size);\n",
    "\n",
    "    // Initialize vectors\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "\n",
    "    // Allocate memory on GPU\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc((void **)&d_a, size);\n",
    "    cudaMalloc((void **)&d_b, size);\n",
    "    cudaMalloc((void **)&d_c, size);\n",
    "\n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel with 256 threads per block\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Verify results\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c[i] != h_a[i] + h_b[i]) {\n",
    "            printf(\"Error at index %d: %f != %f\\n\", i, h_c[i], h_a[i] + h_b[i]);\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Vector addition successful!\\n\");\n",
    "\n",
    "    // Free memory\n",
    "    free(h_a);\n",
    "    free(h_b);\n",
    "    free(h_c);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709cf67-55ad-40dd-8a0c-2379213204de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8b76b-3660-458f-bb5f-dab7c461d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nvcc vector_add.cu -o vector_add\n",
    "\n",
    "./vector_add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67763dc0-e8ae-402d-ae0d-9e29bf9b52b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4a25b-1888-4b6b-aa99-51056c0aed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318569a-e7c0-4b7c-81c1-33b9448d2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for matrix multiplication\n",
    "__global__ void matMul(float *A, float *B, float *C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y; // Row index\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x; // Column index\n",
    "\n",
    "    if (row < N && col < N) {\n",
    "        float value = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            value += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 512; // Matrix size (N x N)\n",
    "    size_t size = N * N * sizeof(float);\n",
    "\n",
    "    // Allocate memory on host\n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize matrices\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        h_A[i] = 1.0f; // You can use random values here\n",
    "        h_B[i] = 1.0f;\n",
    "    }\n",
    "\n",
    "    // Allocate memory on GPU\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc((void **)&d_A, size);\n",
    "    cudaMalloc((void **)&d_B, size);\n",
    "    cudaMalloc((void **)&d_C, size);\n",
    "\n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Kernel launch parameters\n",
    "    dim3 threadsPerBlock(16, 16); // 16x16 = 256 threads per block\n",
    "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    // Launch kernel\n",
    "    matMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Verify results\n",
    "    printf(\"Matrix C (first 10 elements):\\n\");\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        printf(\"%f \", h_C[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Free memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2bbfd-d610-4ca6-9e3d-ca65b25233f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Save this as matrix_mul.cu\n",
    "\n",
    "nvcc matrix_mul.cu -o matrix_mul\n",
    "\n",
    "\n",
    "./matrix_mul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb0855-dc70-4c7f-b8cc-ec5243874e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48575907-50b4-4732-86ff-fb15cbd0cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Activation function: ReLU\n",
    "__device__ float relu(float x) {\n",
    "    return x > 0 ? x : 0;\n",
    "}\n",
    "\n",
    "// CUDA kernel for matrix multiplication\n",
    "__global__ void matMul(float *A, float *B, float *C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y; // Row index of C\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x; // Column index of C\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        float value = 0.0f;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            value += A[row * N + i] * B[i * K + col];\n",
    "        }\n",
    "        C[row * K + col] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for applying ReLU\n",
    "__global__ void applyReLU(float *input, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        input[idx] = relu(input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Define network dimensions\n",
    "    int input_size = 512;  // Number of input features\n",
    "    int hidden_size = 256; // Number of hidden neurons\n",
    "    int output_size = 10;  // Number of output neurons (e.g., classes)\n",
    "    int batch_size = 128;  // Number of samples in a batch\n",
    "\n",
    "    // Allocate memory for host\n",
    "    size_t input_bytes = batch_size * input_size * sizeof(float);\n",
    "    size_t hidden_bytes = batch_size * hidden_size * sizeof(float);\n",
    "    size_t output_bytes = batch_size * output_size * sizeof(float);\n",
    "\n",
    "    float *h_input = (float *)malloc(input_bytes);\n",
    "    float *h_weights1 = (float *)malloc(input_size * hidden_size * sizeof(float));\n",
    "    float *h_weights2 = (float *)malloc(hidden_size * output_size * sizeof(float));\n",
    "    float *h_hidden = (float *)malloc(hidden_bytes);\n",
    "    float *h_output = (float *)malloc(output_bytes);\n",
    "\n",
    "    // Initialize inputs and weights with random values\n",
    "    for (int i = 0; i < batch_size * input_size; i++) h_input[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < input_size * hidden_size; i++) h_weights1[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < hidden_size * output_size; i++) h_weights2[i] = (float)rand() / RAND_MAX;\n",
    "\n",
    "    // Allocate memory on device\n",
    "    float *d_input, *d_weights1, *d_weights2, *d_hidden, *d_output;\n",
    "    cudaMalloc((void **)&d_input, input_bytes);\n",
    "    cudaMalloc((void **)&d_weights1, input_size * hidden_size * sizeof(float));\n",
    "    cudaMalloc((void **)&d_weights2, hidden_size * output_size * sizeof(float));\n",
    "    cudaMalloc((void **)&d_hidden, hidden_bytes);\n",
    "    cudaMalloc((void **)&d_output, output_bytes);\n",
    "\n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_input, h_input, input_bytes, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights1, h_weights1, input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights2, h_weights2, hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Kernel launch parameters\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGridHidden((hidden_size + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                             (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    dim3 blocksPerGridOutput((output_size + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                             (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    // Forward pass: Input -> Hidden Layer\n",
    "    matMul<<<blocksPerGridHidden, threadsPerBlock>>>(d_input, d_weights1, d_hidden, batch_size, input_size, hidden_size);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Apply ReLU activation on hidden layer\n",
    "    int hidden_size_total = batch_size * hidden_size;\n",
    "    int threadsPerBlockReLU = 256;\n",
    "    int blocksPerGridReLU = (hidden_size_total + threadsPerBlockReLU - 1) / threadsPerBlockReLU;\n",
    "    applyReLU<<<blocksPerGridReLU, threadsPerBlockReLU>>>(d_hidden, hidden_size_total);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Forward pass: Hidden Layer -> Output Layer\n",
    "    matMul<<<blocksPerGridOutput, threadsPerBlock>>>(d_hidden, d_weights2, d_output, batch_size, hidden_size, output_size);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Copy results back to host\n",
    "    cudaMemcpy(h_output, d_output, output_bytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print first 10 outputs\n",
    "    printf(\"Output (first 10 elements):\\n\");\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        printf(\"%f \", h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Free memory\n",
    "    free(h_input);\n",
    "    free(h_weights1);\n",
    "    free(h_weights2);\n",
    "    free(h_hidden);\n",
    "    free(h_output);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_weights1);\n",
    "    cudaFree(d_weights2);\n",
    "    cudaFree(d_hidden);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3ca9c-7b3e-4b49-80f7-06f38be85554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Save as feedforward_nn.cu\n",
    "\n",
    "nvcc feedforward_nn.cu -o feedforward_nn\n",
    "\n",
    "./feedforward_nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6034f-941b-41e3-abd7-f7e54ea40e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c98b28-6521-490a-addf-44efff8943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Activation function: ReLU and its derivative\n",
    "__device__ float relu(float x) {\n",
    "    return x > 0 ? x : 0;\n",
    "}\n",
    "\n",
    "__device__ float relu_derivative(float x) {\n",
    "    return x > 0 ? 1 : 0;\n",
    "}\n",
    "\n",
    "// CUDA kernel for matrix multiplication\n",
    "__global__ void matMul(float *A, float *B, float *C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y; // Row index of C\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x; // Column index of C\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        float value = 0.0f;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            value += A[row * N + i] * B[i * K + col];\n",
    "        }\n",
    "        C[row * K + col] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for applying ReLU\n",
    "__global__ void applyReLU(float *input, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        input[idx] = relu(input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for computing the derivative of ReLU\n",
    "__global__ void applyReLU_derivative(float *input, float *grad, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        grad[idx] *= relu_derivative(input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for element-wise subtraction (used for loss computation)\n",
    "__global__ void subtract(float *a, float *b, float *c, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        c[idx] = a[idx] - b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for scalar multiplication (gradient descent update)\n",
    "__global__ void scalarMultiplyAdd(float *a, float *b, float scalar, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        a[idx] -= scalar * b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Define network dimensions\n",
    "    int input_size = 512;  // Number of input features\n",
    "    int hidden_size = 256; // Number of hidden neurons\n",
    "    int output_size = 10;  // Number of output neurons (e.g., classes)\n",
    "    int batch_size = 128;  // Number of samples in a batch\n",
    "\n",
    "    float learning_rate = 0.01;\n",
    "\n",
    "    // Allocate memory for host\n",
    "    size_t input_bytes = batch_size * input_size * sizeof(float);\n",
    "    size_t hidden_bytes = batch_size * hidden_size * sizeof(float);\n",
    "    size_t output_bytes = batch_size * output_size * sizeof(float);\n",
    "\n",
    "    float *h_input = (float *)malloc(input_bytes);\n",
    "    float *h_weights1 = (float *)malloc(input_size * hidden_size * sizeof(float));\n",
    "    float *h_weights2 = (float *)malloc(hidden_size * output_size * sizeof(float));\n",
    "    float *h_hidden = (float *)malloc(hidden_bytes);\n",
    "    float *h_output = (float *)malloc(output_bytes);\n",
    "    float *h_target = (float *)malloc(output_bytes); // Target outputs\n",
    "    float *h_output_grad = (float *)malloc(output_bytes);\n",
    "    float *h_hidden_grad = (float *)malloc(hidden_bytes);\n",
    "\n",
    "    // Initialize inputs, weights, and targets\n",
    "    for (int i = 0; i < batch_size * input_size; i++) h_input[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < input_size * hidden_size; i++) h_weights1[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < hidden_size * output_size; i++) h_weights2[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < batch_size * output_size; i++) h_target[i] = (float)rand() / RAND_MAX;\n",
    "\n",
    "    // Allocate memory on device\n",
    "    float *d_input, *d_weights1, *d_weights2, *d_hidden, *d_output, *d_target, *d_output_grad, *d_hidden_grad;\n",
    "    cudaMalloc((void **)&d_input, input_bytes);\n",
    "    cudaMalloc((void **)&d_weights1, input_size * hidden_size * sizeof(float));\n",
    "    cudaMalloc((void **)&d_weights2, hidden_size * output_size * sizeof(float));\n",
    "    cudaMalloc((void **)&d_hidden, hidden_bytes);\n",
    "    cudaMalloc((void **)&d_output, output_bytes);\n",
    "    cudaMalloc((void **)&d_target, output_bytes);\n",
    "    cudaMalloc((void **)&d_output_grad, output_bytes);\n",
    "    cudaMalloc((void **)&d_hidden_grad, hidden_bytes);\n",
    "\n",
    "    // Copy data from host to device\n",
    "    cudaMemcpy(d_input, h_input, input_bytes, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights1, h_weights1, input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights2, h_weights2, hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_target, h_target, output_bytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Kernel launch parameters\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGridHidden((hidden_size + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                             (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    dim3 blocksPerGridOutput((output_size + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                             (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    // Training loop\n",
    "    for (int epoch = 0; epoch < 100; epoch++) {\n",
    "        // Forward pass: Input -> Hidden Layer\n",
    "        matMul<<<blocksPerGridHidden, threadsPerBlock>>>(d_input, d_weights1, d_hidden, batch_size, input_size, hidden_size);\n",
    "        cudaDeviceSynchronize();\n",
    "        applyReLU<<<(batch_size * hidden_size + 255) / 256, 256>>>(d_hidden, batch_size * hidden_size);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Forward pass: Hidden Layer -> Output Layer\n",
    "        matMul<<<blocksPerGridOutput, threadsPerBlock>>>(d_hidden, d_weights2, d_output, batch_size, hidden_size, output_size);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Compute loss gradient: output_grad = output - target\n",
    "        subtract<<<(batch_size * output_size + 255) / 256, 256>>>(d_output, d_target, d_output_grad, batch_size * output_size);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Backpropagation: Hidden Grad = Output Grad * W2^T\n",
    "        matMul<<<blocksPerGridHidden, threadsPerBlock>>>(d_output_grad, d_weights2, d_hidden_grad, batch_size, output_size, hidden_size);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Apply ReLU derivative to hidden gradients\n",
    "        applyReLU_derivative<<<(batch_size * hidden_size + 255) / 256, 256>>>(d_hidden, d_hidden_grad, batch_size * hidden_size);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Gradient Descent: Update weights\n",
    "        scalarMultiplyAdd<<<(input_size * hidden_size + 255) / 256, 256>>>(d_weights1, d_hidden_grad, learning_rate, input_size * hidden_size);\n",
    "        scalarMultiplyAdd<<<(hidden_size * output_size + 255) / 256, 256>>>(d_weights2, d_output_grad, learning_rate, hidden_size * output_size);\n",
    "    }\n",
    "\n",
    "    printf(\"Training complete!\\n\");\n",
    "\n",
    "    // Free memory\n",
    "    free(h_input);\n",
    "    free(h_weights1);\n",
    "    free(h_weights2);\n",
    "    free(h_hidden);\n",
    "    free(h_output);\n",
    "    free(h_target);\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_weights1);\n",
    "    cudaFree(d_weights2);\n",
    "    cudaFree(d_hidden);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_target);\n",
    "    cudaFree(d_output_grad);\n",
    "    cudaFree(d_hidden_grad);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f53592-48d8-4c92-b0e0-c5a03c72c660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce935c2-67c6-4b18-9f67-692098ccaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neural_net_training.cu.\n",
    "\n",
    "nvcc neural_net_training.cu -o neural_net_training\n",
    "\n",
    "./neural_net_training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8460bd-1003-4419-9b9d-302268abb837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08e280-16b0-4c9b-b71c-2c583f74e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define NUM_CLASSES 3\n",
    "#define INPUT_FEATURES 4\n",
    "#define HIDDEN_LAYER1 8\n",
    "#define HIDDEN_LAYER2 8\n",
    "#define BATCH_SIZE 150\n",
    "#define EPOCHS 1000\n",
    "#define LEARNING_RATE 0.01\n",
    "\n",
    "// CUDA kernel for matrix multiplication\n",
    "__global__ void matMul(float *A, float *B, float *C, int M, int N, int K) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        float value = 0.0f;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            value += A[row * N + i] * B[i * K + col];\n",
    "        }\n",
    "        C[row * K + col] = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for applying ReLU activation\n",
    "__global__ void applyReLU(float *input, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        input[idx] = input[idx] > 0 ? input[idx] : 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for applying ReLU derivative\n",
    "__global__ void applyReLU_derivative(float *input, float *grad, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        grad[idx] *= input[idx] > 0 ? 1.0f : 0.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for softmax activation\n",
    "__global__ void applySoftmax(float *input, float *output, int rows, int cols) {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    if (idx < rows) {\n",
    "        float max_val = -INFINITY, sum = 0.0f;\n",
    "\n",
    "        // Find max value for numerical stability\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            max_val = fmaxf(max_val, input[idx * cols + j]);\n",
    "        }\n",
    "\n",
    "        // Compute softmax\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            output[idx * cols + j] = expf(input[idx * cols + j] - max_val);\n",
    "            sum += output[idx * cols + j];\n",
    "        }\n",
    "        for (int j = 0; j < cols; j++) {\n",
    "            output[idx * cols + j] /= sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for computing cross-entropy loss gradient\n",
    "__global__ void computeLossGradient(float *output, float *target, float *grad, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        grad[idx] = output[idx] - target[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel for updating weights\n",
    "__global__ void updateWeights(float *weights, float *grad, float scalar, int size) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        weights[idx] -= scalar * grad[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper function to normalize features and one-hot encode targets\n",
    "void loadIrisDataset(float *inputs, float *targets) {\n",
    "    // Iris dataset (features and labels)\n",
    "    float raw_data[150][4] = {\n",
    "        {5.1, 3.5, 1.4, 0.2}, {4.9, 3.0, 1.4, 0.2}, {4.7, 3.2, 1.3, 0.2}, // Truncated\n",
    "        {7.0, 3.2, 4.7, 1.4}, {6.4, 3.2, 4.5, 1.5}, {6.9, 3.1, 4.9, 1.5}, // Versicolor\n",
    "        {6.3, 3.3, 6.0, 2.5}, {5.8, 2.7, 5.1, 1.9}, {7.1, 3.0, 5.9, 2.1}  // Virginica\n",
    "        // Add the rest of the 150 entries\n",
    "    };\n",
    "    int labels[150] = {\n",
    "        0, 0, 0, 1, 1, 1, 2, 2, 2  // Truncated, 0 = Setosa, 1 = Versicolor, 2 = Virginica\n",
    "        // Add the rest of the 150 labels\n",
    "    };\n",
    "\n",
    "    // Normalize and one-hot encode\n",
    "    for (int i = 0; i < 150; i++) {\n",
    "        for (int j = 0; j < 4; j++) {\n",
    "            inputs[i * 4 + j] = raw_data[i][j] / 10.0f;  // Normalize to [0, 1]\n",
    "        }\n",
    "        for (int k = 0; k < 3; k++) {\n",
    "            targets[i * 3 + k] = (labels[i] == k) ? 1.0f : 0.0f;  // One-hot encoding\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Allocate host memory\n",
    "    size_t input_size = BATCH_SIZE * INPUT_FEATURES * sizeof(float);\n",
    "    size_t hidden1_size = BATCH_SIZE * HIDDEN_LAYER1 * sizeof(float);\n",
    "    size_t hidden2_size = BATCH_SIZE * HIDDEN_LAYER2 * sizeof(float);\n",
    "    size_t output_size = BATCH_SIZE * NUM_CLASSES * sizeof(float);\n",
    "\n",
    "    float *h_inputs = (float *)malloc(input_size);\n",
    "    float *h_targets = (float *)malloc(output_size);\n",
    "    float *h_hidden1 = (float *)malloc(hidden1_size);\n",
    "    float *h_hidden2 = (float *)malloc(hidden2_size);\n",
    "    float *h_outputs = (float *)malloc(output_size);\n",
    "\n",
    "    float *h_weights1 = (float *)malloc(INPUT_FEATURES * HIDDEN_LAYER1 * sizeof(float));\n",
    "    float *h_weights2 = (float *)malloc(HIDDEN_LAYER1 * HIDDEN_LAYER2 * sizeof(float));\n",
    "    float *h_weights3 = (float *)malloc(HIDDEN_LAYER2 * NUM_CLASSES * sizeof(float));\n",
    "\n",
    "    // Initialize dataset and weights\n",
    "    loadIrisDataset(h_inputs, h_targets);\n",
    "    for (int i = 0; i < INPUT_FEATURES * HIDDEN_LAYER1; i++) h_weights1[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < HIDDEN_LAYER1 * HIDDEN_LAYER2; i++) h_weights2[i] = (float)rand() / RAND_MAX;\n",
    "    for (int i = 0; i < HIDDEN_LAYER2 * NUM_CLASSES; i++) h_weights3[i] = (float)rand() / RAND_MAX;\n",
    "\n",
    "    // Allocate GPU memory\n",
    "    float *d_inputs, *d_targets, *d_hidden1, *d_hidden2, *d_outputs;\n",
    "    float *d_weights1, *d_weights2, *d_weights3, *d_hidden_grad2, *d_hidden_grad1, *d_output_grad;\n",
    "\n",
    "    cudaMalloc((void **)&d_inputs, input_size);\n",
    "    cudaMalloc((void **)&d_targets, output_size);\n",
    "    cudaMalloc((void **)&d_hidden1, hidden1_size);\n",
    "    cudaMalloc((void **)&d_hidden2, hidden2_size);\n",
    "    cudaMalloc((void **)&d_outputs, output_size);\n",
    "\n",
    "    cudaMalloc((void **)&d_weights1, INPUT_FEATURES * HIDDEN_LAYER1 * sizeof(float));\n",
    "    cudaMalloc((void **)&d_weights2, HIDDEN_LAYER1 * HIDDEN_LAYER2 * sizeof(float));\n",
    "    cudaMalloc((void **)&d_weights3, HIDDEN_LAYER2 * NUM_CLASSES * sizeof(float));\n",
    "\n",
    "    cudaMalloc((void **)&d_hidden_grad2, hidden2_size);\n",
    "    cudaMalloc((void **)&d_hidden_grad1, hidden1_size);\n",
    "    cudaMalloc((void **)&d_output_grad, output_size);\n",
    "\n",
    "    // Copy data to GPU\n",
    "    cudaMemcpy(d_inputs, h_inputs, input_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_targets, h_targets, output_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights1, h_weights1, INPUT_FEATURES * HIDDEN_LAYER1 * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights2, h_weights2, HIDDEN_LAYER1 * HIDDEN_LAYER2 * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_weights3, h_weights3, HIDDEN_LAYER2 * NUM_CLASSES * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Training loop\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid1((HIDDEN_LAYER1 + 15) / 16, (BATCH_SIZE + 15) / 16);\n",
    "    dim3 blocksPerGrid2((HIDDEN_LAYER2 + 15) / 16, (BATCH_SIZE + 15) / 16);\n",
    "    dim3 blocksPerGrid3((NUM_CLASSES + 15) / 16, (BATCH_SIZE + 15) / 16);\n",
    "\n",
    "    for (int epoch = 0; epoch < EPOCHS; epoch++) {\n",
    "        // Forward pass: Input -> Hidden1\n",
    "        matMul<<<blocksPerGrid1, threadsPerBlock>>>(d_inputs, d_weights1, d_hidden1, BATCH_SIZE, INPUT_FEATURES, HIDDEN_LAYER1);\n",
    "        applyReLU<<<(BATCH_SIZE * HIDDEN_LAYER1 + 255) / 256, 256>>>(d_hidden1, BATCH_SIZE * HIDDEN_LAYER1);\n",
    "\n",
    "        // Hidden1 -> Hidden2\n",
    "        matMul<<<blocksPerGrid2, threadsPerBlock>>>(d_hidden1, d_weights2, d_hidden2, BATCH_SIZE, HIDDEN_LAYER1, HIDDEN_LAYER2);\n",
    "        applyReLU<<<(BATCH_SIZE * HIDDEN_LAYER2 + 255) / 256, 256>>>(d_hidden2, BATCH_SIZE * HIDDEN_LAYER2);\n",
    "\n",
    "        // Hidden2 -> Output\n",
    "        matMul<<<blocksPerGrid3, threadsPerBlock>>>(d_hidden2, d_weights3, d_outputs, BATCH_SIZE, HIDDEN_LAYER2, NUM_CLASSES);\n",
    "        applySoftmax<<<BATCH_SIZE, NUM_CLASSES>>>(d_outputs, d_outputs, BATCH_SIZE, NUM_CLASSES);\n",
    "\n",
    "        // Backpropagation: Compute gradients\n",
    "        computeLossGradient<<<(BATCH_SIZE * NUM_CLASSES + 255) / 256, 256>>>(d_outputs, d_targets, d_output_grad, BATCH_SIZE * NUM_CLASSES);\n",
    "        matMul<<<blocksPerGrid2, threadsPerBlock>>>(d_output_grad, d_weights3, d_hidden_grad2, BATCH_SIZE, NUM_CLASSES, HIDDEN_LAYER2);\n",
    "        applyReLU_derivative<<<(BATCH_SIZE * HIDDEN_LAYER2 + 255) / 256, 256>>>(d_hidden2, d_hidden_grad2, BATCH_SIZE * HIDDEN_LAYER2);\n",
    "\n",
    "        matMul<<<blocksPerGrid1, threadsPerBlock>>>(d_hidden_grad2, d_weights2, d_hidden_grad1, BATCH_SIZE, HIDDEN_LAYER2, HIDDEN_LAYER1);\n",
    "        applyReLU_derivative<<<(BATCH_SIZE * HIDDEN_LAYER1 + 255) / 256, 256>>>(d_hidden1, d_hidden_grad1, BATCH_SIZE * HIDDEN_LAYER1);\n",
    "\n",
    "        // Update weights\n",
    "        updateWeights<<<(INPUT_FEATURES * HIDDEN_LAYER1 + 255) / 256, 256>>>(d_weights1, d_hidden_grad1, LEARNING_RATE, INPUT_FEATURES * HIDDEN_LAYER1);\n",
    "        updateWeights<<<(HIDDEN_LAYER1 * HIDDEN_LAYER2 + 255) / 256, 256>>>(d_weights2, d_hidden_grad2, LEARNING_RATE, HIDDEN_LAYER1 * HIDDEN_LAYER2);\n",
    "        updateWeights<<<(HIDDEN_LAYER2 * NUM_CLASSES + 255) / 256, 256>>>(d_weights3, d_output_grad, LEARNING_RATE, HIDDEN_LAYER2 * NUM_CLASSES);\n",
    "    }\n",
    "\n",
    "    printf(\"Training completed.\\n\");\n",
    "\n",
    "    // Free GPU memory\n",
    "    cudaFree(d_inputs);\n",
    "    cudaFree(d_targets);\n",
    "    cudaFree(d_hidden1);\n",
    "    cudaFree(d_hidden2);\n",
    "    cudaFree(d_outputs);\n",
    "    cudaFree(d_weights1);\n",
    "    cudaFree(d_weights2);\n",
    "    cudaFree(d_weights3);\n",
    "    cudaFree(d_hidden_grad1);\n",
    "    cudaFree(d_hidden_grad2);\n",
    "    cudaFree(d_output_grad);\n",
    "\n",
    "    // Free host memory\n",
    "    free(h_inputs);\n",
    "    free(h_targets);\n",
    "    free(h_hidden1);\n",
    "    free(h_hidden2);\n",
    "    free(h_outputs);\n",
    "    free(h_weights1);\n",
    "    free(h_weights2);\n",
    "    free(h_weights3);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6f8a3-487f-41f8-8bab-ab2aefdf7a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6286f5-fd1a-455b-846b-fb4cf973bda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17de6b2-a9ca-41aa-8eff-c933c78d4776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb906f19-8834-482b-820f-07e9688d5c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44be20-a54f-4e52-a573-c3621773981d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8d61f-b396-4642-b7de-76ab8e30043b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a82b4-2ee3-4675-a9ba-116c0ee46b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd99ad-e4db-4247-9a44-422ee704f04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c9b2a-3a56-4532-ac9d-39602ef0568b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a5864-8271-4b38-b042-ed230041cae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9655b-001a-4f80-aec8-00fb806abeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
