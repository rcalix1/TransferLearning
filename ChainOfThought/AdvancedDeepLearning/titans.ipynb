{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bebd48-eaba-46d4-b0aa-3db149ae4b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308eaeb-930f-4592-a190-ea062b920f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Memory Module\n",
    "class MemoryModule:\n",
    "    def __init__(self, memory_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Memory module for storing and retrieving input-output representations.\n",
    "        Args:\n",
    "            memory_size (int): Maximum size of the memory buffer.\n",
    "            hidden_size (int): Size of hidden representations.\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory = []  # Memory buffer to store query-response pairs\n",
    "\n",
    "    def write(self, query_repr, response_repr):\n",
    "        \"\"\"\n",
    "        Write a new query-response pair into the memory.\n",
    "        Args:\n",
    "            query_repr (torch.Tensor): Input/query representation.\n",
    "            response_repr (torch.Tensor): Output/response representation.\n",
    "        \"\"\"\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)  # Remove the oldest entry if memory is full\n",
    "        self.memory.append((query_repr.detach(), response_repr.detach()))\n",
    "\n",
    "    def read(self, query_repr):\n",
    "        \"\"\"\n",
    "        Retrieve the most similar memory entry to the given query.\n",
    "        Args:\n",
    "            query_repr (torch.Tensor): Input/query representation.\n",
    "        Returns:\n",
    "            torch.Tensor: Retrieved response representation.\n",
    "        \"\"\"\n",
    "        if not self.memory:\n",
    "            return torch.zeros_like(query_repr)  # Return zero vector if memory is empty\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarities = [torch.cosine_similarity(query_repr, mem_query, dim=0) for mem_query, _ in self.memory]\n",
    "        best_match_idx = torch.argmax(torch.tensor(similarities))\n",
    "        return self.memory[best_match_idx][1]\n",
    "\n",
    "# GPT Configuration Class\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, memory_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.memory_size = memory_size  # Size of the memory buffer\n",
    "\n",
    "# Memory-Augmented GPT Model\n",
    "class MemoryAugmentedGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.n_embd,\n",
    "            nhead=config.n_head,\n",
    "            num_encoder_layers=config.n_layer,\n",
    "            num_decoder_layers=config.n_layer,\n",
    "            dim_feedforward=4*config.n_embd,\n",
    "            dropout=0.1,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Memory Module\n",
    "        self.memory = MemoryModule(config.memory_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        b, t = x.size()\n",
    "        assert t <= self.pos_embedding.size(1), \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.embedding(x)\n",
    "        position_embeddings = self.pos_embedding[:, :t, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, embed_dim)\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # Memory interaction\n",
    "        query_repr = x.mean(dim=1)  # Aggregate representation\n",
    "        memory_output = self.memory.read(query_repr)  # Read from memory\n",
    "        logits += memory_output.unsqueeze(1)  # Add memory output to logits\n",
    "\n",
    "        # Write to memory (only if training or after generating a sequence)\n",
    "        if targets is not None:\n",
    "            response_repr = logits.mean(dim=1)  # Use logits as the response representation\n",
    "            self.memory.write(query_repr, response_repr)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "# Example Dataset and Training\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, vocab_size, block_size):\n",
    "        self.texts = texts\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert text to indices (example logic; needs actual tokenizer)\n",
    "        tokens = [random.randint(0, self.vocab_size - 1) for _ in range(self.block_size)]\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        targets = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return input_ids, targets\n",
    "\n",
    "# Training the Memory-Augmented GPT\n",
    "def train_memory_augmented_gpt():\n",
    "    config = GPTConfig(\n",
    "        vocab_size=100,  # Example vocab size\n",
    "        block_size=128,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        n_embd=256,\n",
    "        memory_size=10\n",
    "    )\n",
    "    model = MemoryAugmentedGPT(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # Example dataset\n",
    "    dataset = TextDataset([\"example text\"] * 1000, config.vocab_size, config.block_size)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for input_ids, targets in dataloader:\n",
    "            logits, loss = model(input_ids, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_memory_augmented_gpt()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16a2aa-fbca-45c8-9e71-e2eca6ae2e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fdd6e-f536-45ed-8b00-a495ec536fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Memory Module\n",
    "class MemoryModule:\n",
    "    def __init__(self, memory_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Memory module for storing and retrieving input-output representations.\n",
    "        Args:\n",
    "            memory_size (int): Maximum size of the memory buffer.\n",
    "            hidden_size (int): Size of hidden representations.\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory = []  # Memory buffer to store query-response pairs\n",
    "\n",
    "    def write(self, query_repr, response_repr):\n",
    "        \"\"\"\n",
    "        Write a new query-response pair into the memory.\n",
    "        Args:\n",
    "            query_repr (torch.Tensor): Input/query representation.\n",
    "            response_repr (torch.Tensor): Output/response representation.\n",
    "        \"\"\"\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)  # Remove the oldest entry if memory is full\n",
    "        self.memory.append((query_repr.detach(), response_repr.detach()))\n",
    "\n",
    "    def read(self, query_repr):\n",
    "        \"\"\"\n",
    "        Retrieve the most similar memory entry to the given query.\n",
    "        Args:\n",
    "            query_repr (torch.Tensor): Input/query representation.\n",
    "        Returns:\n",
    "            torch.Tensor: Retrieved response representation.\n",
    "        \"\"\"\n",
    "        if not self.memory:\n",
    "            return torch.zeros_like(query_repr)  # Return zero vector if memory is empty\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarities = [torch.cosine_similarity(query_repr, mem_query, dim=0) for mem_query, _ in self.memory]\n",
    "        best_match_idx = torch.argmax(torch.tensor(similarities))\n",
    "        return self.memory[best_match_idx][1]\n",
    "\n",
    "# GPT Configuration Class\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "# GPT Model with Memory Augmentation\n",
    "class MemoryAugmentedGPT(nn.Module):\n",
    "    def __init__(self, config, memory_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config.n_embd, nhead=config.n_head, dim_feedforward=4 * config.n_embd\n",
    "            ) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Memory Module\n",
    "        self.memory = MemoryModule(memory_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        b, t = x.size()\n",
    "        token_embeddings = self.embedding(x)\n",
    "        position_embeddings = self.pos_embedding[:, :t, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # Memory interaction\n",
    "        query_repr = x.mean(dim=1)  # Aggregate representation\n",
    "        memory_output = self.memory.read(query_repr)\n",
    "        logits += memory_output.unsqueeze(1)  # Add memory contribution\n",
    "\n",
    "        # Write to memory during training\n",
    "        if targets is not None:\n",
    "            response_repr = logits.mean(dim=1)\n",
    "            self.memory.write(query_repr, response_repr)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    def generate(self, start_tokens, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate text using the memory-augmented GPT.\n",
    "        Args:\n",
    "            start_tokens (torch.Tensor): Starting tokens for generation.\n",
    "            max_new_tokens (int): Maximum number of tokens to generate.\n",
    "        Returns:\n",
    "            torch.Tensor: Generated token sequence.\n",
    "        \"\"\"\n",
    "        generated = start_tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(generated)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            # Update memory during generation\n",
    "            query_repr = self.embedding(next_token).mean(dim=1)\n",
    "            response_repr = logits.mean(dim=1)\n",
    "            self.memory.write(query_repr, response_repr)\n",
    "\n",
    "        return generated\n",
    "\n",
    "# Training and Inference Integration Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = GPTConfig(\n",
    "        vocab_size=100,  # Example vocab size\n",
    "        block_size=128,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        n_embd=256\n",
    "    )\n",
    "    memory_size = 10\n",
    "    model = MemoryAugmentedGPT(config, memory_size)\n",
    "\n",
    "    # Dummy input for testing\n",
    "    x = torch.randint(0, config.vocab_size, (2, 128))\n",
    "    targets = torch.randint(0, config.vocab_size, (2, 128))\n",
    "\n",
    "    # Forward pass with memory\n",
    "    logits, loss = model(x, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Text generation example\n",
    "    start_tokens = torch.randint(0, config.vocab_size, (1, 10))\n",
    "    generated_tokens = model.generate(start_tokens, max_new_tokens=20)\n",
    "    print(f\"Generated tokens: {generated_tokens}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c91dc5-fd08-4658-9291-876d5b354bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a410dee-5187-4185-a3e5-fa66d1c2d891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bec99-b66d-4877-9c67-425bdff4bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Memory Module for Time Series\n",
    "class MemoryModule:\n",
    "    def __init__(self, memory_size, hidden_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory = []\n",
    "\n",
    "    def write(self, query_repr, response_repr):\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((query_repr.detach(), response_repr.detach()))\n",
    "\n",
    "    def read(self, query_repr):\n",
    "        if not self.memory:\n",
    "            return torch.zeros_like(query_repr)\n",
    "        similarities = [torch.cosine_similarity(query_repr, mem_query, dim=0) for mem_query, _ in self.memory]\n",
    "        best_match_idx = torch.argmax(torch.tensor(similarities))\n",
    "        return self.memory[best_match_idx][1]\n",
    "\n",
    "# Transformer-based Backbone for Time Series (Replaces LSTM with GPT-like architecture)\n",
    "class TransformerBackbone(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, n_head, block_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, block_size, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size, nhead=n_head, dim_feedforward=4 * hidden_size, dropout=0.1\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, f = x.size()\n",
    "        assert t <= self.block_size, \"Input sequence length exceeds block size.\"\n",
    "\n",
    "        x = self.embedding(x)  # Project input features to hidden size\n",
    "        x = x + self.pos_embedding[:, :t, :]  # Add positional embeddings\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x[:, -1, :]  # Use the last token's output as the sequence representation\n",
    "\n",
    "# Titans Model for Time Series with Transformer Backbone\n",
    "class TitansTimeSeries(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, n_head, block_size, memory_size):\n",
    "        super().__init__()\n",
    "        self.backbone = TransformerBackbone(input_size, hidden_size, num_layers, n_head, block_size)\n",
    "        self.memory = MemoryModule(memory_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        query_repr = self.backbone(x)\n",
    "        memory_output = self.memory.read(query_repr)\n",
    "        combined_repr = query_repr + memory_output\n",
    "        output = self.output_layer(combined_repr)\n",
    "\n",
    "        if y is not None:\n",
    "            self.memory.write(query_repr, y)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Generate synthetic time series data with batch structure\n",
    "def generate_synthetic_time_series_with_batch(num_samples, sequence_length, input_size, output_size):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data with a vector of inputs and outputs at each time step.\n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate (batch size).\n",
    "        sequence_length (int): Length of each time series sequence.\n",
    "        input_size (int): Size of the input vector at each time step.\n",
    "        output_size (int): Size of the output vector at each time step.\n",
    "    Returns:\n",
    "        Tuple of tensors: (inputs, outputs)\n",
    "    \"\"\"\n",
    "    inputs = torch.rand(num_samples, sequence_length, input_size)  # [batch, sequence_length, input_size]\n",
    "    outputs = torch.sin(inputs.sum(dim=2, keepdim=True)) + torch.rand(num_samples, sequence_length, output_size) * 0.1\n",
    "    return inputs, outputs\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurations\n",
    "    input_size = 4  # Number of features in the time series\n",
    "    hidden_size = 64\n",
    "    output_size = 4  # Forecasting a vector of size 4\n",
    "    num_layers = 4\n",
    "    n_head = 8\n",
    "    block_size = 20  # Maximum sequence length\n",
    "    memory_size = 10\n",
    "    batch_size = 16\n",
    "    sequence_length = 20\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TitansTimeSeries(input_size, hidden_size, output_size, num_layers, n_head, block_size, memory_size)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    inputs, outputs = generate_synthetic_time_series_with_batch(batch_size, sequence_length, input_size, output_size)\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs, outputs)\n",
    "        loss = F.mse_loss(predictions, outputs[:, -1, :])  # Predict based on the last time step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Inference\n",
    "    model.eval()\n",
    "    test_sample = torch.rand(1, sequence_length, input_size)  # A single test sequence\n",
    "    forecast = model(test_sample)\n",
    "    print(f\"Test input: {test_sample}\")\n",
    "    print(f\"Forecast: {forecast}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d01a4-2bd0-4b75-8daf-715832e26f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3388b-f31c-4913-a19a-56d6f5862ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c3b20-8d77-48b3-b7d7-cfab80df0e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc08099-7aa6-47bf-84cc-86f71ff3692d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab7813-77ea-45cf-a08f-096556614442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d718f-713a-4892-bcc7-9d4541645a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2084f-e1be-498d-9f65-9f1a90280d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
