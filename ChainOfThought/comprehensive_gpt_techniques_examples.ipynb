{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# GPT Techniques Examples\n", "This notebook demonstrates 10 key techniques implemented using GPT-style models in PyTorch."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Instruction Tuning with GPT"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPTInstruction(nn.Module):\n", "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n", "        super(GPTInstruction, self).__init__()\n", "        self.embedding = nn.Embedding(vocab_size, embed_size)\n", "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n", "        self.transformer = nn.TransformerEncoder(\n", "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n", "            num_layers\n", "        )\n", "        self.fc = nn.Linear(embed_size, vocab_size)\n", "\n", "    def forward(self, x):\n", "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n", "        x = self.transformer(x)\n", "        return self.fc(x[:, -1, :])\n", "\n", "# Example vocabulary and encoding\n", "vocab = {word: i for i, word in enumerate(\"Translate Hello to French World Bonjour Monde\".split())}\n", "def encode(sentence):\n", "    return torch.tensor([vocab[word] for word in sentence.split() if word in vocab])\n", "\n", "model = GPTInstruction(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n", "criterion = nn.CrossEntropyLoss()\n", "optimizer = optim.Adam(model.parameters(), lr=0.01)\n", "\n", "data = [\n", "    (\"Translate: 'Hello' to French.\", \"Bonjour\"),\n", "    (\"Translate: 'World' to French.\", \"Monde\"),\n", "]\n", "\n", "# Training Loop\n", "for epoch in range(10):\n", "    total_loss = 0\n", "    for instruction, response in data:\n", "        inputs = encode(instruction).unsqueeze(0)\n", "        targets = encode(response)\n", "        optimizer.zero_grad()\n", "        output = model(inputs)\n", "        loss = criterion(output, targets)\n", "        loss.backward()\n", "        optimizer.step()\n", "        total_loss += loss.item()\n", "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n", "\n", "test_input = encode(\"Translate: 'Hello' to French.\").unsqueeze(0)\n", "output = model(test_input)\n", "predicted = torch.argmax(output, dim=1)\n", "decoded = {idx: word for word, idx in vocab.items()}\n", "print(f\"Predicted: {[decoded[idx.item()] for idx in predicted]}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Self-Consistency Decoding with GPT"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = [\n", "    (\"What is the capital of France?\", \"Paris\"),\n", "    (\"Capital of France?\", \"Paris\"),\n", "    (\"France's capital?\", \"Paris\"),\n", "]\n", "\n", "class GPTSelfConsistency(nn.Module):\n", "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim):\n", "        super(GPTSelfConsistency, self).__init__()\n", "        self.embedding = nn.Embedding(vocab_size, embed_size)\n", "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n", "        self.transformer = nn.TransformerEncoder(\n", "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n", "            num_layers\n", "        )\n", "        self.fc = nn.Linear(embed_size, vocab_size)\n", "\n", "    def forward(self, x):\n", "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n", "        x = self.transformer(x)\n", "        return self.fc(x[:, -1, :])\n", "\n", "model = GPTSelfConsistency(len(vocab), embed_size=16, num_heads=2, num_layers=2, hidden_dim=64)\n", "criterion = nn.CrossEntropyLoss()\n", "optimizer = optim.Adam(model.parameters(), lr=0.01)\n", "\n", "for epoch in range(10):\n", "    total_loss = 0\n", "    for question, answer in data:\n", "        inputs = encode(question).unsqueeze(0)\n", "        targets = encode(answer)\n", "        optimizer.zero_grad()\n", "        output = model(inputs)\n", "        loss = criterion(output, targets)\n", "        loss.backward()\n", "        optimizer.step()\n", "        total_loss += loss.item()\n", "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n", "\n", "test_question = encode(\"What is the capital of France?\").unsqueeze(0)\n", "outputs = [torch.argmax(model(test_question), dim=1) for _ in range(3)]\n", "decoded = {idx: word for word, idx in vocab.items()}\n", "responses = [\"\".join([decoded[idx.item()] for idx in output]) for output in outputs]\n", "most_consistent = max(set(responses), key=responses.count)\n", "print(f\"Responses: {responses}\\nMost Consistent: {most_consistent}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.12"}}, "nbformat": 4, "nbformat_minor": 5}