# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nbsdCH4Txfkmk1UELY7JNo0RNwfBeV_s
"""

import pandas as pd

data_train_rc_df = pd.read_csv('/content/drive/MyDrive/train.csv', on_bad_lines='skip')
data_test_rc_df  = pd.read_csv('/content/drive/MyDrive/test.csv',  on_bad_lines='skip')

data_train_rc_df.rename(columns={'id': 'idx', 'text': 'sentence', 'target':'label'}, inplace=True)
data_test_rc_df.rename( columns={'id': 'idx', 'text': 'sentence'}, inplace=True)

data_train_rc_df

data_test_rc_df

from datasets import load_dataset

from datasets import Dataset, DatasetDict

# Create Dataset from DataFrame
train_dataset = Dataset.from_pandas( data_train_rc_df)
test_dataset  = Dataset.from_pandas( data_test_rc_df )

train_dataset

test_dataset

train_dataset[7]

def preprocess_function(examples):
    if sentence2_key is None:
        return tokenizer(examples[sentence1_key], truncation=True)
    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)

"/content/drive/MyDrive/review.csv"

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mnli-mm": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

task = "cola"
model_checkpoint = "distilbert-base-uncased"
batch_size = 16

sentence1_key, sentence2_key = task_to_keys[task]

sentence1_key, sentence2_key

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

num_labels =  2

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

## metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
metric_name = "accuracy"

model_name = model_checkpoint.split("/")[-1]

args = TrainingArguments(
    f"{model_name}-finetuned-{task}",
    eval_strategy = "no", ## "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=False,
    metric_for_best_model=metric_name
    ## push_to_hub=True,
)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if task != "stsb":
        predictions = np.argmax(predictions, axis=1)
    else:
        predictions = predictions[:, 0]
    return metric.compute(predictions=predictions, references=labels)

## !pip install evaluate

import numpy as np
import evaluate
from transformers import DataCollatorWithPadding

# Load the metric needed for compute_metrics
metric = evaluate.load("accuracy")

# Use a DataCollator to handle padding, which avoids the 'tokenizer' keyword issue in Trainer
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"

# The Trainer initialization using data_collator instead of tokenizer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=encoded_train_dataset,
    ## eval_dataset=encoded_dataset[validation_key],
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

import wandb
wandb.init(mode="disabled")

trainer.train()



# Encode test set
encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)

# Predict
predictions = trainer.predict(encoded_test_dataset)

# Get predicted labels
predicted_labels = np.argmax(predictions.predictions, axis=1)

# Show a few predictions
for i in range(5):
    print(f"Sentence: {data_test_rc_df.iloc[i]['sentence']}")
    print(f"Predicted label: {predicted_labels[i]}")
    print()

















from google.colab import drive
drive.mount('/content/drive')