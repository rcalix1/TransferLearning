{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b84470-55e0-4e4c-afb2-d7e43a2037ce",
   "metadata": {},
   "source": [
    "\n",
    "## Load Weights in C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53278288-f286-4e4f-86cf-112e47e4de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2206c85-350b-4a74-8b70-eadfe3983513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "model_name = \"gpt2\"  # You can also use \"gpt2-medium\" or other variants\n",
    "model      = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Access the weights for a specific layer (e.g., final linear layer)\n",
    "weights = model.lm_head.weight.detach().numpy()\n",
    "\n",
    "# Save the weights to a binary file\n",
    "import numpy as np\n",
    "weights.astype(np.float32).tofile(\"gpt2_weights.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4074a-97f1-4ade-bf95-34da7567b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3490ae-4c83-4122-8405-a1d4dc2b0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "model_name = \"gpt2\"  # Or \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Collect weights into a dictionary\n",
    "weights = {}\n",
    "\n",
    "# Save embedding weights\n",
    "weights['token_embedding'] = model.transformer.wte.weight.detach().numpy()\n",
    "weights['position_embedding'] = model.transformer.wpe.weight.detach().numpy()\n",
    "\n",
    "# Save transformer block weights\n",
    "for i, block in enumerate(model.transformer.h):\n",
    "    prefix = f\"block_{i}\"\n",
    "    weights[f\"{prefix}_attn_qkv\"] = block.attn.c_attn.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_attn_proj\"] = block.attn.c_proj.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_mlp_fc\"] = block.mlp.c_fc.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_mlp_proj\"] = block.mlp.c_proj.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_ln1_weight\"] = block.ln_1.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_ln1_bias\"] = block.ln_1.bias.detach().numpy()\n",
    "    weights[f\"{prefix}_ln2_weight\"] = block.ln_2.weight.detach().numpy()\n",
    "    weights[f\"{prefix}_ln2_bias\"] = block.ln_2.bias.detach().numpy()\n",
    "\n",
    "# Save final layer normalization\n",
    "weights['ln_f_weight'] = model.transformer.ln_f.weight.detach().numpy()\n",
    "weights['ln_f_bias'] = model.transformer.ln_f.bias.detach().numpy()\n",
    "\n",
    "# Save final linear layer (logits projection)\n",
    "weights['lm_head'] = model.lm_head.weight.detach().numpy()\n",
    "\n",
    "# Save all weights to a binary file\n",
    "with open(\"gpt2_weights_ALL.bin\", \"wb\") as f:\n",
    "    for key, value in weights.items():\n",
    "        np.array(value.shape, dtype=np.int32).tofile(f)  # Save the shape\n",
    "        value.astype(np.float32).tofile(f)  # Save the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992bef2-dfd6-4ed2-951e-97fa018256d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1599a7-9b85-488a-9ac2-5ec4bed33638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6d0eba-8a90-4914-a3d6-dfb8494092f2",
   "metadata": {},
   "source": [
    "\n",
    "## C code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52e67-97a8-468a-9956-001945d96827",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Matrix multiplication\n",
    "void matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < K; j++) {\n",
    "            C[i * K + j] = 0;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i * K + j] += A[i * N + k] * B[k * K + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Simplified forward pass\n",
    "void forward(float* input, float* weights, float* output, int input_dim, int output_dim) {\n",
    "    matmul(input, weights, output, 1, input_dim, output_dim);  // Single layer example\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int input_dim = 768;    // GPT hidden size\n",
    "    int output_dim = 50257; // Vocabulary size\n",
    "\n",
    "    // Load weights\n",
    "    FILE* weight_file = fopen(\"gpt2_weights.bin\", \"rb\");\n",
    "    float* weights = malloc(input_dim * output_dim * sizeof(float));\n",
    "    fread(weights, sizeof(float), input_dim * output_dim, weight_file);\n",
    "    fclose(weight_file);\n",
    "\n",
    "    // Input vector\n",
    "    float input[input_dim];\n",
    "    for (int i = 0; i < input_dim; i++) input[i] = 1.0f;  // Example input\n",
    "\n",
    "    // Output vector\n",
    "    float output[output_dim];\n",
    "    forward(input, weights, output, input_dim, output_dim);\n",
    "\n",
    "    // Print top predictions\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        printf(\"Output[%d]: %f\\n\", i, output[i]);\n",
    "    }\n",
    "\n",
    "    free(weights);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd924b-c4e7-49ad-8f70-8c8892bff68e",
   "metadata": {},
   "source": [
    "\n",
    "## ALL weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ab583-7944-4df6-ac53-437d010387a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Constants\n",
    "#define MAX_SEQ_LEN 1024\n",
    "#define DIM 768\n",
    "#define NUM_HEADS 12\n",
    "#define NUM_BLOCKS 12\n",
    "#define VOCAB_SIZE 50257\n",
    "\n",
    "// Helper: Matrix multiplication\n",
    "void matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    for (int i = 0; i < M; i++) {\n",
    "        for (int j = 0; j < K; j++) {\n",
    "            C[i * K + j] = 0;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i * K + j] += A[i * N + k] * B[k * K + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper: Layer normalization\n",
    "void layer_norm(float* input, float* output, float* weight, float* bias, int dim) {\n",
    "    float mean = 0, variance = 0;\n",
    "    for (int i = 0; i < dim; i++) mean += input[i];\n",
    "    mean /= dim;\n",
    "\n",
    "    for (int i = 0; i < dim; i++) variance += (input[i] - mean) * (input[i] - mean);\n",
    "    variance /= dim;\n",
    "\n",
    "    float epsilon = 1e-5;\n",
    "    for (int i = 0; i < dim; i++) {\n",
    "        output[i] = (input[i] - mean) / sqrt(variance + epsilon) * weight[i] + bias[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper: Self-attention\n",
    "void self_attention(float* qkv, float* output, int seq_len, int head_dim, int n_heads) {\n",
    "    int dim = head_dim * n_heads;\n",
    "    float* Q = qkv;\n",
    "    float* K = qkv + seq_len * dim;\n",
    "    float* V = qkv + 2 * seq_len * dim;\n",
    "\n",
    "    float scores[seq_len * seq_len];\n",
    "    float attention[seq_len * seq_len];\n",
    "\n",
    "    // Compute scores\n",
    "    matmul(Q, K, scores, seq_len, head_dim, seq_len);\n",
    "    for (int i = 0; i < seq_len * seq_len; i++) scores[i] /= sqrt(head_dim);\n",
    "\n",
    "    // Softmax\n",
    "    for (int i = 0; i < seq_len; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < seq_len; j++) {\n",
    "            scores[i * seq_len + j] = exp(scores[i * seq_len + j]);\n",
    "            sum += scores[i * seq_len + j];\n",
    "        }\n",
    "        for (int j = 0; j < seq_len; j++) {\n",
    "            attention[i * seq_len + j] = scores[i * seq_len + j] / sum;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Compute attention output\n",
    "    matmul(attention, V, output, seq_len, seq_len, head_dim);\n",
    "}\n",
    "\n",
    "// Forward pass for one block\n",
    "void forward_block(float* input, float* output, float* qkv_weight, float* proj_weight, float* fc_weight, float* proj_fc_weight, float* ln1_weight, float* ln1_bias, float* ln2_weight, float* ln2_bias, int seq_len, int dim) {\n",
    "    float qkv[3 * seq_len * dim];\n",
    "    float attn_output[seq_len * dim];\n",
    "    float ln1_output[seq_len * dim];\n",
    "    float fc_output[seq_len * dim];\n",
    "    float ln2_output[seq_len * dim];\n",
    "\n",
    "    // Layer norm 1\n",
    "    layer_norm(input, ln1_output, ln1_weight, ln1_bias, dim);\n",
    "\n",
    "    // Self-attention\n",
    "    matmul(ln1_output, qkv_weight, qkv, seq_len, dim, 3 * dim);\n",
    "    self_attention(qkv, attn_output, seq_len, dim / NUM_HEADS, NUM_HEADS);\n",
    "\n",
    "    // Project attention output\n",
    "    matmul(attn_output, proj_weight, ln2_output, seq_len, dim, dim);\n",
    "\n",
    "    // Add residual\n",
    "    for (int i = 0; i < seq_len * dim; i++) ln2_output[i] += input[i];\n",
    "\n",
    "    // Layer norm 2\n",
    "    layer_norm(ln2_output, ln2_output, ln2_weight, ln2_bias, dim);\n",
    "\n",
    "    // Feedforward\n",
    "    matmul(ln2_output, fc_weight, fc_output, seq_len, dim, dim);\n",
    "    matmul(fc_output, proj_fc_weight, output, seq_len, dim, dim);\n",
    "\n",
    "    // Add residual\n",
    "    for (int i = 0; i < seq_len * dim; i++) output[i] += ln2_output[i];\n",
    "}\n",
    "\n",
    "// Forward pass for GPT\n",
    "void gpt_forward(float* input, float* token_embedding, float* position_embedding, float* lm_head, float* block_weights[][8], int seq_len, int dim) {\n",
    "    float embedded[MAX_SEQ_LEN * DIM];\n",
    "\n",
    "    // Token and position embeddings\n",
    "    for (int i = 0; i < seq_len; i++) {\n",
    "        for (int j = 0; j < dim; j++) {\n",
    "            embedded[i * dim + j] = input[i * dim + j] +\n",
    "                                    token_embedding[input[i] * dim + j] +\n",
    "                                    position_embedding[i * dim + j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Transformer blocks\n",
    "    float current[MAX_SEQ_LEN * DIM];\n",
    "    float next[MAX_SEQ_LEN * DIM];\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) {\n",
    "        forward_block(embedded, next, block_weights[i][0], block_weights[i][1], block_weights[i][2], block_weights[i][3], block_weights[i][4], block_weights[i][5], block_weights[i][6], block_weights[i][7], seq_len, dim);\n",
    "        float* temp = embedded;\n",
    "        embedded = next;\n",
    "        next = temp;\n",
    "    }\n",
    "\n",
    "    // Final projection\n",
    "    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n",
    "    matmul(embedded, lm_head, logits, seq_len, dim, VOCAB_SIZE);\n",
    "}\n",
    "\n",
    "// Load weights from file\n",
    "void load_weights(const char* filename, float** token_embedding, float** position_embedding, float** lm_head, float* block_weights[][8], int num_blocks) {\n",
    "    FILE* file = fopen(filename, \"rb\");\n",
    "    if (!file) {\n",
    "        printf(\"Failed to open weights file.\\n\");\n",
    "        exit(1);\n",
    "    }\n",
    "\n",
    "    // Allocate and load token and position embeddings\n",
    "    *token_embedding = (float*)malloc(DIM * VOCAB_SIZE * sizeof(float));\n",
    "    fread(*token_embedding, sizeof(float), DIM * VOCAB_SIZE, file);\n",
    "\n",
    "    *position_embedding = (float*)malloc(MAX_SEQ_LEN * DIM * sizeof(float));\n",
    "    fread(*position_embedding, sizeof(float), MAX_SEQ_LEN * DIM, file);\n",
    "\n",
    "    // Allocate and load transformer block weights\n",
    "    for (int i = 0; i < num_blocks; i++) {\n",
    "        for (int j = 0; j < 8; j++) {\n",
    "            block_weights[i][j] = (float*)malloc(DIM * DIM * sizeof(float)); // Simplified size allocation\n",
    "            fread(block_weights[i][j], sizeof(float), DIM * DIM, file);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Allocate and load logits projection\n",
    "    *lm_head = (float*)malloc(DIM * VOCAB_SIZE * sizeof(float));\n",
    "    fread(*lm_head, sizeof(float), DIM * VOCAB_SIZE, file);\n",
    "\n",
    "    fclose(file);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int seq_len = 8;  // Example sequence length\n",
    "    int dim = DIM;\n",
    "\n",
    "    // Allocate memory for weights\n",
    "    float* token_embedding;\n",
    "    float* position_embedding;\n",
    "    float* lm_head;\n",
    "    float* block_weights[NUM_BLOCKS][8];\n",
    "\n",
    "    // Load weights\n",
    "    load_weights(\"gpt2_weights.bin\", &token_embedding, &position_embedding, &lm_head, block_weights, NUM_BLOCKS);\n",
    "\n",
    "    // Example input: token IDs\n",
    "    float input[MAX_SEQ_LEN] = {0, 1, 2, 3, 4, 5, 6, 7};\n",
    "\n",
    "    // Perform GPT forward pass\n",
    "    gpt_forward(input, token_embedding, position_embedding, lm_head, block_weights, seq_len, dim);\n",
    "\n",
    "    printf(\"GPT Forward Pass Complete.\\n\");\n",
    "\n",
    "    // Free allocated memory\n",
    "    free(token_embedding);\n",
    "    free(position_embedding);\n",
    "    free(lm_head);\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) {\n",
    "        for (int j = 0; j < 8; j++) {\n",
    "            free(block_weights[i][j]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139169ee-09ca-4bfe-8199-66b5da679182",
   "metadata": {},
   "source": [
    "\n",
    "## faster c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef95e54-37d5-47bd-8d6d-5929f2f4bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cblas.h>\n",
    "\n",
    "// Matrix multiplication using BLAS\n",
    "void matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans,\n",
    "                M, K, N, 1.0, A, N, B, K, 0.0, C, K);\n",
    "}\n",
    "\n",
    "// Simplified feedforward pass of a single layer\n",
    "void feedforward(float* input, float* weights, float* output, int input_dim, int output_dim) {\n",
    "    matmul(input, weights, output, 1, input_dim, output_dim);\n",
    "}\n",
    "\n",
    "// Load weights from a binary file\n",
    "float* load_weights(const char* filename, int size) {\n",
    "    FILE* file = fopen(filename, \"rb\");\n",
    "    if (!file) {\n",
    "        printf(\"Error: Unable to open file %s\\\\n\", filename);\n",
    "        exit(1);\n",
    "    }\n",
    "    float* weights = malloc(size * sizeof(float));\n",
    "    fread(weights, sizeof(float), size, file);\n",
    "    fclose(file);\n",
    "    return weights;\n",
    "}\n",
    "\n",
    "// Simple softmax function\n",
    "void softmax(float* logits, int size) {\n",
    "    float max = logits[0];\n",
    "    for (int i = 1; i < size; i++) {\n",
    "        if (logits[i] > max) max = logits[i];\n",
    "    }\n",
    "    float sum = 0.0;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        logits[i] = expf(logits[i] - max);  // Prevent overflow\n",
    "        sum += logits[i];\n",
    "    }\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        logits[i] /= sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Model dimensions\n",
    "    int input_dim = 768;    // GPT hidden size\n",
    "    int output_dim = 50257; // GPT vocabulary size\n",
    "\n",
    "    // Load pre-trained weights (e.g., from PyTorch or custom binary format)\n",
    "    float* weights = load_weights(\"gpt2_weights.bin\", input_dim * output_dim);\n",
    "\n",
    "    // Input vector (example input, normally generated from embeddings)\n",
    "    float input[input_dim];\n",
    "    for (int i = 0; i < input_dim; i++) input[i] = 1.0f;  // Example input\n",
    "\n",
    "    // Output vector\n",
    "    float* output = malloc(output_dim * sizeof(float));\n",
    "\n",
    "    // Perform a forward pass\n",
    "    feedforward(input, weights, output, input_dim, output_dim);\n",
    "\n",
    "    // Apply softmax to logits\n",
    "    softmax(output, output_dim);\n",
    "\n",
    "    // Print top predictions\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        printf(\"Logit[%d]: %f\\\\n\", i, output[i]);\n",
    "    }\n",
    "\n",
    "    // Free memory\n",
    "    free(weights);\n",
    "    free(output);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bea5f-632b-4710-a1fb-544c1c41bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sudo apt-get install libopenblas-dev\n",
    "\n",
    "gcc -o gpt_optimized gpt_optimized.c -lopenblas -lm\n",
    "gcc -o gpt_forward gpt_forward.c -lm\n",
    "\n",
    "\n",
    "./gpt_optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fc8e7-8a6e-46aa-9bab-fc5f035243ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Save weights in binary format\n",
    "model = torch.load(\"gpt2_model.pth\")\n",
    "weights = model[\"decoder\"][\"linear.weight\"].cpu().numpy()\n",
    "with open(\"gpt2_weights.bin\", \"wb\") as f:\n",
    "    f.write(weights.tobytes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80f3e9-90de-4f3f-bbdd-a7b5f50444d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Logit[0]: 0.123456\n",
    "Logit[1]: 0.098765\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c242f1-1765-49ad-be6d-c50651dd3c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2fdf329-50cf-4975-96fb-afb55b9bb234",
   "metadata": {},
   "source": [
    "\n",
    "## C faster again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950c350-4c16-4d09-81fa-d7c21e578fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b7208-07c1-462f-9066-f16fe46098d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <cblas.h>  // Include BLAS header\n",
    "\n",
    "// Constants\n",
    "#define MAX_SEQ_LEN 1024\n",
    "#define DIM 768\n",
    "#define NUM_HEADS 12\n",
    "#define NUM_BLOCKS 12\n",
    "#define VOCAB_SIZE 50257\n",
    "\n",
    "// Helper: Optimized matrix multiplication using BLAS\n",
    "void matmul(float* A, float* B, float* C, int M, int N, int K) {\n",
    "    // BLAS matrix multiplication: C = alpha * A * B + beta * C\n",
    "    cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans,\n",
    "                M, K, N,\n",
    "                1.0, A, N,\n",
    "                B, K,\n",
    "                0.0, C, K);\n",
    "}\n",
    "\n",
    "// Helper: Layer normalization\n",
    "void layer_norm(float* input, float* output, float* weight, float* bias, int dim) {\n",
    "    float mean = 0, variance = 0;\n",
    "    for (int i = 0; i < dim; i++) mean += input[i];\n",
    "    mean /= dim;\n",
    "\n",
    "    for (int i = 0; i < dim; i++) variance += (input[i] - mean) * (input[i] - mean);\n",
    "    variance /= dim;\n",
    "\n",
    "    float epsilon = 1e-5;\n",
    "    for (int i = 0; i < dim; i++) {\n",
    "        output[i] = (input[i] - mean) / sqrt(variance + epsilon) * weight[i] + bias[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper: Self-attention using BLAS\n",
    "void self_attention(float* qkv, float* output, int seq_len, int head_dim, int n_heads) {\n",
    "    int dim = head_dim * n_heads;\n",
    "    float* Q = qkv;\n",
    "    float* K = qkv + seq_len * dim;\n",
    "    float* V = qkv + 2 * seq_len * dim;\n",
    "\n",
    "    float scores[seq_len * seq_len];\n",
    "    float attention[seq_len * seq_len];\n",
    "\n",
    "    // Compute attention scores\n",
    "    matmul(Q, K, scores, seq_len, head_dim, seq_len);\n",
    "    for (int i = 0; i < seq_len * seq_len; i++) scores[i] /= sqrt(head_dim);\n",
    "\n",
    "    // Apply softmax\n",
    "    for (int i = 0; i < seq_len; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < seq_len; j++) {\n",
    "            scores[i * seq_len + j] = exp(scores[i * seq_len + j]);\n",
    "            sum += scores[i * seq_len + j];\n",
    "        }\n",
    "        for (int j = 0; j < seq_len; j++) {\n",
    "            attention[i * seq_len + j] = scores[i * seq_len + j] / sum;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Compute attention output\n",
    "    matmul(attention, V, output, seq_len, seq_len, head_dim);\n",
    "}\n",
    "\n",
    "// Forward pass for one block\n",
    "void forward_block(float* input, float* output, float* qkv_weight, float* proj_weight, float* fc_weight, float* proj_fc_weight, float* ln1_weight, float* ln1_bias, float* ln2_weight, float* ln2_bias, int seq_len, int dim) {\n",
    "    float qkv[3 * seq_len * dim];\n",
    "    float attn_output[seq_len * dim];\n",
    "    float ln1_output[seq_len * dim];\n",
    "    float fc_output[seq_len * dim];\n",
    "    float ln2_output[seq_len * dim];\n",
    "\n",
    "    // Layer norm 1\n",
    "    layer_norm(input, ln1_output, ln1_weight, ln1_bias, dim);\n",
    "\n",
    "    // Self-attention\n",
    "    matmul(ln1_output, qkv_weight, qkv, seq_len, dim, 3 * dim);\n",
    "    self_attention(qkv, attn_output, seq_len, dim / NUM_HEADS, NUM_HEADS);\n",
    "\n",
    "    // Project attention output\n",
    "    matmul(attn_output, proj_weight, ln2_output, seq_len, dim, dim);\n",
    "\n",
    "    // Add residual\n",
    "    for (int i = 0; i < seq_len * dim; i++) ln2_output[i] += input[i];\n",
    "\n",
    "    // Layer norm 2\n",
    "    layer_norm(ln2_output, ln2_output, ln2_weight, ln2_bias, dim);\n",
    "\n",
    "    // Feedforward\n",
    "    matmul(ln2_output, fc_weight, fc_output, seq_len, dim, dim);\n",
    "    matmul(fc_output, proj_fc_weight, output, seq_len, dim, dim);\n",
    "\n",
    "    // Add residual\n",
    "    for (int i = 0; i < seq_len * dim; i++) output[i] += ln2_output[i];\n",
    "}\n",
    "\n",
    "// Forward pass for GPT\n",
    "void gpt_forward(float* input, float* token_embedding, float* position_embedding, float* lm_head, float* block_weights[][8], int seq_len, int dim) {\n",
    "    float embedded[MAX_SEQ_LEN * DIM];\n",
    "\n",
    "    // Token and position embeddings\n",
    "    for (int i = 0; i < seq_len; i++) {\n",
    "        for (int j = 0; j < dim; j++) {\n",
    "            embedded[i * dim + j] = token_embedding[input[i] * dim + j] +\n",
    "                                    position_embedding[i * dim + j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Transformer blocks\n",
    "    float current[MAX_SEQ_LEN * DIM];\n",
    "    float next[MAX_SEQ_LEN * DIM];\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) {\n",
    "        forward_block(embedded, next, block_weights[i][0], block_weights[i][1], block_weights[i][2], block_weights[i][3], block_weights[i][4], block_weights[i][5], block_weights[i][6], block_weights[i][7], seq_len, dim);\n",
    "        float* temp = embedded;\n",
    "        embedded = next;\n",
    "        next = temp;\n",
    "    }\n",
    "\n",
    "    // Final projection\n",
    "    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n",
    "    matmul(embedded, lm_head, logits, seq_len, dim, VOCAB_SIZE);\n",
    "}\n",
    "\n",
    "// Load weights from file\n",
    "void load_weights(const char* filename, float** token_embedding, float** position_embedding, float** lm_head, float* block_weights[][8], int num_blocks) {\n",
    "    FILE* file = fopen(filename, \"rb\");\n",
    "    if (!file) {\n",
    "        printf(\"Failed to open weights file.\\n\");\n",
    "        exit(1);\n",
    "    }\n",
    "\n",
    "    // Allocate and load token and position embeddings\n",
    "    *token_embedding = (float*)malloc(DIM * VOCAB_SIZE * sizeof(float));\n",
    "    fread(*token_embedding, sizeof(float), DIM * VOCAB_SIZE, file);\n",
    "\n",
    "    *position_embedding = (float*)malloc(MAX_SEQ_LEN * DIM * sizeof(float));\n",
    "    fread(*position_embedding, sizeof(float), MAX_SEQ_LEN * DIM, file);\n",
    "\n",
    "    // Allocate and load transformer block weights\n",
    "    for (int i = 0; i < num_blocks; i++) {\n",
    "        for (int j = 0; j < 8; j++) {\n",
    "            block_weights[i][j] = (float*)malloc(DIM * DIM * sizeof(float)); // Simplified size allocation\n",
    "            fread(block_weights[i][j], sizeof(float), DIM * DIM, file);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Allocate and load logits projection\n",
    "    *lm_head = (float*)malloc(DIM * VOCAB_SIZE * sizeof(float));\n",
    "    fread(*lm_head, sizeof(float), DIM * VOCAB_SIZE, file);\n",
    "\n",
    "    fclose(file);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int seq_len = 8;  // Example sequence length\n",
    "    int dim = DIM;\n",
    "\n",
    "    // Configure OpenBLAS threads\n",
    "    openblas_set_num_threads(4);\n",
    "\n",
    "    // Allocate memory for weights\n",
    "    float* token_embedding;\n",
    "    float* position_embedding;\n",
    "    float* lm_head;\n",
    "    float* block_weights[NUM_BLOCKS][8];\n",
    "\n",
    "    // Load weights\n",
    "    load_weights(\"gpt2_weights.bin\", &token_embedding, &position_embedding, &lm_head, block_weights, NUM_BLOCKS);\n",
    "\n",
    "    // Example input: token IDs\n",
    "    int input[MAX_SEQ_LEN] = {0, 1, 2, 3, 4, 5, 6, 7};\n",
    "\n",
    "    // Perform GPT forward pass\n",
    "    gpt_forward(input, token_embedding, position_embedding, lm_head, block_weights, seq_len, dim);\n",
    "\n",
    "    printf(\"GPT Forward Pass Complete.\\n\");\n",
    "\n",
    "    // Free allocated memory\n",
    "    free(token_embedding);\n",
    "    free(position_embedding);\n",
    "    free(lm_head);\n",
    "    for (int i = 0; i < NUM_BLOCKS; i++) {\n",
    "        for (int j = 0; j < 8; j++) {\n",
    "            free(block_weights[i][j]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442c8c0-589f-4f2e-bad0-86a79cce0032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67843d75-a249-4692-a5eb-17726214dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gcc -o gpt -lopenblas gpt.c -lm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4cd63-04b6-4376-93ea-e0f5cde51b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7b4dc-68d2-4538-9c50-176a03b68297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "./gpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f89ffb-b78e-4901-a104-d02ed2fcb0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3390b0a-9676-43d9-92f8-d498d887e7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def24f7-e7c8-479f-83e9-099e3263bb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8ffa1-e37f-4211-865d-3e09f7624610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd3b81-d737-4a4d-9546-123d4286f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
