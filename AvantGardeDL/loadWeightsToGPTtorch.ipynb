{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ab37f4-2105-4e4e-94cc-cfd16539b5a6",
   "metadata": {},
   "source": [
    "\n",
    "## Load weights to GPT with torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30203642-6fd3-43aa-8c9c-429f54e08415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b042ca-b577-43d0-a34c-fc3ff7045847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6abfb83-1ce2-4322-b05f-a266a1a199d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.994893074035645\n",
      "Epoch 2, Loss: 10.771008491516113\n",
      "Epoch 3, Loss: 10.615301132202148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "# GPT Architecture (compatible with pre-trained weights)\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(1024, 1024)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q = self.query(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = attn.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Load GPT-2 Pre-Trained Weights\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Access weights\n",
    "gpt2_weights = gpt2_model.state_dict()\n",
    "\n",
    "# Initialize Custom GPT Model\n",
    "vocab_size = gpt2_weights[\"transformer.wte.weight\"].shape[0]\n",
    "block_size = gpt2_model.config.n_ctx\n",
    "n_embd = gpt2_model.config.n_embd\n",
    "n_layer = gpt2_model.config.n_layer\n",
    "n_head = gpt2_model.config.n_head\n",
    "\n",
    "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head)\n",
    "\n",
    "# Map GPT-2 weights to custom GPT model\n",
    "model.token_embedding.weight.data = gpt2_weights[\"transformer.wte.weight\"].clone()\n",
    "model.position_embedding.weight.data = gpt2_weights[\"transformer.wpe.weight\"].clone()\n",
    "\n",
    "# Fine-Tuning\n",
    "def fine_tune(model, data, epochs=3, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data:\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Example data\n",
    "data = [  # Dummy data: sequence input and target\n",
    "    (torch.randint(0, vocab_size, (4, block_size)), torch.randint(0, vocab_size, (4, block_size)))\n",
    "]\n",
    "\n",
    "# Fine-Tune\n",
    "fine_tune(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cc977-2f42-48b4-bb40-2acb1a821e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5c208-77f5-413b-921b-1b63e105e202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94edaf4-0736-4057-bd2d-2d2b5e1c6c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc840355-3f32-4138-913c-dc256decc9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d3c00-3651-4688-a93e-e8edbe5683eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0aebed-a99b-4eb1-826a-46ebb92f814a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b0a98-3094-4442-a8b8-5639aa8dd810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fd2aa-e84f-48fb-87b2-6b6f36ec52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd9699-7ee5-4a9e-9f06-e764f3244cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
