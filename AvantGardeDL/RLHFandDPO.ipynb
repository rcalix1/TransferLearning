{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f34a7e-6a5c-45d9-b4fc-cd83ab0cd012",
   "metadata": {},
   "source": [
    "\n",
    "## RLHF with DPO and GPTs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1044f056-6604-4726-923b-1ea9f2ba7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7293c4c6-00a1-4ada-9c9f-341cf232ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab_size  = 100     # Small vocab for synthetic data\n",
    "embed_size  = 128\n",
    "num_heads   = 4\n",
    "num_layers  = 2\n",
    "hidden_dim  = 256\n",
    "max_seq_len = 32\n",
    "seq_len     = 16\n",
    "batch_size  = 32\n",
    "epochs      = 10\n",
    "lr          = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101a3b03-2afe-4aa2-986f-7ce26750d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim, max_seq_len):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding           = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_seq_len, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13480772-3612-44b9-9260-4b5acc54ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_synthetic_data(batch_size, seq_len, vocab_size):\n",
    "    \n",
    "    # Generate random token sequences\n",
    "    seq_a = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    seq_b = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    # Randomly assign preferences (1 means seq_a preferred over seq_b, 0 otherwise)\n",
    "    preferences = torch.randint(0, 2, (batch_size,))\n",
    "    return seq_a, seq_b, preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb8579d5-f739-4712-9e72-ca4678a99050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_dim):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        return self.fc(embeddings).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215647ad-aa9e-40ee-b380-48fb9e7b9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dpo_loss( reward_a, reward_b, preferences, beta=0.1 ):\n",
    "    \n",
    "    logits = (reward_a - reward_b) / beta\n",
    "  \n",
    "  \n",
    "    loss   = -torch.mean( preferences * torch.log_softmax(logits, dim=0))\n",
    "   \n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c6e0ee-9237-4160-8af3-89f72c76b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_dpo(gpt_model, reward_model, optimizer_gpt, optimizer_reward, vocab_size, seq_len, epochs, batch_size):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        seq_a, seq_b, preferences = generate_synthetic_data(batch_size, seq_len, vocab_size)\n",
    "\n",
    "       \n",
    "        logits_a = gpt_model(seq_a)\n",
    "        logits_b = gpt_model(seq_b)\n",
    "\n",
    "\n",
    "        \n",
    "        reward_a = reward_model(logits_a.mean(dim=1))\n",
    "        reward_b = reward_model(logits_b.mean(dim=1))\n",
    "\n",
    "        loss_dpo_gpt = dpo_loss(reward_a, reward_b, preferences)\n",
    "        optimizer_gpt.zero_grad()\n",
    "        loss_dpo_gpt.backward()\n",
    "        optimizer_gpt.step()\n",
    "\n",
    "        # Recompute logits for the Reward Model update\n",
    "        logits_a = gpt_model(seq_a).detach()  # Detach to avoid tracking gradients for GPT again\n",
    "        logits_b = gpt_model(seq_b).detach()\n",
    "\n",
    "        # Forward pass through the reward model\n",
    "        reward_a = reward_model(logits_a.mean(dim=1))\n",
    "        reward_b = reward_model(logits_b.mean(dim=1))\n",
    "\n",
    "        # Calculate DPO loss and backpropagate for reward model\n",
    "        loss_dpo_reward = dpo_loss(reward_a, reward_b, preferences)\n",
    "        optimizer_reward.zero_grad()\n",
    "        loss_dpo_reward.backward()\n",
    "        optimizer_reward.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss (GPT): {loss_dpo_gpt.item()}, Loss (Reward): {loss_dpo_reward.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fee2204-e0e2-4d9d-965d-95eb10f1c192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/py38_Cyber_ML/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpt_model        = GPT(vocab_size, embed_size, num_heads, num_layers, hidden_dim, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d317ef63-6c03-4c2a-b5b7-6b8112a2c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## reward_model     = RewardModel(embed_size, hidden_dim)\n",
    "## vocab_size\n",
    "reward_model     = RewardModel(vocab_size, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0279b8-5194-4688-adb1-6f9364957050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_gpt    = optim.Adam(gpt_model.parameters(), lr=lr)\n",
    "optimizer_reward = optim.Adam(reward_model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7a3da-0ebd-4e8f-b875-c1e9a3ffea40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "473e2b21-a018-4650-a7a5-5f94cf2f35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss (GPT): 1.329703450202942, Loss (Reward): 1.2532249689102173\n",
      "Epoch 2/10, Loss (GPT): 1.2963676452636719, Loss (Reward): 1.2614021301269531\n",
      "Epoch 3/10, Loss (GPT): 1.8905909061431885, Loss (Reward): 1.815993309020996\n",
      "Epoch 4/10, Loss (GPT): 1.4970364570617676, Loss (Reward): 1.485247015953064\n",
      "Epoch 5/10, Loss (GPT): 1.2721004486083984, Loss (Reward): 1.2359671592712402\n",
      "Epoch 6/10, Loss (GPT): 2.0174336433410645, Loss (Reward): 1.9564580917358398\n",
      "Epoch 7/10, Loss (GPT): 1.782792091369629, Loss (Reward): 1.735109567642212\n",
      "Epoch 8/10, Loss (GPT): 2.339132070541382, Loss (Reward): 2.3257839679718018\n",
      "Epoch 9/10, Loss (GPT): 1.5593814849853516, Loss (Reward): 1.536184310913086\n",
      "Epoch 10/10, Loss (GPT): 1.5795890092849731, Loss (Reward): 1.538716435432434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dpo(gpt_model, reward_model, optimizer_gpt, optimizer_reward, vocab_size, seq_len, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4db41-bdd7-4f45-81f3-b7b06bbf1528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57ff17-5705-4e8c-8723-f2fb42bcd830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af642f2-c0f2-4611-9557-52413c51bf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb599bc-1fc1-403f-8d09-105174b8f187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719c76a-a61d-4419-8dd6-16010859b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bdc09-6464-4233-b887-01c40c1ce32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810578e-0a44-4d61-9d55-e1b57a226562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4b346-9825-4f0f-9068-57f0c16021c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f144226-eac8-4be0-91da-373390cdb22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97449538-ec85-494e-8139-22ee87b1f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
