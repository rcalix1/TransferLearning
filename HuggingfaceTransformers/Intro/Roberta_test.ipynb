{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d840cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea59fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e4acf9fd874c4f99a82d0bc78c9a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\anaconda3\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user1\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadc33db889d459c93179072604cadb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da063381c21d4904b53a987b4a09dcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712691bf1899410e9c0468f280062f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model     = RobertaModel.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c73eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0399,  0.0799, -0.0311,  ..., -0.0870, -0.0856,  0.0220],\n",
      "         [-0.1976, -0.1169, -0.0138,  ..., -0.4788,  0.0394,  0.0516],\n",
      "         [ 0.0785,  0.1410,  0.0916,  ..., -0.3762, -0.0541,  0.0768],\n",
      "         ...,\n",
      "         [ 0.0406,  0.1816,  0.1674,  ...,  0.3509, -0.0937, -0.0614],\n",
      "         [ 0.0473, -0.1750,  0.1297,  ..., -0.1781, -0.1147,  0.1281],\n",
      "         [-0.0271,  0.0792, -0.0639,  ..., -0.1392, -0.0931,  0.0163]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 5.5089e-03, -2.0769e-01, -1.9937e-01, -6.2223e-02,  1.2380e-01,\n",
      "          1.9589e-01,  2.6027e-01, -8.1134e-02, -7.8488e-02, -1.6498e-01,\n",
      "          2.3004e-01, -1.3411e-02, -1.0554e-01,  9.3170e-02, -1.4054e-01,\n",
      "          4.9607e-01,  2.1941e-01, -4.6742e-01,  3.4409e-02, -3.2802e-02,\n",
      "         -2.6974e-01,  6.1645e-02,  4.5983e-01,  3.2322e-01,  1.2665e-01,\n",
      "          6.9717e-02, -1.4089e-01, -3.0230e-02,  1.9447e-01,  2.4181e-01,\n",
      "          2.9150e-01,  4.7572e-02,  1.0576e-01,  2.4660e-01, -2.4467e-01,\n",
      "          7.7988e-02, -3.0649e-01,  1.0402e-02,  2.5558e-01, -1.9651e-01,\n",
      "         -7.7544e-02,  1.7272e-01,  2.0621e-01, -1.3444e-01, -1.1626e-01,\n",
      "          4.0904e-01,  2.3875e-01,  2.7856e-02, -1.2853e-01, -8.5814e-02,\n",
      "         -3.4696e-01,  3.5747e-01,  2.9414e-01,  1.9575e-01,  1.5542e-03,\n",
      "          5.0073e-02, -1.5868e-01,  2.5290e-01, -9.3492e-02, -9.7420e-02,\n",
      "         -1.2342e-01, -2.0120e-01,  3.3150e-03, -6.2205e-02,  1.7270e-02,\n",
      "         -1.4268e-01,  7.8810e-02, -1.6385e-01, -1.5778e-01,  7.2886e-02,\n",
      "         -7.9698e-02,  1.2150e-01,  1.6011e-01, -2.9367e-01, -2.9419e-01,\n",
      "          3.7092e-02, -5.9159e-01, -1.1189e-01,  2.9819e-01,  4.2782e-01,\n",
      "         -1.1694e-01,  1.9376e-01,  4.0266e-02,  2.3093e-01, -1.7593e-02,\n",
      "         -5.9502e-02, -4.5680e-02, -1.0046e-01,  1.7261e-01,  2.8199e-01,\n",
      "         -1.9910e-01, -3.9335e-01,  7.2076e-02,  2.2104e-02, -9.1624e-02,\n",
      "          1.8084e-02, -1.5789e-02, -8.7499e-02, -1.6152e-01, -1.8200e-01,\n",
      "          7.1583e-02, -2.6666e-01, -1.3428e-01,  2.6277e-01, -2.5988e-02,\n",
      "         -1.9365e-01, -2.2166e-02,  3.0285e-01,  7.3243e-02, -1.2110e-01,\n",
      "         -1.7330e-01,  4.2722e-01,  3.0855e-01, -2.1507e-03,  2.1593e-02,\n",
      "          1.7428e-01,  1.2113e-01, -3.0916e-01,  4.1943e-01, -3.1522e-01,\n",
      "         -1.2614e-02, -1.0830e-01,  1.1434e-01,  1.3801e-01, -2.1497e-01,\n",
      "          2.8515e-01,  1.4284e-01,  2.6252e-01,  1.8217e-01,  1.0310e-01,\n",
      "         -3.6456e-02,  1.2194e-01, -1.0588e-01,  1.4298e-01,  2.1227e-01,\n",
      "          1.1914e-01, -3.6432e-03, -3.3756e-01, -2.1885e-01,  2.6013e-01,\n",
      "          3.4796e-01,  1.6547e-01, -4.4641e-02,  1.8890e-01,  1.0690e-01,\n",
      "          2.2974e-01,  1.4328e-01, -4.1227e-01,  3.9662e-02,  3.5711e-01,\n",
      "          9.9342e-02,  1.5879e-01, -7.2569e-02, -2.7610e-01, -2.4113e-01,\n",
      "         -8.1227e-02,  3.2326e-02, -3.1846e-01, -1.2046e-01,  3.6710e-01,\n",
      "          2.1183e-02,  6.5964e-03, -1.4817e-01, -2.1914e-01, -3.3170e-02,\n",
      "         -1.1349e-01,  2.9032e-02,  9.5070e-02, -8.1549e-02, -4.1578e-01,\n",
      "         -9.5431e-02, -5.5330e-01, -1.3459e-01,  1.9931e-01, -3.0730e-01,\n",
      "          2.4069e-01, -3.1188e-01,  9.6137e-02,  3.9284e-01,  2.2540e-02,\n",
      "         -8.5384e-03, -1.8233e-01, -6.9128e-03,  1.1493e-01,  3.2224e-01,\n",
      "          2.4914e-01, -3.9570e-01,  9.9221e-02,  1.4918e-01,  2.4525e-01,\n",
      "          1.4798e-01, -3.6924e-02, -1.4217e-01,  1.3289e-01, -2.0671e-01,\n",
      "          1.5319e-01, -2.2302e-01,  1.7759e-01, -2.5527e-01, -2.1133e-01,\n",
      "          2.9064e-01, -4.1257e-01, -3.6363e-02,  8.7023e-02,  2.7446e-01,\n",
      "          1.6164e-02, -4.5551e-02, -9.3476e-02,  1.1010e-01,  1.7881e-01,\n",
      "          1.3702e-01, -3.8887e-01,  2.6576e-01, -2.1107e-02, -3.2164e-02,\n",
      "         -3.7336e-02,  1.6574e-01,  2.4841e-01,  1.0866e-01, -3.9841e-01,\n",
      "         -1.3792e-01,  1.1342e-01,  2.9358e-01, -2.2647e-01,  1.6904e-01,\n",
      "         -2.8540e-01, -3.9293e-01, -1.2482e-01,  2.0361e-01,  2.1603e-01,\n",
      "          1.6627e-01, -2.6004e-01,  1.6319e-01, -9.9369e-02, -4.2770e-01,\n",
      "         -3.5937e-01, -1.0174e-01,  2.6555e-01,  1.8367e-01,  1.6671e-01,\n",
      "          2.6110e-01,  3.2564e-02,  1.2482e-01,  1.5326e-01,  1.6226e-01,\n",
      "         -1.5349e-01,  1.7668e-01, -3.5961e-01, -6.3114e-02, -2.7119e-01,\n",
      "         -1.9828e-01, -2.1629e-01,  4.0155e-01, -2.1892e-01,  2.2024e-01,\n",
      "          3.9611e-01, -3.1276e-01, -1.0444e-01,  1.6202e-01,  9.9406e-02,\n",
      "          7.9671e-02, -1.1603e-01,  1.8781e-01,  1.5266e-01, -1.1209e-01,\n",
      "          2.4593e-01, -7.9463e-03,  2.6179e-01,  1.7836e-01,  1.0072e-01,\n",
      "          1.3224e-01,  1.2675e-01, -1.5072e-01,  4.2776e-02,  3.3947e-03,\n",
      "         -2.9046e-02, -2.4658e-01, -1.5232e-01,  2.3007e-01, -5.1195e-02,\n",
      "          2.5305e-02, -1.5569e-01, -1.1066e-01,  1.0253e-02,  4.0917e-01,\n",
      "         -3.5904e-01,  2.5867e-01,  8.2328e-02,  1.6544e-01, -2.3262e-01,\n",
      "         -2.0073e-01,  7.3016e-02,  1.7271e-01, -4.0683e-01,  8.3123e-03,\n",
      "          1.6189e-01,  1.0571e-01,  2.0730e-01,  2.7475e-01, -1.0436e-03,\n",
      "         -1.0900e-01,  4.8666e-01, -1.3931e-01, -1.3280e-01,  2.6896e-01,\n",
      "         -2.5891e-01, -2.8182e-01,  2.5612e-01, -1.0386e-02,  2.9975e-01,\n",
      "          1.2432e-01,  3.1151e-02,  6.6552e-02, -5.9801e-01,  5.9794e-02,\n",
      "         -4.6940e-01, -7.7133e-03,  2.5040e-02, -9.2573e-02, -1.9608e-01,\n",
      "          1.5387e-01,  2.8523e-01, -2.4536e-01, -2.3038e-02,  2.0287e-01,\n",
      "          7.5557e-02, -1.1092e-01,  4.7914e-01, -6.3852e-03,  2.1113e-01,\n",
      "         -7.9863e-02,  2.5759e-01, -2.0103e-01,  2.5351e-01, -2.5166e-01,\n",
      "         -9.9547e-02,  6.0616e-03,  8.3687e-02,  6.2645e-02, -6.2211e-02,\n",
      "         -3.3980e-01,  2.2781e-01, -3.9283e-03, -5.1313e-02, -5.2678e-02,\n",
      "          1.0704e-01, -3.7396e-03,  4.5329e-02,  5.2650e-02,  3.4176e-01,\n",
      "          2.3280e-01, -3.0591e-02, -3.6823e-01, -1.1452e-02, -9.3783e-02,\n",
      "          4.3509e-02,  1.6303e-02, -1.4876e-02,  4.4033e-01, -8.8530e-02,\n",
      "          8.4731e-03, -1.3375e-01,  2.6457e-01,  1.9429e-01,  1.3563e-01,\n",
      "          1.2357e-01,  8.1338e-02,  1.3775e-01, -3.7806e-02, -9.7107e-03,\n",
      "         -1.6032e-01, -2.2003e-01, -2.7142e-01,  2.1004e-01, -2.3864e-01,\n",
      "         -1.6102e-01,  1.7495e-01,  2.1778e-01, -1.3424e-01,  1.2934e-01,\n",
      "          3.1594e-01,  1.0560e-01, -1.4693e-01,  2.6400e-01, -1.1088e-01,\n",
      "          1.0313e-01,  3.1988e-01, -1.5785e-02,  1.7095e-01,  5.0620e-01,\n",
      "          2.2535e-01, -3.6895e-01, -2.3196e-02, -2.1637e-01, -2.8277e-03,\n",
      "          2.4482e-01, -1.5500e-01,  1.8892e-01,  3.9296e-01,  3.0477e-01,\n",
      "          4.5545e-01, -5.0075e-04, -1.2834e-01,  8.1746e-02,  2.0117e-01,\n",
      "          2.9577e-02, -1.5627e-01, -1.7152e-01,  2.5309e-01,  6.9304e-02,\n",
      "         -1.5044e-01, -2.8479e-02, -1.3818e-01,  4.2379e-02, -1.4219e-01,\n",
      "         -3.9924e-01,  2.6147e-02,  2.0215e-01, -4.8005e-01,  1.0303e-01,\n",
      "         -2.9140e-01,  4.1790e-02, -2.3209e-01,  2.1531e-01, -2.4193e-01,\n",
      "         -1.1343e-01,  3.8828e-01, -8.1184e-02,  4.4284e-02, -1.8725e-01,\n",
      "         -1.4381e-01,  2.2642e-02,  1.0765e-02, -2.0593e-02, -2.1474e-02,\n",
      "          3.5438e-01, -1.2023e-01,  4.1614e-02,  1.6344e-02,  2.1853e-01,\n",
      "         -5.4339e-02,  1.7777e-01,  3.2427e-02, -1.3419e-01, -3.8637e-01,\n",
      "          1.2934e-01, -1.9147e-01, -4.2672e-01, -3.6895e-01,  3.5867e-01,\n",
      "         -1.3371e-01, -2.3648e-01, -2.1906e-01, -2.4636e-01,  7.0358e-02,\n",
      "          1.7607e-01,  4.6072e-01, -3.8398e-01, -6.8931e-02,  4.9437e-01,\n",
      "         -6.0801e-02, -1.9009e-01,  2.8643e-01,  2.0145e-01, -3.1449e-01,\n",
      "          3.4252e-01,  2.7029e-01, -4.8126e-02,  3.0894e-02,  5.2150e-01,\n",
      "          1.2724e-01,  1.8801e-01, -2.1856e-01,  4.5709e-01, -2.1000e-01,\n",
      "          3.2338e-01, -1.6112e-01, -2.1278e-01, -2.1798e-01,  5.6800e-03,\n",
      "          3.2694e-01,  1.9438e-01, -4.1540e-01, -1.1243e-01,  3.0854e-02,\n",
      "          3.5064e-01, -3.7660e-01, -7.2636e-02,  1.3596e-02, -3.5020e-01,\n",
      "          1.1656e-01,  8.9648e-02,  2.2971e-01, -3.7961e-01, -1.4298e-02,\n",
      "          3.9653e-01, -3.2147e-01,  1.2461e-01,  3.0595e-01,  8.2440e-02,\n",
      "          3.6272e-01, -3.8335e-02, -2.1560e-03,  4.2851e-02, -2.3457e-01,\n",
      "         -3.7404e-02,  1.3408e-01,  5.5615e-01,  1.5281e-01, -3.7712e-01,\n",
      "          1.0319e-01,  2.4608e-01, -1.5391e-01,  3.0286e-01, -8.6100e-02,\n",
      "         -5.3592e-02,  2.5818e-01, -3.8459e-02,  1.4252e-01, -1.0787e-01,\n",
      "         -2.4681e-01, -3.0708e-01,  3.7033e-01, -2.0800e-01, -1.0198e-01,\n",
      "         -1.5836e-01, -1.0262e-01, -1.4573e-01,  5.8234e-02, -3.7629e-01,\n",
      "          3.5123e-01,  1.3876e-01, -2.0120e-01, -7.9514e-02, -9.5375e-02,\n",
      "         -1.6546e-01, -2.2120e-01, -2.7076e-01,  4.3353e-01, -1.7167e-01,\n",
      "         -4.5127e-01,  2.6840e-01,  4.7233e-02,  3.5233e-01,  3.7975e-02,\n",
      "          9.7685e-02, -4.9184e-02,  1.4232e-01,  1.0990e-01, -1.1726e-01,\n",
      "          2.7266e-01,  6.1793e-02, -5.5200e-01, -1.2804e-01, -2.2415e-01,\n",
      "          7.8716e-02,  1.9359e-01, -3.5181e-01,  2.8643e-02,  1.2932e-02,\n",
      "          1.3989e-01,  3.6665e-02, -1.0544e-01, -7.2972e-02,  3.8369e-01,\n",
      "          2.3649e-01,  2.8850e-01,  9.2487e-02,  2.3269e-01, -2.3725e-02,\n",
      "         -3.2926e-01,  4.1883e-02,  8.3466e-02, -1.9405e-01,  4.5069e-01,\n",
      "         -1.0747e-01, -4.0432e-01, -7.5176e-02,  4.0132e-01,  1.1404e-01,\n",
      "         -2.2865e-02, -3.9775e-02,  2.0653e-01,  1.5294e-01, -1.3072e-01,\n",
      "          1.7690e-01, -3.9735e-02, -1.3966e-01, -1.1489e-01,  9.7657e-02,\n",
      "         -2.1835e-01,  4.5221e-02, -1.5239e-01, -2.3946e-02, -1.9325e-01,\n",
      "          1.5834e-03, -1.9906e-01,  2.5554e-01, -3.3957e-01,  1.1202e-01,\n",
      "          7.2102e-02,  2.9558e-01, -3.3939e-01, -1.7100e-01, -6.5720e-02,\n",
      "          1.7530e-01,  2.5835e-01,  3.5185e-01,  1.2493e-02,  2.4155e-02,\n",
      "         -1.6605e-01, -2.6158e-01,  8.3116e-02, -2.1563e-01,  1.4370e-01,\n",
      "          6.6084e-02,  2.3130e-01, -3.2377e-01, -1.8050e-01,  2.1140e-01,\n",
      "         -1.0994e-01, -1.4627e-01,  4.1303e-01,  2.4018e-01,  2.1471e-01,\n",
      "          2.5548e-02,  2.4919e-01,  2.8492e-02, -1.8463e-01, -1.2373e-01,\n",
      "         -2.6559e-01,  5.9787e-02, -8.0512e-02, -6.8777e-02, -5.0492e-02,\n",
      "         -1.2376e-01, -1.9724e-01, -1.5054e-01,  1.3930e-01,  1.3623e-01,\n",
      "          1.8751e-02, -4.2240e-02, -1.0840e-02, -2.8245e-01,  3.0564e-01,\n",
      "          2.2063e-02,  4.7607e-02, -6.7277e-02,  3.5990e-02, -1.3319e-01,\n",
      "          2.3331e-01,  2.1826e-01,  7.8162e-02, -1.9003e-01, -5.5433e-02,\n",
      "         -2.9025e-01, -3.5937e-01,  6.0977e-02,  1.2955e-01,  1.2198e-01,\n",
      "         -7.3401e-02, -2.6670e-01,  1.2089e-03, -1.3588e-01,  1.8425e-01,\n",
      "          1.7582e-02, -1.4403e-01, -9.8724e-02, -6.1054e-02, -4.2132e-02,\n",
      "          7.2096e-02, -2.0544e-01, -1.8875e-01, -1.0678e-01, -6.3250e-02,\n",
      "         -7.4950e-02,  3.5211e-01, -5.4213e-02,  2.7625e-01, -1.4645e-01,\n",
      "         -2.4692e-03, -1.6235e-01,  1.0015e-01, -7.5035e-02,  5.2829e-02,\n",
      "          2.6560e-01, -4.4650e-01, -1.5683e-01, -5.5133e-03, -2.0453e-01,\n",
      "         -1.4609e-01, -6.7070e-02, -3.6094e-02,  2.2104e-01, -3.4844e-01,\n",
      "          2.4064e-01, -1.1919e-01,  1.7915e-01, -6.3135e-02, -2.5740e-01,\n",
      "         -1.4077e-01,  7.0999e-03,  2.4211e-01, -3.4835e-01, -2.4513e-01,\n",
      "         -2.7377e-01, -1.0128e-01, -8.0601e-02, -2.7155e-01,  4.3099e-01,\n",
      "         -1.3109e-01, -7.6986e-02,  8.5791e-03,  4.5365e-01,  2.1246e-01,\n",
      "          1.5275e-01,  2.1835e-01, -2.8092e-02,  3.5058e-02,  1.1738e-01,\n",
      "         -4.6166e-01,  2.2353e-01, -2.4271e-01, -1.2826e-01,  5.5097e-03,\n",
      "          1.0833e-01, -1.5052e-02,  4.9523e-03, -1.3963e-01, -9.0636e-02,\n",
      "          2.1550e-01, -3.7187e-01, -3.3275e-02,  2.7192e-01,  1.5584e-01,\n",
      "         -2.4882e-01,  3.6535e-02,  1.2541e-01,  3.7535e-01,  1.0071e-01,\n",
      "         -2.1702e-01,  1.1843e-01, -3.4427e-01, -3.1185e-02, -1.8652e-01,\n",
      "         -2.9666e-01,  1.4519e-01, -7.0421e-02,  9.0006e-02, -9.4923e-02,\n",
      "         -2.9961e-01,  2.1801e-01, -5.5066e-02, -7.7099e-02,  4.2364e-01,\n",
      "          1.2425e-02, -1.1578e-01,  1.2917e-01,  1.9264e-02,  2.7741e-02,\n",
      "         -1.0188e-01,  2.7264e-01,  2.1920e-01, -2.7759e-01,  1.3998e-01,\n",
      "         -1.3603e-01, -4.8570e-02, -1.0197e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"the cat is so sad .\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(   **encoded_input    )\n",
    "\n",
    "print(  output   )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f67a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token   token_str                   sequence\n",
      "0  0.162565  11962        cute       The cat is so cute.>\n",
      "1  0.062341   4045       sweet      The cat is so sweet.>\n",
      "2  0.050228   2721   beautiful  The cat is so beautiful.>\n",
      "3  0.039934  19222    handsome   The cat is so handsome.>\n",
      "4  0.030951   6269       funny      The cat is so funny.>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fillmask = pipeline('fill-mask', model='roberta-base', tokenizer=tokenizer)\n",
    "\n",
    "res = pd.DataFrame(  fillmask(\"The cat is so <mask> .>\")   )\n",
    "\n",
    "print(  res   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887d9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(    tokenizer.mask_token     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0b81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
