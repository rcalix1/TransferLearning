{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Causal language modeling: the model has to predict the next token in the sentence (so the labels are the same as the inputs shifted to the right). To make sure the model does not cheat, it gets an attention mask that will prevent it to access the tokens after token i when trying to predict the token i+1 in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \\n',\n",
       "  \" As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \\n\",\n",
       "  \" Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \\n\"]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasets[\"train\"][15:18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_own_datasets = load_dataset(\"text\", data_files={ \"train\": \"/home/rcalix/Desktop/rc_train.txt\", \n",
    "                                                    \"validation\": \"/home/rcalix/Desktop/rc_validation.txt\"} )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_own_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers .',\n",
       "  \"As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attache to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \",\n",
       "  \"Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \",\n",
       "  'this is the fourth sentence ']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_own_datasets[\"train\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The gother other s , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers .',\n",
       "  \"As tdfdfdfdfdfd  defer wise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attache to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \",\n",
       "  \"Partlydsfd dfdff dfdfd ue to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \",\n",
       "  'this is tfdfdf urth sentence ']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_own_datasets[\"validation\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=20):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    \n",
    "    picks = []\n",
    "    \n",
    "    for _ in range( num_examples ):\n",
    "        \n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame( dataset[picks] )        ## indexing 10 picks \n",
    "    \n",
    "    print(df)\n",
    "    print(dataset.features.items())\n",
    "    \n",
    "    for column, typ in dataset.features.items():\n",
    "        print(column)\n",
    "        print(typ)\n",
    "        print(ClassLabel)\n",
    "        ## The isinstance() function returns True if the specified object \n",
    "        ## is of the specified type, otherwise False\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            print(\"Hello\")\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "            print(typ.names[i])\n",
    "            \n",
    "    display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0                                   = Fern Hobbs = \\n\n",
      "1    The single most important prognostic factor i...\n",
      "2                       = Le souper de Beaucaire = \\n\n",
      "3                                                    \n",
      "4                                                    \n",
      "5    Illinois was laid down on 10 February 1897 by...\n",
      "6                                                    \n",
      "7                                                    \n",
      "8    The chancel contains memorials to the Lords S...\n",
      "9    Fowler 's return against Birmingham City in F...\n",
      "10                     = = = Hurricane Alice = = = \\n\n",
      "11               Harry Seawright , federal judge . \\n\n",
      "12   Frederick Reines was born in Paterson , New J...\n",
      "13                                                   \n",
      "14   During Roman Britain , what is now the Strand...\n",
      "15   On 27 December 2006 , it was revealed that th...\n",
      "16   On October 18 , 2010 , Dylan released Volume ...\n",
      "17                                 = = History = = \\n\n",
      "18   The first camp opened on May 15 , 1941 near B...\n",
      "19                                                   \n",
      "dict_items([('text', Value(dtype='string', id=None))])\n",
      "text\n",
      "Value(dtype='string', id=None)\n",
      "<class 'datasets.features.features.ClassLabel'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Fern Hobbs = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The single most important prognostic factor in AML is cytogenetics , or the chromosomal structure of the leukemic cell . Certain cytogenetic abnormalities are associated with very good outcomes ( for example , the ( 15 ; 17 ) translocation in acute promyelocytic leukemia ) . About half of people with AML have \" normal \" cytogenetics ; they fall into an intermediate risk group . A number of other cytogenetic abnormalities are known to associate with a poor prognosis and a high risk of relapse after treatment . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Le souper de Beaucaire = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Illinois was laid down on 10 February 1897 by the Newport News Shipbuilding &amp; Dry Dock Company of Newport News , Virginia . She was launched on 4 October 1898 , sponsored by Miss Nancy Leiter , daughter of Chicago multi @-@ millionaire Levi Leiter and commissioned on 16 September 1901 . The ship 's first commander was Captain George A. Converse . Illinois was the first member of her class to be authorized , but the last to enter service . After commissioning , the ship began a shakedown cruise in the Chesapeake Bay , followed by initial training . She left the area on 20 November to test a new floating dry dock in Algiers , Louisiana . The ship was back in Newport News in January 1902 . She served briefly as the flagship of Rear Admiral Robley D. Evans from 15 to 28 February ; during this period , she took part in a reception for Prince Henry of Prussia , the brother of the German Kaiser . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The chancel contains memorials to the Lords Stanley of Alderley . The memorial to John Stanley , 1st Baron Stanley of Alderley contains his effigy dressed in peer 's robes lying under a canopy with his hand on a book , dated 1856 and by Richard Westmacott . On the other side of the chancel is a memorial to his son Edward Stanley , his effigy holding a scroll in his hand and with a dog at his feet . Engraved in brass on the side of the memorial are the figures of his widow and children . Lady Stanley is seated in the middle with their four surviving sons on her right , five surviving daughters on her left and three children who had died at a young age at her knee and on her lap . A memorial tablet to John Constantine Stanley , who died in 1878 , is by Joseph Boehm . The chancel contains a monument to Rev. Edward Shipton , rector of the church from 1625 to 1630 . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fowler 's return against Birmingham City in February 2006 was labelled by the tabloid press as the stuff of fairytales , and he himself said he felt like \" a kid waking up on Christmas morning every day \" . Fowler 's first appearance back at Anfield was as a substitute against Birmingham , receiving a standing ovation upon his introduction . After his return , he had three goals ruled out for offside , before finally getting off the mark on 15 March 2006 in a home game against Fulham , the same opponents against which he scored his first ever goal for Liverpool 13 years earlier . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>= = = Hurricane Alice = = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harry Seawright , federal judge . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Frederick Reines was born in Paterson , New Jersey , one of four children of Gussie ( Cohen ) and Israel Reines . His parents were Jewish emigrants from the same town in Russia , but only met in New York City , where they were later married . He had an older sister , Paula , who became a doctor , and two older brothers , David and William , who became lawyers . He said that his \" early education was strongly influenced \" by his studious siblings . He was the great @-@ nephew of the Rabbi Yitzchak Yaacov Reines , the founder of Mizrachi , a religious Zionist movement . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>During Roman Britain , what is now the Strand was part of the route to Silchester , known as \" Iter VIII \" on the Antonine Itinerary , and which later became known by the name Akeman Street . It was briefly part of a trading town called Lundenwic that developed around 600 AD , and stretched from Trafalgar Square to Aldwych . Alfred the Great gradually moved the settlement into the old Roman town of Londinium from around 886 AD onwards , leaving no mark of the old town , and the area returned to fields . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>On 27 December 2006 , it was revealed that the Cabinet had approved over half a billion baht worth of funding for a 14 @,@ 000 @-@ man secret anti @-@ protest special operations force , of which General Saprang was Commander . The so @-@ called CNS Special Operations Center , funded with 556 million baht diverted from the Defense Ministry , Police Office , and government emergency reserve fund , had been secretly established by the CNS on 1 December 2006 in order to control protests . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>On October 18 , 2010 , Dylan released Volume 9 of his Bootleg Series , The Witmark Demos . This comprised 47 demo recordings of songs taped between 1962 and 1964 for Dylan 's earliest music publishers : Leeds Music in 1962 , and Witmark Music from 1962 to 1964 . One reviewer described the set as \" a hearty glimpse of young Bob Dylan changing the music business , and the world , one note at a time . \" The critical aggregator website Metacritic awarded the album a Metascore of 86 , indicating \" universal acclaim \" . In the same week , Sony Legacy released Bob Dylan : The Original Mono Recordings , a box set that for the first time presented Dylan 's eight earliest albums , from Bob Dylan ( 1962 ) to John Wesley Harding ( 1967 ) , in their original mono mix in the CD format . The CDs were housed in miniature facsimiles of the original album covers , replete with original liner notes . The set was accompanied by a booklet featuring an essay by music critic Greil Marcus . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>= = History = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The first camp opened on May 15 , 1941 near Baltimore , Maryland . A total of 152 camps and units were established over the next six years . The federal government provided work projects , housing , camp furnishings and paid for transportation to the camps . The responsibilities of the churches included day @-@ to @-@ day management of the camps , subsistence costs , meals and healthcare for the men . When the young men arrived at the first camps , they started a six @-@ month experiment that would extend to six years . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "show_random_elements(datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer_checkpoint = \"sgugger/gpt2-like-tokenizer\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text \n",
    "\n",
    " The labels will be the same as the inputs, shifted to the left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x2ae993c391f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f7e754ea374524aa91fbf54f2e0d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e95a31dd9e14365b120e8bc4823d87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d1bafd2d12465f912ff2c0fe1a6351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "          tokenize_function, \n",
    "          batched=True, \n",
    "          num_proc=4, \n",
    "          remove_columns=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [238, 8576, 9441, 2987, 238, 252],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## try several indices \n",
    "\n",
    "tokenized_datasets[\"train\"][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    \n",
    "    ## concatenated_examples2 = {k: examples[k] for k in examples.keys()}\n",
    "    ## print(concatenated_examples2)\n",
    "    ## input()\n",
    "    \n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [ t[i : i + block_size] for i in range(0, total_length, block_size) ]\n",
    "                    for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ca7a9e6c8f4355b21d652e5e524edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3641be4b02dd4a9aa701590f7b740378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3954bb9537bf4b658618e87d34b971e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 4358\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 36718\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 3760\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in tokenized_datasets.keys():\n",
    "    print( tokenized_datasets[k] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples1 =  tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## concatenated_examples3 = {k: sum(examples1[k], []) for k in examples1.keys()}\n",
    "## concatenated_examples3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198,\n",
       " 548,\n",
       " 301,\n",
       " 15410,\n",
       " 555,\n",
       " 227,\n",
       " 198,\n",
       " 1538,\n",
       " 209,\n",
       " 13067,\n",
       " 6455,\n",
       " 201,\n",
       " 198,\n",
       " 1538,\n",
       " 2081,\n",
       " 1231,\n",
       " 221,\n",
       " 196,\n",
       " 1306,\n",
       " 201,\n",
       " 756,\n",
       " 2726,\n",
       " 684,\n",
       " 293,\n",
       " 7376,\n",
       " 968,\n",
       " 225,\n",
       " 1136,\n",
       " 5168,\n",
       " 6395,\n",
       " 209,\n",
       " 24281,\n",
       " 198,\n",
       " 906,\n",
       " 1422,\n",
       " 6455,\n",
       " 390,\n",
       " 1136,\n",
       " 277,\n",
       " 2737,\n",
       " 747,\n",
       " 6455,\n",
       " 13516,\n",
       " 227,\n",
       " 1579,\n",
       " 5860,\n",
       " 1718,\n",
       " 209,\n",
       " 929,\n",
       " 198,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 7131,\n",
       " 201,\n",
       " 3079,\n",
       " 3411,\n",
       " 390,\n",
       " 13230,\n",
       " 5382,\n",
       " 201,\n",
       " 766,\n",
       " 218,\n",
       " 642,\n",
       " 1763,\n",
       " 196,\n",
       " 4014,\n",
       " 6669,\n",
       " 702,\n",
       " 1413,\n",
       " 824,\n",
       " 221,\n",
       " 198,\n",
       " 1231,\n",
       " 218,\n",
       " 198,\n",
       " 674,\n",
       " 209,\n",
       " 1415,\n",
       " 390,\n",
       " 471,\n",
       " 2610,\n",
       " 1076,\n",
       " 1388,\n",
       " 2377,\n",
       " 3130,\n",
       " 227,\n",
       " 198,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 506,\n",
       " 906,\n",
       " 3641,\n",
       " 828,\n",
       " 201,\n",
       " 1360,\n",
       " 543,\n",
       " 1581,\n",
       " 196,\n",
       " 1365,\n",
       " 2978,\n",
       " 1496,\n",
       " 209,\n",
       " 252,\n",
       " 261,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 2094,\n",
       " 1232,\n",
       " 201,\n",
       " 198,\n",
       " 1028,\n",
       " 73,\n",
       " 52,\n",
       " 58,\n",
       " 1232,\n",
       " 201,\n",
       " 301,\n",
       " 3493,\n",
       " 547,\n",
       " 3660,\n",
       " 340,\n",
       " 7784,\n",
       " 9345,\n",
       " 9441,\n",
       " 209]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lm_datasets[\"train\"][4][\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198,\n",
       " 548,\n",
       " 301,\n",
       " 15410,\n",
       " 555,\n",
       " 227,\n",
       " 198,\n",
       " 1538,\n",
       " 209,\n",
       " 13067,\n",
       " 6455,\n",
       " 201,\n",
       " 198,\n",
       " 1538,\n",
       " 2081,\n",
       " 1231,\n",
       " 221,\n",
       " 196,\n",
       " 1306,\n",
       " 201,\n",
       " 756,\n",
       " 2726,\n",
       " 684,\n",
       " 293,\n",
       " 7376,\n",
       " 968,\n",
       " 225,\n",
       " 1136,\n",
       " 5168,\n",
       " 6395,\n",
       " 209,\n",
       " 24281,\n",
       " 198,\n",
       " 906,\n",
       " 1422,\n",
       " 6455,\n",
       " 390,\n",
       " 1136,\n",
       " 277,\n",
       " 2737,\n",
       " 747,\n",
       " 6455,\n",
       " 13516,\n",
       " 227,\n",
       " 1579,\n",
       " 5860,\n",
       " 1718,\n",
       " 209,\n",
       " 929,\n",
       " 198,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 7131,\n",
       " 201,\n",
       " 3079,\n",
       " 3411,\n",
       " 390,\n",
       " 13230,\n",
       " 5382,\n",
       " 201,\n",
       " 766,\n",
       " 218,\n",
       " 642,\n",
       " 1763,\n",
       " 196,\n",
       " 4014,\n",
       " 6669,\n",
       " 702,\n",
       " 1413,\n",
       " 824,\n",
       " 221,\n",
       " 198,\n",
       " 1231,\n",
       " 218,\n",
       " 198,\n",
       " 674,\n",
       " 209,\n",
       " 1415,\n",
       " 390,\n",
       " 471,\n",
       " 2610,\n",
       " 1076,\n",
       " 1388,\n",
       " 2377,\n",
       " 3130,\n",
       " 227,\n",
       " 198,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 506,\n",
       " 906,\n",
       " 3641,\n",
       " 828,\n",
       " 201,\n",
       " 1360,\n",
       " 543,\n",
       " 1581,\n",
       " 196,\n",
       " 1365,\n",
       " 2978,\n",
       " 1496,\n",
       " 209,\n",
       " 252,\n",
       " 261,\n",
       " 674,\n",
       " 268,\n",
       " 83,\n",
       " 2094,\n",
       " 1232,\n",
       " 201,\n",
       " 198,\n",
       " 1028,\n",
       " 73,\n",
       " 52,\n",
       " 58,\n",
       " 1232,\n",
       " 201,\n",
       " 301,\n",
       " 3493,\n",
       " 547,\n",
       " 3660,\n",
       " 340,\n",
       " 7784,\n",
       " 9345,\n",
       " 9441,\n",
       " 209]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lm_datasets[\"train\"][4][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First note that we duplicate the inputs tolabels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model  = AutoModelForCausalLM.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"/scratch/scholar/rcalix/{model_checkpoint}-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6747' max='6747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6747/6747 17:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.548100</td>\n",
       "      <td>6.475266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.191400</td>\n",
       "      <td>6.202877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.013000</td>\n",
       "      <td>6.115944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6747, training_loss=6.389684912044242, metrics={'train_runtime': 1043.7357, 'train_samples_per_second': 51.711, 'train_steps_per_second': 6.464, 'total_flos': 3525678710784000.0, 'train_loss': 6.389684912044242, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='242' max='242' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [242/242 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 453.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The perplexity is still quite high since for this demo we trained on a small dataset for a small number of epochs. For a real LM training, you would need a larger dataset and more epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Masked language modeling\n",
    "\n",
    "For masked language modeling (MLM) we are going to use the same preprocessing as before for our dataset with one additional step: we will randomly mask some tokens (by replacing them by [MASK]) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens). \n",
    "\n",
    "We will use the bert-base-cased model for this example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint     = \"bert-base-cased\"\n",
    "tokenizer_checkpoint = \"sgugger/bert-like-tokenizer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1c1fda48f24b3692460a157b6daa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2a3477faf64568966f886c069653e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878e617799d9494798289864ba42d59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e42aa3a3604dc285c9688664d44b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41197ef30be548e9824262f4d86d9242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bec7b9741e942b2b1554170b44d541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model  = AutoModelForMaskedLM.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"/scratch/scholar/rcalix/test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    "    ## push_to_hub_model_id=f\"{model_checkpoint}-wikitext2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we use a special data_collator. \n",
    "\n",
    "The data_collator is a function that is responsible of taking the samples and batching them in tensors. In the previous example, we had nothing special to do, so we just used the default for this argument. \n",
    "\n",
    "Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) but then the tokens would always be masked the same way at each epoch. By doing this step inside the data_collator, we ensure this random masking is done in a new way each time we go over the data.\n",
    "\n",
    "To do this masking for us, the library provides a DataCollatorForLanguageModeling. We can adjust the probability of the masking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7038' max='7038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7038/7038 20:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.100500</td>\n",
       "      <td>7.057284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.889600</td>\n",
       "      <td>6.890655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.872200</td>\n",
       "      <td>6.889329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7038, training_loss=7.043899787627489, metrics={'train_runtime': 1211.3565, 'train_samples_per_second': 46.463, 'train_steps_per_second': 5.81, 'total_flos': 3703423157830656.0, 'train_loss': 7.043899787627489, 'epoch': 3.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 990.34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## More on the grouping function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Anaconda 2020.11)",
   "language": "python",
   "name": "anaconda-2020.11-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
