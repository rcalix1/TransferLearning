{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1745bd",
   "metadata": {},
   "source": [
    "\n",
    "## HF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce853d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install pandas\n",
    "## !pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4be79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa803f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06287447f1ef419cbb973ff6ea54e979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\anaconda3\\envs\\py37_ITS530_HF\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user1\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c91e153a8c348048c7e79f64167e626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385414c610274ff5bd0c97c1f29003fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "model     = AlbertModel.from_pretrained(\"albert-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47af6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.0939,  0.5053,  0.1715,  ..., -0.4915, -0.7119,  1.1999],\n",
      "         [ 0.1346,  1.4558,  1.0297,  ..., -0.2366,  1.0784, -0.2260],\n",
      "         [-0.7314, -0.7786, -0.3179,  ...,  0.1351,  1.5941, -0.9347],\n",
      "         ...,\n",
      "         [-0.3901, -0.3225,  0.5177,  ...,  0.1546,  1.4358,  0.5165],\n",
      "         [ 0.6785,  0.1826,  0.2188,  ..., -0.4804,  1.1837,  0.4180],\n",
      "         [ 0.0544,  0.1334, -0.0490,  ..., -0.1220,  0.1077,  0.1971]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.5389, -0.6086,  0.8374, -0.8752, -0.6615, -0.9479,  0.5728, -0.5487,\n",
      "          0.5840, -0.9994,  0.8545,  0.5818, -0.5577, -0.9433, -0.9580, -0.5357,\n",
      "          0.6166,  0.5180,  0.9972, -0.5242, -0.7181, -0.9925,  0.9959,  0.9557,\n",
      "          0.5715, -0.5299,  0.6058, -0.9689, -0.9995, -0.5381, -1.0000,  0.6917,\n",
      "          0.5956,  0.5521,  0.5793, -0.4755,  0.6820,  0.9975, -0.6022,  0.6362,\n",
      "          0.5400, -0.9743, -0.7325,  0.6179,  0.5656,  0.5581,  0.9794, -0.9897,\n",
      "          0.8847, -0.5357, -0.5040, -0.4795, -0.5231, -0.9948, -0.4365,  0.6682,\n",
      "         -0.6268, -0.5502,  0.9999, -0.9274,  0.5563, -0.7180,  0.6925, -0.0518,\n",
      "         -0.5860,  0.5159,  0.5491,  0.9987, -0.4818,  0.9509,  0.5379,  0.6610,\n",
      "          0.3893, -0.7199,  0.9678,  0.6112,  0.2541,  0.5337, -0.1678, -0.9875,\n",
      "          0.7976,  0.6921, -0.6272,  0.6089, -0.9083, -0.9670,  0.7986, -0.9999,\n",
      "          0.6013,  0.9556,  0.5744,  0.6996, -0.5221, -1.0000,  0.4949, -0.6163,\n",
      "         -0.9996,  0.6177,  0.4992, -0.6089,  0.0204, -0.4823,  0.6955, -0.6689,\n",
      "         -0.5657, -0.5609,  0.5371,  0.9156,  0.5947,  0.9987, -0.9881, -0.5697,\n",
      "          0.5092,  0.9699, -0.5373,  0.2463, -0.6231,  0.8376, -0.9970,  0.6121,\n",
      "          0.5532, -0.1323,  0.5625, -0.2801,  0.5866, -0.0411,  0.4946, -0.5807,\n",
      "          0.9781, -0.7949,  0.7598,  0.4959, -0.9996, -0.2695,  0.5900,  0.9567,\n",
      "         -0.5043, -0.5384, -0.4652,  0.5677,  0.9913, -0.5343,  0.4876, -0.6243,\n",
      "         -0.5905,  0.5376,  0.1107, -0.4722,  0.5395,  0.9920, -0.9649,  0.9803,\n",
      "          0.6290, -0.9754, -0.9965,  0.4031,  0.9585, -0.8586,  0.7727, -0.6127,\n",
      "         -0.5031, -0.8986, -0.9972, -0.6447, -0.9983, -0.6674,  0.9995, -0.0982,\n",
      "          0.9989, -0.9960, -0.5467,  0.6419, -0.5579,  0.9756,  0.5637,  0.5709,\n",
      "          0.5049,  0.7744, -0.6163, -0.4750,  0.9860, -0.9988,  0.5437,  0.6146,\n",
      "          0.9866,  0.5762,  0.6972, -0.8261,  0.5362, -0.7132, -0.5479, -0.8171,\n",
      "          0.6692,  0.8044,  1.0000, -0.6661,  0.9877, -0.9413,  0.9603, -0.9974,\n",
      "         -0.5681,  0.8138,  0.9554,  0.6140,  0.6060,  0.9017, -0.9037, -0.9976,\n",
      "         -1.0000, -0.5775, -0.9990,  0.9761, -0.9929,  0.5654, -0.9995,  0.9895,\n",
      "          0.9828, -0.5895,  0.9999, -0.5955,  0.7423,  0.5605, -1.0000,  0.9405,\n",
      "          0.4755,  0.5402, -0.0427, -0.5301,  0.9834, -0.9979, -0.9650, -0.1920,\n",
      "          0.6147, -0.9998, -0.7863, -0.5956,  0.6833,  0.6220,  0.6133, -0.9984,\n",
      "          0.9997,  0.5902, -0.4455, -0.5963, -0.4481,  1.0000, -0.6990, -0.9864,\n",
      "         -0.5737,  0.9986,  0.9650,  0.6409,  0.7254, -0.5913,  0.7585, -0.4945,\n",
      "         -0.9905, -0.9907, -0.9885,  0.5429, -0.9832,  0.3368, -0.4427, -0.9990,\n",
      "         -0.5921,  0.8455,  0.9974,  0.9907,  0.5569, -0.7269,  0.5541, -0.5849,\n",
      "          0.9503, -0.3441,  0.9727, -0.5766, -0.9643,  0.6135,  0.6478,  0.5075,\n",
      "         -0.6038, -0.9645,  0.6113, -0.5924,  0.9977, -0.8475,  0.9993, -0.9840,\n",
      "         -0.9997,  0.5611, -0.6752, -0.5998,  0.9976, -0.5602, -0.9991, -0.9998,\n",
      "          0.5379,  0.9600,  0.9109, -0.9273,  0.6442, -0.5728, -0.5806,  0.9976,\n",
      "          0.6053, -0.5299, -0.4652,  0.5691,  0.5444,  0.7444, -0.7575, -0.5889,\n",
      "         -0.6324, -0.9904, -0.6146, -0.1315, -0.5207, -0.7519,  0.9885,  0.9849,\n",
      "         -0.5481, -0.7331,  0.9983, -0.9965,  0.6848, -1.0000,  0.8843, -0.9998,\n",
      "         -0.9986, -0.4946, -0.8764, -0.5258, -0.6298, -0.9630,  0.5616, -0.9943,\n",
      "          0.5348, -0.6639,  0.9751,  0.9655, -0.9999, -0.5451, -0.9690,  0.4979,\n",
      "          0.6217,  0.5347,  0.5720, -0.4130,  0.6365, -0.9977,  0.5660,  0.8981,\n",
      "         -0.6503,  0.8731, -0.5977,  0.5923, -0.9534, -0.6619, -0.5056,  0.9936,\n",
      "          0.9946, -0.8731, -0.5158,  0.5799, -0.8774,  0.9835, -0.9888,  0.9781,\n",
      "         -0.9987, -0.6409, -0.9903,  0.9985,  0.9770, -0.0145, -0.6365, -0.8916,\n",
      "         -0.9632,  0.6548, -0.5847, -0.4420, -0.5395,  0.9907,  0.5294,  0.9598,\n",
      "         -0.8369,  0.3878,  0.4403,  0.8903, -0.9897,  0.9904, -0.9379, -0.5925,\n",
      "          0.7264,  1.0000, -0.6151,  0.5495, -0.9939, -0.9947, -0.6024,  0.5712,\n",
      "          0.9903, -0.5553, -0.8521,  0.9633,  0.9941, -0.9958,  0.6838,  0.9977,\n",
      "          0.9777,  0.6092,  0.6768,  0.9565,  0.8763,  0.5598,  0.9858, -0.5119,\n",
      "          0.9999, -0.9974, -0.9997,  0.9946, -0.5673,  0.9900, -0.3811,  0.5111,\n",
      "         -0.5730,  0.6244, -0.6128, -0.5874,  0.5110,  0.7326,  0.6362,  0.9932,\n",
      "          0.4849, -0.9857, -1.0000, -0.4840,  0.7177,  0.7035, -0.5624,  0.9677,\n",
      "         -0.5916,  0.3152, -0.6548, -0.6172,  0.5814, -0.9997,  1.0000, -0.9984,\n",
      "          0.9956, -0.5522,  0.9703,  0.8778,  0.9980, -0.6644, -1.0000, -0.6022,\n",
      "         -0.9981,  0.6386, -0.5405, -0.9466, -0.5577,  0.7479, -0.5130,  0.9974,\n",
      "          0.6783, -0.6663, -0.9988,  1.0000, -0.8558, -0.9997,  0.5939,  0.5842,\n",
      "          0.7857,  0.5055,  0.4649,  0.5300, -0.7545, -0.5991,  0.9925, -0.9735,\n",
      "          0.6699, -0.9737,  0.9788, -0.6243, -0.1971,  0.9100,  0.9976,  0.9965,\n",
      "         -0.5905, -1.0000, -0.9894, -0.9985, -0.9902,  0.5256, -0.9414,  0.8575,\n",
      "         -0.5909, -0.5276,  0.9843,  0.5991, -0.5081, -0.5691, -0.9928, -0.8762,\n",
      "         -0.5308, -0.8378,  0.6305, -0.9992, -0.7223, -0.9795, -0.9966, -0.2209,\n",
      "         -0.9893,  0.5206, -0.9999,  0.1896, -0.5885, -0.2791, -0.5350,  0.6116,\n",
      "          0.5779, -0.5441,  0.5181,  0.9797, -0.6044,  1.0000,  0.9804,  0.9325,\n",
      "          0.6564, -0.5283, -0.5807,  0.9990, -0.5780, -0.3293,  0.8932, -0.9560,\n",
      "          0.2547, -0.9955,  0.9760, -0.7774, -0.9482, -0.9731, -0.6382, -1.0000,\n",
      "         -0.9998, -0.9997,  0.6137,  0.9058, -0.9950,  0.9975,  0.7070, -0.7353,\n",
      "          0.9996,  0.6472, -0.9668,  0.4811, -0.5817,  0.9999, -0.5038,  0.9689,\n",
      "         -0.6226, -0.9957,  0.7648,  0.8525, -0.5277, -0.5593, -0.9321, -0.8895,\n",
      "          0.0924, -0.5239,  0.6398,  1.0000, -0.8751,  0.5711, -0.6633,  0.6174,\n",
      "         -0.9837,  0.5024, -0.3760, -0.0277, -0.9935, -0.9125,  0.9988,  0.5705,\n",
      "          0.6303,  0.5907,  0.6922, -0.4449,  0.9999, -0.9999, -0.4663,  0.5208,\n",
      "         -0.7330,  0.7680, -0.5072, -0.6962,  0.4947,  0.9871, -0.5109,  0.9981,\n",
      "         -0.2512,  0.6404,  0.6248,  0.5514,  0.4868,  0.9998,  0.6474, -0.8779,\n",
      "         -0.5295, -0.5331, -0.5324,  0.9995,  0.9986, -0.9297, -0.8414, -0.4643,\n",
      "         -0.9900,  0.9704,  0.3923,  0.9969,  0.9962,  0.6685, -0.4709, -0.9802,\n",
      "          0.9997,  0.9108, -0.5998,  0.7974,  0.6599, -0.5918,  0.2114,  0.8868,\n",
      "         -0.9023,  0.4456,  0.6417,  0.9894,  0.9965, -0.5891,  0.7945, -1.0000,\n",
      "         -0.5476,  0.9729, -0.9571,  0.6201,  0.9929, -1.0000, -0.5281, -0.5658,\n",
      "          0.9561,  0.9945,  0.4898,  0.5265, -0.4376, -0.6331,  0.9994,  0.5002,\n",
      "         -0.6261,  0.9982, -0.6541, -0.5192,  0.6054,  0.6585,  0.9997,  0.9172,\n",
      "         -0.9721, -0.5747,  0.5340, -0.6338,  0.9969, -0.9998, -0.5476,  0.3739,\n",
      "         -0.9095,  0.4989,  0.7928,  0.6511, -0.6124, -0.9995,  0.5622,  0.9802,\n",
      "          0.6183,  0.9917,  0.5452, -0.3209,  0.6751,  0.8842, -0.0295,  0.8924,\n",
      "         -0.9983,  0.5719, -0.9400,  0.5640, -0.6941, -0.9860, -0.7384,  0.5401,\n",
      "          0.9944,  0.6418,  0.6126,  0.4074, -0.6566,  0.5857,  0.5752,  0.4527,\n",
      "         -0.5325, -0.7019, -0.6813, -0.9992,  0.4352,  0.5594, -0.3176,  0.6317,\n",
      "         -0.5097, -0.9699,  0.5408, -0.5417,  0.6882, -0.5613, -0.9999, -0.6752,\n",
      "         -0.8950, -0.6237,  0.5450, -0.6474,  0.6165, -0.5778, -1.0000, -0.3299,\n",
      "          0.4772, -0.9755,  0.6563, -0.9908,  0.4970,  0.9999,  1.0000, -0.9309,\n",
      "          0.5245, -0.9981, -0.5973, -0.6928, -1.0000,  0.5746,  1.0000,  0.7874,\n",
      "          0.4824, -0.9958, -0.9598,  0.9957, -0.9904, -0.6838,  0.5736, -0.5372,\n",
      "         -0.9996,  0.8443,  0.6096,  0.6239,  0.6185, -0.9927, -0.6150,  0.4123,\n",
      "         -0.9988,  0.5088,  0.9972, -0.5270,  0.5174,  0.2681, -0.9984,  0.6499]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"the cat is so sad .\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac75f704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdf0f4eae2b4673920bccd5ef2fae64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token     token_str                     sequence\n",
      "0  0.281033  10901          cute          the cat is so cute.\n",
      "1  0.094896  26354      adorable      the cat is so adorable.\n",
      "2  0.042963   1700         happy         the cat is so happy.\n",
      "3  0.040976   5066         funny         the cat is so funny.\n",
      "4  0.024234  28803  affectionate  the cat is so affectionate.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fillmask = pipeline('fill-mask', model='albert-base-v2')\n",
    "res = pd.DataFrame(fillmask(\"The cat is so [MASK] .\"))\n",
    "\n",
    "\n",
    "print(  res   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b3b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token   token_str                          sequence\n",
      "0  0.031417  27668  figurative  el chapo is a figurative person.\n",
      "1  0.028689  18496  franciscan  el chapo is a franciscan person.\n",
      "2  0.025276   9650   dominican   el chapo is a dominican person.\n",
      "3  0.022960  19210    moroccan    el chapo is a moroccan person.\n",
      "4  0.017772  14484      basque      el chapo is a basque person.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "res1 = pd.DataFrame(fillmask(\"El chapo is a  [MASK] person.\"))\n",
    "print(  res1   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec13fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token  token_str                     sequence\n",
      "0  0.059892   5934  wonderful  anna is a wonderful person.\n",
      "1  0.057768   5066      funny      anna is a funny person.\n",
      "2  0.047015    254       good       anna is a good person.\n",
      "3  0.046512   8601     lovely     anna is a lovely person.\n",
      "4  0.038440   2210       nice       anna is a nice person.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res2 = pd.DataFrame(fillmask(\"Anna is a  [MASK] person.\"))\n",
    "print(  res2   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a7ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token token_str  \\\n",
      "0  0.210027     24        he   \n",
      "1  0.208615     39       she   \n",
      "2  0.108849  28153     joyah   \n",
      "3  0.071829  29833    evalle   \n",
      "4  0.010178   1687    doctor   \n",
      "\n",
      "                                            sequence  \n",
      "0  the doctor is examining the patient. he is wri...  \n",
      "1  the doctor is examining the patient. she is wr...  \n",
      "2  the doctor is examining the patient. joyah is ...  \n",
      "3  the doctor is examining the patient. evalle is...  \n",
      "4  the doctor is examining the patient. doctor is...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "res3 = pd.DataFrame(fillmask(\"The doctor is examining the patient.  [MASK] is writing down notes.\"))\n",
    "print(  res3   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d20463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModel, pipeline, BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f644e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "nlp = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb14e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "text = '''\n",
    "Aviation is the activities surrounding mechanical flight and the aircraft industry. Aircraft includes fixed-wing and rotary-wing types, \n",
    "morphable wings, wing-less lifting bodies, as well as lighter-than-air craft such as hot air balloons and airships.\n",
    "Aviation began in the 18th century with the development of the hot air balloon, an apparatus capable of atmospheric displacement through buoyancy. \n",
    "Some of the most significant advancements in aviation technology came with the controlled gliding flying of Otto Lilienthal in 1896; then a large \n",
    "step in significance came with the construction of the first powered airplane by the Wright brothers in the early 1900s. Since that time, aviation \n",
    "has been technologically revolutionized by the introduction of the jet which permitted a major form of transport throughout the world.\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980e7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Aircraft includes fixed-wing and rotary-wing types, morphable wings, wing-less lifting bodies, and lighter-than-air craft such as hot air balloons and airships . Aviation began in the 18th century with the development of the hot air balloon, an apparatus capable of atmospheric displacement through buoyancy .'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "q = nlp(text)\n",
    "\n",
    "print(q)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6519a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from transformers import AutoModel\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ad1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0566febf4b1435a8784bdf929f02d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c11a3bddb6344a7a3d7e6a62eba3327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abac6622fe6547d9b6f8bd5cb2334d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2478, 19081, 2003, 3733, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Using transformers is easy!\"\n",
    "\n",
    "print(    tokenizer(text)    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f436cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2478, 19081,  2003,  3733,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")   ## pt is for pytorch tensors\n",
    "\n",
    "print(  encoded_input   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "088f96df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166edf40053f4e8fbe41e80f9ffa7ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0245,  0.1256,  0.1062,  ..., -0.0637,  0.0344,  0.3892],\n",
      "         [ 0.2678,  0.3460, -0.3190,  ..., -0.1258,  0.1899,  0.2213],\n",
      "         [ 1.8175, -0.0306, -0.1497,  ..., -0.4804, -0.1272,  0.4366],\n",
      "         ...,\n",
      "         [-0.0793,  0.1282,  0.1530,  ...,  0.0619, -0.0437,  0.0854],\n",
      "         [-0.5069, -0.3263, -0.0349,  ...,  0.6772,  0.0161, -0.0475],\n",
      "         [ 0.7605,  0.1723, -0.0956,  ...,  0.3013, -0.4924, -0.2380]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.0693e-01, -2.2955e-01, -7.8098e-02,  5.0498e-01, -5.4595e-02,\n",
      "          4.6806e-03,  7.9700e-01,  1.5625e-01, -2.2330e-02, -9.9973e-01,\n",
      "         -9.3650e-02,  2.7417e-01,  9.7285e-01, -9.1490e-02,  8.9556e-01,\n",
      "         -4.5264e-01, -9.2253e-03, -5.1343e-01,  1.1915e-01, -5.0694e-01,\n",
      "          5.1190e-01,  9.9036e-01,  4.3037e-01,  2.3087e-01,  2.4903e-01,\n",
      "          4.4874e-01, -5.5464e-01,  8.9291e-01,  9.3205e-01,  6.3521e-01,\n",
      "         -5.6062e-01,  1.5136e-01, -9.7535e-01, -1.0466e-01, -8.7350e-03,\n",
      "         -9.6917e-01,  2.3301e-01, -6.6448e-01,  1.3110e-01,  4.0307e-02,\n",
      "         -8.4948e-01,  9.2704e-02,  9.9731e-01, -6.5108e-01,  9.2232e-03,\n",
      "         -3.0088e-01, -9.9990e-01,  5.3405e-02, -8.2420e-01,  6.9862e-02,\n",
      "          1.5873e-01, -2.9166e-01,  1.0858e-01,  3.1045e-01,  3.5964e-01,\n",
      "          1.9965e-01, -2.5448e-01, -8.5427e-03, -9.1700e-02, -4.0565e-01,\n",
      "         -5.8833e-01,  2.1108e-01, -1.7970e-01, -8.1340e-01,  1.6097e-01,\n",
      "         -7.7431e-02,  5.0666e-02, -1.7087e-01,  2.8893e-02, -1.6621e-02,\n",
      "          8.0907e-01,  8.7130e-02,  3.7305e-02, -7.7836e-01, -1.5121e-01,\n",
      "          1.3257e-01, -4.1371e-01,  1.0000e+00, -1.6688e-01, -9.5741e-01,\n",
      "         -1.3882e-02,  1.2986e-01,  3.1587e-01,  3.8013e-01, -1.6050e-01,\n",
      "         -9.9999e-01,  1.9682e-01, -2.3730e-02, -9.7978e-01,  1.7979e-01,\n",
      "          2.6340e-01, -1.2108e-01,  8.1585e-03,  3.6060e-01, -2.1808e-01,\n",
      "         -1.4638e-01, -1.3528e-01, -1.9099e-01, -3.3304e-02, -1.4138e-02,\n",
      "         -1.2780e-01, -7.7660e-02, -5.6965e-03, -2.9172e-01,  9.7807e-02,\n",
      "         -3.3428e-01, -3.8657e-01,  2.6733e-01, -2.6927e-01,  5.7803e-01,\n",
      "          2.5838e-01, -1.9271e-01,  2.0445e-01, -9.3075e-01,  5.1738e-01,\n",
      "         -1.4734e-01, -9.7571e-01, -3.6683e-01, -9.7983e-01,  4.9812e-01,\n",
      "         -9.0744e-02, -7.2502e-02,  9.3019e-01,  3.9441e-01,  2.2934e-01,\n",
      "          8.5643e-02,  1.4300e-01, -1.0000e+00, -1.7908e-01, -2.4885e-01,\n",
      "          1.4101e-01, -1.0805e-01, -9.5851e-01, -9.3664e-01,  4.5034e-01,\n",
      "          9.3644e-01,  1.3158e-01,  9.9654e-01, -3.9841e-02,  8.9691e-01,\n",
      "          1.9111e-01, -7.9264e-02, -2.4080e-01, -3.2739e-01,  4.5909e-01,\n",
      "          2.5007e-03, -4.6659e-01,  1.9817e-01,  1.3912e-01, -3.3462e-01,\n",
      "         -3.3291e-01, -9.8980e-02,  2.2952e-03, -9.1728e-01, -2.6936e-01,\n",
      "          8.9525e-01,  2.2041e-01, -6.2468e-02,  5.1193e-01, -9.5714e-02,\n",
      "         -3.0350e-01,  7.5995e-01,  3.6758e-01,  2.4042e-01, -3.2262e-02,\n",
      "          2.4398e-01, -2.1624e-01,  4.2241e-01, -7.8136e-01,  3.1716e-01,\n",
      "          2.1915e-01, -2.1981e-01, -4.8427e-02, -9.5833e-01, -2.1616e-01,\n",
      "          4.1545e-01,  9.7993e-01,  7.2754e-01,  1.6402e-01,  1.8560e-01,\n",
      "         -1.2295e-01,  6.6035e-02, -9.1547e-01,  9.6345e-01, -4.9247e-02,\n",
      "          1.2726e-01,  1.3058e-01,  6.5870e-02, -8.1989e-01, -3.7575e-01,\n",
      "          7.5660e-01, -8.3923e-02, -7.2467e-01,  1.6125e-01, -3.4961e-01,\n",
      "         -3.0578e-01, -2.0488e-02,  2.2186e-01, -1.7275e-01, -2.9424e-01,\n",
      "          2.4525e-02,  8.9628e-01,  9.2672e-01,  6.9406e-01, -4.4220e-01,\n",
      "          4.5836e-01, -8.5466e-01, -3.8480e-01, -4.4047e-02,  1.7052e-01,\n",
      "         -8.2675e-02,  9.8684e-01, -2.7132e-01, -2.8912e-02, -8.6806e-01,\n",
      "         -9.7365e-01, -1.0019e-01, -8.4460e-01,  6.3547e-02, -6.4409e-01,\n",
      "          2.6812e-01,  4.5650e-01, -9.2767e-02,  2.0368e-01, -9.3706e-01,\n",
      "         -6.5065e-01,  2.6622e-01, -2.2203e-01,  3.5747e-01, -2.0383e-01,\n",
      "          4.3871e-01,  1.4815e-01, -5.1123e-01,  6.2445e-01,  8.8006e-01,\n",
      "          8.8060e-02, -6.4801e-01,  7.1853e-01, -1.6384e-01,  7.8464e-01,\n",
      "         -4.7550e-01,  9.5512e-01,  1.3076e-01,  4.3126e-01, -8.7076e-01,\n",
      "          2.3859e-02, -8.1405e-01,  1.5223e-01,  8.9342e-02, -4.2130e-01,\n",
      "          1.5582e-01,  4.0571e-01,  1.9589e-01,  7.0439e-01, -3.0846e-01,\n",
      "          9.8070e-01, -6.0916e-01, -9.1614e-01, -1.7916e-02, -4.8824e-02,\n",
      "         -9.7768e-01,  1.7690e-01,  2.1155e-01, -4.2166e-01, -2.6435e-01,\n",
      "         -4.8533e-01, -9.2835e-01,  7.5747e-01, -1.8330e-04,  9.6480e-01,\n",
      "          6.5538e-02, -8.1384e-01, -2.1544e-01, -8.9222e-01, -2.4304e-01,\n",
      "         -2.4165e-02,  3.3696e-01, -1.5462e-01, -9.3290e-01,  3.7076e-01,\n",
      "          4.5001e-01,  3.5216e-01,  5.0091e-02,  9.8980e-01,  9.9975e-01,\n",
      "          9.5633e-01,  8.5565e-01,  7.9330e-01, -9.7050e-01, -2.4648e-01,\n",
      "          9.9988e-01, -5.8991e-01, -9.9998e-01, -8.8661e-01, -3.3174e-01,\n",
      "          3.5820e-01, -1.0000e+00, -1.0059e-01,  1.0458e-01, -8.7745e-01,\n",
      "         -2.2353e-02,  9.6624e-01,  9.6099e-01, -1.0000e+00,  7.4753e-01,\n",
      "          8.9533e-01, -4.5733e-01,  4.5401e-01, -1.0749e-01,  9.4313e-01,\n",
      "          4.1960e-01,  3.1564e-01, -9.0901e-02,  2.3180e-01, -4.0300e-01,\n",
      "         -7.4911e-01,  1.8190e-01,  5.0469e-02,  8.4004e-01, -2.4601e-03,\n",
      "         -6.1238e-01, -8.7067e-01, -3.6567e-02, -1.4675e-01, -4.0689e-01,\n",
      "         -9.4310e-01, -5.4568e-02, -2.2922e-01,  4.7034e-01, -8.4271e-04,\n",
      "          1.6473e-01, -6.1009e-01,  9.5913e-02, -5.1800e-01,  3.4012e-01,\n",
      "          4.9001e-01, -8.9927e-01, -4.8331e-01,  1.5024e-01, -4.1442e-01,\n",
      "          3.2478e-02, -9.3292e-01,  9.4318e-01, -1.5642e-01, -2.2795e-01,\n",
      "          1.0000e+00, -8.9335e-02, -7.4984e-01,  2.1740e-01,  5.3060e-02,\n",
      "          1.2262e-02,  1.0000e+00,  4.5659e-01, -9.6248e-01, -3.3499e-01,\n",
      "          2.7310e-01, -3.0940e-01, -3.1532e-01,  9.9502e-01, -1.3470e-01,\n",
      "          1.0798e-02,  2.8401e-01,  9.5626e-01, -9.7733e-01,  7.7631e-01,\n",
      "         -8.5197e-01, -9.3308e-01,  9.3958e-01,  9.0363e-01, -2.4957e-01,\n",
      "         -5.3120e-01, -7.3842e-02, -6.7206e-02,  1.4951e-01, -9.1531e-01,\n",
      "          3.3933e-01,  3.3719e-01, -4.4206e-02,  8.1836e-01, -7.0472e-01,\n",
      "         -3.7476e-01,  2.1586e-01, -1.3151e-01,  2.8981e-01,  7.6345e-02,\n",
      "          3.2799e-01, -1.0584e-01, -4.8380e-02, -1.5923e-01, -3.1009e-01,\n",
      "         -9.4464e-01, -1.0402e-01,  1.0000e+00,  7.8810e-02, -3.8804e-02,\n",
      "         -1.5313e-01, -1.0382e-02, -3.6609e-01,  4.0297e-01,  3.2693e-01,\n",
      "         -8.5416e-02, -7.7358e-01,  1.5549e-02, -8.7544e-01, -9.7545e-01,\n",
      "          6.4336e-01,  1.1659e-01, -1.9802e-01,  9.9840e-01,  1.8976e-01,\n",
      "          7.6887e-02, -1.1438e-01,  6.0018e-01, -2.9937e-02,  4.1683e-01,\n",
      "          8.6059e-02,  9.6045e-01, -1.4988e-01,  3.7188e-01,  7.5653e-01,\n",
      "         -1.1260e-01, -2.0171e-01, -4.6066e-01, -1.8401e-02, -9.0042e-01,\n",
      "          2.3770e-01, -9.0984e-01,  9.3333e-01,  5.1747e-02,  3.0526e-01,\n",
      "          8.7149e-02, -1.1829e-02,  1.0000e+00, -4.2711e-01,  5.0115e-01,\n",
      "         -5.3655e-02,  6.9584e-01, -9.6526e-01, -6.8838e-01, -2.7324e-01,\n",
      "          5.4192e-02,  1.5302e-02, -1.8431e-01,  1.1719e-01, -9.4914e-01,\n",
      "          1.1118e-02, -1.8535e-01, -9.4378e-01, -9.7960e-01,  3.9752e-01,\n",
      "          6.5870e-01, -9.1114e-02, -6.2071e-01, -4.9771e-01, -5.1922e-01,\n",
      "         -2.3533e-02, -1.3315e-02, -9.0396e-01,  4.8514e-01, -1.1471e-01,\n",
      "          3.1671e-01, -9.0658e-02,  4.1215e-01, -9.0740e-02,  8.1691e-01,\n",
      "          2.4675e-01,  1.2968e-01,  3.0002e-03, -6.9687e-01,  7.1903e-01,\n",
      "         -7.0665e-01, -1.1214e-01, -1.0464e-01,  1.0000e+00, -2.1131e-01,\n",
      "          1.9290e-01,  7.0170e-01,  5.0720e-01, -2.1303e-02,  7.6784e-02,\n",
      "          2.3659e-01,  6.9486e-02,  5.8781e-02,  1.3068e-01, -4.6417e-01,\n",
      "         -2.5202e-01,  4.7214e-01, -9.7722e-02, -1.9600e-01,  6.6032e-01,\n",
      "          3.3888e-01,  1.8278e-02,  3.3838e-02, -1.1809e-01,  9.9396e-01,\n",
      "          6.8474e-02,  4.0136e-02, -3.2415e-01,  7.9705e-02, -1.7239e-01,\n",
      "         -2.0474e-01,  9.9999e-01,  1.6814e-01,  3.7092e-02, -9.7786e-01,\n",
      "         -9.2082e-03, -8.5347e-01,  9.9918e-01,  7.4692e-01, -7.1508e-01,\n",
      "          3.4106e-01,  2.9014e-01, -1.0467e-01,  5.5030e-01,  3.2316e-02,\n",
      "         -2.2904e-01,  1.2492e-01,  6.2244e-02,  9.3175e-01, -3.9400e-01,\n",
      "         -9.4904e-01, -5.4308e-01,  2.0346e-01, -9.2968e-01,  9.7265e-01,\n",
      "         -3.7871e-01, -1.2349e-01, -2.4950e-01,  4.7224e-01,  2.8212e-01,\n",
      "         -1.8212e-01, -9.6503e-01, -9.2420e-02, -7.0992e-02,  9.1386e-01,\n",
      "          6.3773e-02, -3.6354e-01, -8.6207e-01, -1.3284e-01,  2.8337e-02,\n",
      "         -7.2289e-02, -8.9489e-01,  9.4697e-01, -9.5790e-01,  3.6471e-01,\n",
      "          9.9998e-01,  3.1712e-01, -4.8636e-01, -2.7477e-02, -3.4102e-01,\n",
      "          2.1831e-01, -9.3478e-02,  4.5264e-01, -9.2096e-01, -2.0185e-01,\n",
      "         -6.8922e-02,  1.4302e-01, -1.1499e-01,  3.0689e-01,  6.4045e-01,\n",
      "          1.7534e-01, -3.1930e-01, -4.2536e-01,  1.5577e-01,  2.5666e-01,\n",
      "          6.6747e-01, -1.6247e-01,  2.7846e-02,  3.5049e-02, -3.7073e-02,\n",
      "         -8.3882e-01, -1.6867e-01, -6.7778e-02, -9.9383e-01,  6.0389e-01,\n",
      "         -1.0000e+00, -3.6400e-01, -4.3181e-01, -1.3990e-01,  7.4709e-01,\n",
      "          2.4853e-01,  8.5506e-02, -6.8283e-01, -1.0749e-01,  7.3225e-01,\n",
      "          6.4275e-01, -1.1077e-02,  3.6798e-01, -6.0260e-01,  6.6456e-02,\n",
      "         -4.9960e-03,  1.2260e-01,  1.5128e-01,  7.4629e-01, -1.1590e-01,\n",
      "          1.0000e+00,  1.0917e-01, -3.3361e-01, -9.2606e-01,  1.7550e-01,\n",
      "         -7.6893e-02,  9.9993e-01, -7.9612e-01, -9.2217e-01,  2.0935e-01,\n",
      "         -3.9006e-01, -7.3423e-01,  1.2146e-01, -1.3270e-01, -5.3680e-01,\n",
      "         -2.6603e-01,  9.1959e-01,  7.7091e-01, -4.1181e-01,  2.6311e-01,\n",
      "         -2.2255e-01, -2.7543e-01, -1.3545e-01,  6.9295e-02,  9.7576e-01,\n",
      "          2.3980e-01,  8.3575e-01,  5.5182e-01,  5.2120e-02,  9.5429e-01,\n",
      "          3.8379e-02,  3.8238e-01,  3.0231e-02,  9.9999e-01,  1.9562e-01,\n",
      "         -8.6649e-01,  4.8111e-01, -9.6812e-01, -8.0678e-02, -9.0729e-01,\n",
      "          1.3852e-01,  3.4657e-02,  8.2973e-01, -1.3381e-01,  9.2618e-01,\n",
      "          2.3641e-01, -1.1484e-01,  6.7370e-02,  4.6496e-01,  2.0901e-01,\n",
      "         -8.6575e-01, -9.7216e-01, -9.7511e-01,  2.1821e-01, -2.9150e-01,\n",
      "          9.9907e-02,  1.6243e-01, -2.8210e-03,  1.9700e-01,  3.2306e-01,\n",
      "         -9.9999e-01,  8.8178e-01,  2.0983e-01,  1.1092e-01,  9.3068e-01,\n",
      "          3.0156e-01,  2.5755e-01,  2.0259e-01, -9.7336e-01, -9.2025e-01,\n",
      "         -2.5134e-01, -8.8316e-02,  6.6548e-01,  4.6846e-01,  8.1476e-01,\n",
      "          1.9317e-01, -4.2984e-01, -1.6354e-01,  2.7414e-01, -5.0309e-01,\n",
      "         -9.8432e-01,  2.7842e-01,  1.0795e-01, -8.9846e-01,  9.3812e-01,\n",
      "         -3.7096e-01, -7.3985e-02,  4.2313e-01, -1.6020e-01,  9.0063e-01,\n",
      "          6.5636e-01,  2.0661e-01,  9.4530e-02,  2.8253e-01,  8.1507e-01,\n",
      "          9.0654e-01,  9.6963e-01, -1.2809e-01,  6.4264e-01,  2.2661e-01,\n",
      "          2.8017e-01,  6.7664e-01, -9.0934e-01,  1.4490e-02, -1.4224e-02,\n",
      "         -4.4035e-02,  1.2798e-01, -1.0869e-01, -9.0210e-01,  5.6138e-01,\n",
      "         -5.1869e-02,  4.4514e-01, -2.4095e-01,  2.1980e-01, -3.1606e-01,\n",
      "         -6.0214e-02, -6.3803e-01, -3.1309e-01,  4.3863e-01,  1.2491e-01,\n",
      "          8.7836e-01,  4.4623e-01,  4.5004e-02, -3.6396e-01, -2.1821e-02,\n",
      "          2.7998e-02, -8.9432e-01,  8.4768e-01,  4.0873e-03,  3.6565e-01,\n",
      "          1.1024e-01, -1.6959e-01,  7.8012e-01, -2.4000e-01, -2.4772e-01,\n",
      "         -2.2759e-01, -6.5326e-01,  7.4380e-01, -4.9723e-02, -3.8314e-01,\n",
      "         -4.1102e-01,  5.3696e-01,  2.0416e-01,  9.9133e-01, -6.7996e-02,\n",
      "         -1.8402e-01, -1.9683e-01, -2.2829e-01,  1.6786e-01, -2.1096e-01,\n",
      "         -9.9999e-01,  2.4763e-01, -1.7585e-02,  1.3428e-01,  3.6199e-02,\n",
      "          1.3687e-01, -3.4179e-02, -9.5323e-01, -1.6859e-01,  1.8482e-01,\n",
      "          5.6347e-02, -4.5788e-01, -1.9958e-01,  3.4789e-01,  3.8013e-01,\n",
      "          5.0409e-01,  7.7521e-01,  2.5048e-01,  4.4049e-01,  4.6310e-01,\n",
      "         -1.2782e-01, -5.6950e-01,  8.4918e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "output = model(**encoded_input)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f17604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a14ad07fb05443dbfd41cb6170d8de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.09747550636529922, 'token': 10533, 'token_str': 'carpenter', 'sequence': 'the man worked as a carpenter.'}, {'score': 0.052383385598659515, 'token': 15610, 'token_str': 'waiter', 'sequence': 'the man worked as a waiter.'}, {'score': 0.04962700605392456, 'token': 13362, 'token_str': 'barber', 'sequence': 'the man worked as a barber.'}, {'score': 0.037886135280132294, 'token': 15893, 'token_str': 'mechanic', 'sequence': 'the man worked as a mechanic.'}, {'score': 0.03768078237771988, 'token': 18968, 'token_str': 'salesman', 'sequence': 'the man worked as a salesman.'}]\n",
      "      score  token  token_str                        sequence\n",
      "0  0.097476  10533  carpenter  the man worked as a carpenter.\n",
      "1  0.052383  15610     waiter     the man worked as a waiter.\n",
      "2  0.049627  13362     barber     the man worked as a barber.\n",
      "3  0.037886  15893   mechanic   the man worked as a mechanic.\n",
      "4  0.037681  18968   salesman   the man worked as a salesman.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "result = unmasker(\"the man worked as a [MASK].\")     \n",
    "\n",
    "print(result)\n",
    "\n",
    "print(   pd.DataFrame(result)     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eddde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ElectraTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "155dd26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at google/electra-small-generator and are newly initialized: ['electra.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n",
    "\n",
    "fillmask = pipeline('fill-mask', model='google/electra-small-generator', tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa4eb51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token token_str                                sequence\n",
      "0  0.172026   3280       die      the goat on the mountain will die.\n",
      "1  0.049207   5438      feed     the goat on the mountain will feed.\n",
      "2  0.036642   4521       eat      the goat on the mountain will eat.\n",
      "3  0.034830   2022        be       the goat on the mountain will be.\n",
      "4  0.024741   5788   survive  the goat on the mountain will survive.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#res = pd.DataFrame(  fillmask(\"The cat is very [MASK] .\")   )\n",
    "\n",
    "res = pd.DataFrame(  fillmask(\"The goat on the mountain will [MASK] .\")  )\n",
    "\n",
    "print(  res   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0605de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(    tokenizer.mask_token     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2023d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "493b7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa4b3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sequence = \"I am going to france\" \n",
    "\n",
    "label = ['travel', 'cooking', 'dancing']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22c75015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a011163f1c74f769ef4633f10d7555e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16e428cca1b4ea390e6efb2e04fd3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9534dbf45974714973590ba5a6a5d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4baeec58a10416b83878684bb6fcafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be124a8d36e84155ae41ab2d7addfeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e069c0ea27fc4847af99f1bb9fea0049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5300ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "premise    = sequence\n",
    "hypothesis = f'This example is {label}.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df3ca7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am going to france'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "premise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebdf4598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This example is ['travel', 'cooking', 'dancing'].\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hypothesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "376266f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\anaconda3\\envs\\py37_ITS530_HF\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2421: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3740de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   100,   524,   164,     7,  6664,  2389,     2,     2,   713,\n",
       "          1246,    16, 47052, 28881,  3934,   128, 35190,   154,  3934,   128,\n",
       "           417,  7710,   108,  8174,     2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dfbe3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "logits = nli_model(x.to(device))[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a412933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0327,  1.3776,  0.6796]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ddd032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0327,  0.6796]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "entail_contradiction_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29917be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0623, 0.9377]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "\n",
    "probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7e37745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9377], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "prob_label_is_true = probs[:,1]\n",
    "prob_label_is_true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859dcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2318c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6da3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b32056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7ab36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f269ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f17397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8472b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91591cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca28ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f507a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75700689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3195ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
